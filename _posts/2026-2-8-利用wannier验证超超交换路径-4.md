---
layout: post
title: "(原创自研)利用wannier验证超超交换路径(4)"
subtitle: "Experience Sharing"
background: '/img/bg-sci-note.jpg'
categories: sci-note
permalink: /sci-note_posts/20260208-super4
---

## <center>说明</center>

本篇文章是此系列最后一篇文章，提取跃迁矩阵元，将其对角化后量化d轨道和p轨道的分轨道的相对能量并画图。
文件路径：
```shell
/public/home/cssong/song/1mrx/9_single_layer/19_ReIrGe2S6/TcIrGeSe/3_static_band/4_wannier/ncl/20260207_detail/crystal_spiltl
```

## <center>代码</center>  

(1) 第一步
    
核心用途：从 Wannier90 的 wannier90_hr.dat 里提取每个原子对应的“局域在位（on-site）哈密顿量”子块，做本征分解得到“局域能级”（可理解为晶场/原子轨道能级的数值化近似），
并且用 wannier90.win 的投影顺序给这些能级打上 px/py/pz/dxy/... 之类的轨道标签，最后输出若干 CSV 统计文件。

该脚本通过读取三个关键文件构建材料模型：

wannier90_hr.dat（对应 --hr参数）：提取 R=(0,0,0) 的矩阵元，构建 on‑site 哈密顿量并做厄米化处理（0.5*(H0+H0†)）。

edges.csv（对应 --edges参数）：提供 Wannier 函数与原子/元素的归属关系，必须包含 m,n,atom m,atom n,elem m,elem n列；若同一 WF 被标记为不同原子或元素，脚本会报错并要
求修正一致性。

wannier90.win（对应 --win参数）：仅解析 begin projections…end projections块，获取各元素的投影轨道信息（支持自动将 d/p/s 展开为 5d/3p/1s）。

以上文件共同用于构建哈密顿量并关联轨道、原子与元素信息。

总的来说：专门用于分析Wannier函数局域能级的工具。它通过整合三份输入文件——hr.dat（提供哈密顿量矩阵元）、edges.csv（定义Wannier函数与原子的归属关系）和wannier90.win（规
定轨道的投影顺序）——来构建并分解每个原子附近的电子结构。

其核心计算流程分为六步：首先从哈密顿量中提取on-site相互作用部分；接着将Wannier函数按原子分组；然后依据投影信息为这些函数打上轨道标签；之后对每个原子的哈密顿量子块进行本
征分解，获得局域能级；再为每个能级标记出其主导轨道成分；最后还可选择性地进行能级配对诊断，以分析能级分裂特征。

脚本会输出三份CSV结果文件：levels.csv详细列出每个原子的所有能级及其主导轨道；pair_centers.csv提供能级配对的诊断信息；orbital_order.csv则汇总了每个原子各类轨道的最低
能量。总而言之，该工具将抽象的哈密顿量矩阵转化为清晰、按原子和轨道分解的能级图谱，极大地便利了后续的材料电子结构分析与可视化。

```python

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import csv
from pathlib import Path
import numpy as np
import pandas as pd


# =========================
# helpers: parse hr.dat
# =========================
def parse_hr_dat(hr_path: Path):
    lines = hr_path.read_text(encoding="utf-8", errors="ignore").splitlines()
    if len(lines) < 4:
        raise RuntimeError("hr.dat too short")
    num_wann = int(lines[1].strip())
    nrpts = int(lines[2].strip())

    degen = []
    idx = 3
    while len(degen) < nrpts:
        degen.extend([int(t) for t in lines[idx].split()])
        idx += 1
    rec_lines = lines[idx:]
    return num_wann, rec_lines


def build_H0(num_wann, rec_lines):
    H0 = np.zeros((num_wann, num_wann), dtype=np.complex128)
    for ln in rec_lines:
        if not ln.strip():
            continue
        toks = ln.split()
        if len(toks) < 7:
            continue
        Rx, Ry, Rz = int(toks[0]), int(toks[1]), int(toks[2])
        if (Rx, Ry, Rz) != (0, 0, 0):
            continue
        m = int(toks[3]) - 1
        n = int(toks[4]) - 1
        re0 = float(toks[5])
        im0 = float(toks[6])
        H0[m, n] = re0 + 1j * im0
    return 0.5 * (H0 + H0.conj().T)


# =========================
# helpers: parse edges.csv -> WF -> (atom_id, elem)
# =========================
def read_edges_wf_map(edges_path: Path):
    df = pd.read_csv(edges_path)
    needed = ["m", "n", "atom_m", "atom_n", "elem_m", "elem_n"]
    for c in needed:
        if c not in df.columns:
            raise RuntimeError(f"edges.csv missing required column: {c}")

    wf_to_pairs = {}
    for _, r in df.iterrows():
        m = int(r["m"]); n = int(r["n"])
        am = int(r["atom_m"]); an = int(r["atom_n"])
        em = str(r["elem_m"]); en = str(r["elem_n"])
        wf_to_pairs.setdefault(m, set()).add((am, em))
        wf_to_pairs.setdefault(n, set()).add((an, en))

    bad = []
    wf_map = {}
    for wf, s in wf_to_pairs.items():
        if len(s) != 1:
            bad.append((wf, sorted(list(s))))
        else:
            wf_map[wf] = next(iter(s))

    if bad:
        msg = ["[ERROR] Some WFs map to multiple (atom_id, elem) labels. Fix edges.csv labeling first."]
        for wf, pairs in bad[:50]:
            msg.append(f"  wf={wf}: {pairs}")
        raise RuntimeError("\n".join(msg))

    return wf_map


def atoms_by_element(wf_map, num_wann):
    elem_to_atoms = {}
    for wf in range(1, num_wann + 1):
        aid, elem = wf_map[wf]
        elem_to_atoms.setdefault(elem, set()).add(aid)
    return {e: sorted(list(s)) for e, s in elem_to_atoms.items()}


# =========================
# helpers: parse win projections (explicit order)
# =========================
def norm_orb(tok: str) -> str:
    t = tok.strip().lower()
    t = t.replace("dx2y2", "dx2-y2").replace("dx2_y2", "dx2-y2")
    t = t.replace("d(z2)", "dz2").replace("d(z^2)", "dz2").replace("d(z**2)", "dz2")
    t = t.replace("d(x2-y2)", "dx2-y2")
    return t


def expand_token(tok: str):
    t = norm_orb(tok)
    if t == "d":
        return ["dxy", "dyz", "dxz", "dz2", "dx2-y2"]
    if t == "p":
        return ["px", "py", "pz"]
    if t == "s":
        return ["s"]
    return [t]


def parse_win_projections(win_path: Path):
    lines = win_path.read_text(encoding="utf-8", errors="ignore").splitlines()
    in_proj = False
    proj_lines = []
    for ln in lines:
        s = ln.strip()
        low = s.lower()
        if low.startswith("begin projections"):
            in_proj = True
            continue
        if low.startswith("end projections"):
            in_proj = False
            continue
        if not in_proj:
            continue
        if (not s) or s.startswith("#") or s.startswith("!"):
            continue
        proj_lines.append(s)

    out = []
    for raw in proj_lines:
        if ":" not in raw:
            continue
        elem, rhs = raw.split(":", 1)
        elem = elem.strip()
        rhs = rhs.replace(",", " ").replace(";", " ")
        toks = [t for t in rhs.split() if t.strip()]
        orbs = []
        for t in toks:
            orbs += expand_token(t)
        out.append((elem, orbs))

    if not out:
        raise RuntimeError("[ERROR] No projections found in win. Check begin/end projections block.")
    return out


def build_wf_labels_from_win(proj_spec, elem_to_atoms, num_wann):
    """
    Construct expected WF label sequence in the SAME ordering as Wannier90 projections expansion.
    We assume SOC spinor duplication if num_wann == 2 * spatial_count.
    Return: list labels (1..num_wann): (atom_id, elem, orb)
    """
    spatial = []
    for elem, orbs in proj_spec:
        if elem not in elem_to_atoms:
            raise RuntimeError(f"[ERROR] win projections element '{elem}' not in edges elements {list(elem_to_atoms.keys())}")
        for aid in elem_to_atoms[elem]:
            for orb in orbs:
                spatial.append((aid, elem, orb))

    if len(spatial) == num_wann:
        return spatial, False
    if 2 * len(spatial) == num_wann:
        spinor = []
        for item in spatial:
            spinor.append(item)
            spinor.append(item)
        return spinor, True

    raise RuntimeError(
        "[ERROR] projection expansion count mismatch.\n"
        f"  spatial_count={len(spatial)}, num_wann={num_wann}\n"
        "Expected num_wann == spatial_count (no spinor) OR num_wann == 2*spatial_count (SOC spinor).\n"
    )


# =========================
# local diag + orbital labeling from basis
# =========================
def diag_local(H0, wf_list_1based):
    idx = [w - 1 for w in wf_list_1based]
    sub = H0[np.ix_(idx, idx)]
    evals, evecs = np.linalg.eigh(sub)
    order = np.argsort(np.real(evals))
    return np.real(evals[order]), evecs[:, order]


def try_pairs(evals_rel):
    if len(evals_rel) % 2 != 0:
        return []
    out = []
    for i in range(0, len(evals_rel), 2):
        e1 = float(evals_rel[i]); e2 = float(evals_rel[i + 1])
        out.append((0.5 * (e1 + e2), abs(e2 - e1), i, i + 1))
    return out


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--hr", default="wannier90_hr.dat")
    ap.add_argument("--edges", default="edges.csv")
    ap.add_argument("--win", default="wannier90.win")
    ap.add_argument("--Ef", type=float, default=0.0, help="output energies as E_rel = E - Ef (eV)")
    ap.add_argument("--out_prefix", default="cfFINAL")
    ap.add_argument("--pair_tol", type=float, default=0.02, help="warn if pair split > tol (eV)")
    args = ap.parse_args()

    hr_path = Path(args.hr)
    edges_path = Path(args.edges)
    win_path = Path(args.win)
    for p in [hr_path, edges_path, win_path]:
        if not p.exists():
            raise FileNotFoundError(p.resolve())

    num_wann, rec_lines = parse_hr_dat(hr_path)
    H0 = build_H0(num_wann, rec_lines)

    wf_map = read_edges_wf_map(edges_path)
    elem_to_atoms = atoms_by_element(wf_map, num_wann)

    proj_spec = parse_win_projections(win_path)
    expected_labels, spinor_dup = build_wf_labels_from_win(proj_spec, elem_to_atoms, num_wann)

    # Build WF groups per atom from edges (ground truth for which WF belongs to which atom)
    atom_to_wfs = {}
    atom_to_elem = {}
    for wf in range(1, num_wann + 1):
        aid, elem = wf_map[wf]
        atom_to_wfs.setdefault(aid, []).append(wf)
        atom_to_elem[aid] = elem
    for aid in atom_to_wfs:
        atom_to_wfs[aid] = sorted(atom_to_wfs[aid])

    # Now: assign each WF an orbital label by matching its position in the expected sequence.
    # This requires that the WF ordering produced by Wannier90 follows the projection expansion ordering.
    # We'll build wf_orb[wf] = orb_label.
    wf_orb = {}
    wf_expected_atom = {}
    for wf in range(1, num_wann + 1):
        aid_e, elem_e, orb_e = expected_labels[wf - 1]
        wf_expected_atom[wf] = (aid_e, elem_e)
        wf_orb[wf] = orb_e

    # Consistency check: WF->atom from edges should match expected atom from win ordering
    mism = []
    for wf in range(1, num_wann + 1):
        aid_true, elem_true = wf_map[wf]
        aid_e, elem_e = wf_expected_atom[wf]
        if (aid_true != aid_e) or (elem_true != elem_e):
            mism.append((wf, (aid_true, elem_true), (aid_e, elem_e)))
    if mism:
        # We do not abort; but we warn loudly because orbital labeling would be unreliable.
        print("[WARN] WF ordering does NOT match win projection expansion ordering for these WFs (first 20 shown):")
        for it in mism[:20]:
            print(f"  wf={it[0]} edges={it[1]} expected_from_win={it[2]}")
        print("[WARN] In this case, you must reorder WFs by parsing wout spread table or use a stricter mapping method.")
        # If this happens, stop now to avoid producing wrong orbital order.
        raise RuntimeError("WF ordering mismatch: cannot safely label orbitals from win ordering.")

    # Outputs
    out_levels = f"{args.out_prefix}_levels.csv"
    out_order = f"{args.out_prefix}_orbital_order.csv"
    out_pairs = f"{args.out_prefix}_pair_centers.csv"

    rows_levels = []
    rows_pairs = []
    lowest = {}  # (aid, elem, orb) -> lowest E_rel

    for aid in sorted(atom_to_wfs.keys()):
        elem = atom_to_elem[aid]
        wfs = atom_to_wfs[aid]
        evals, evecs = diag_local(H0, wfs)
        evals_rel = evals - args.Ef

        # basis orbital label list for this atom (same length as wfs)
        basis_orbs = [wf_orb[wf] for wf in wfs]

        for j in range(len(evals_rel)):
            c2 = np.abs(evecs[:, j]) ** 2
            # orbital weight = sum |c_i|^2 over basis functions with same orb label
            orb_w = {}
            for i, orb in enumerate(basis_orbs):
                orb_w[orb] = orb_w.get(orb, 0.0) + float(c2[i])
            dom_orb = max(orb_w.keys(), key=lambda k: orb_w[k])
            dom_w = orb_w[dom_orb]

            rows_levels.append({
                "atom_id": aid,
                "elem": elem,
                "n_wf": len(wfs),
                "level_index": j + 1,
                "E_rel": float(evals_rel[j]),
                "dominant_orb": dom_orb,
                "dominant_w": float(dom_w)
            })

            key = (aid, elem, dom_orb)
            lowest[key] = min(lowest.get(key, 1e30), float(evals_rel[j]))

        # pair diagnostics
        pairs = try_pairs(evals_rel)
        for pi, (center, split, i1, i2) in enumerate(pairs, start=1):
            warn = int(split > args.pair_tol)
            # label by combined weights
            c2 = (np.abs(evecs[:, i1]) ** 2 + np.abs(evecs[:, i2]) ** 2)
            orb_w = {}
            for i, orb in enumerate(basis_orbs):
                orb_w[orb] = orb_w.get(orb, 0.0) + float(c2[i])
            dom_orb = max(orb_w.keys(), key=lambda k: orb_w[k])
            dom_w = orb_w[dom_orb]

            rows_pairs.append({
                "atom_id": aid,
                "elem": elem,
                "n_wf": len(wfs),
                "pair_index": pi,
                "center_rel": float(center),
                "split": float(split),
                "dominant_orb_pair": dom_orb,
                "dominant_w_pair": float(dom_w),
                "warn_split_gt_tol": warn
            })

    pd.DataFrame(rows_levels).to_csv(out_levels, index=False)
    pd.DataFrame(rows_pairs).to_csv(out_pairs, index=False)

    out_rows = []
    for (aid, elem, orb), E0 in sorted(lowest.items(), key=lambda x: (x[0][1], x[0][0], x[1], x[0][2])):
        out_rows.append({
            "atom_id": aid, "elem": elem, "orbital": orb,
            "lowest_E_rel": float(E0)
        })
    pd.DataFrame(out_rows).to_csv(out_order, index=False)

    print("[OK] Done.")
    print(f"  spinor_dup = {spinor_dup}")
    print(f"  wrote: {out_levels}")
    print(f"  wrote: {out_pairs}")
    print(f"  wrote: {out_order}")
    print("Notes:")
    print("  - This route labels orbitals from win projection expansion order; it is stable and does NOT depend on chk/amn.")
    print("  - If you are FM+SOC (TR broken), pair_centers are diagnostic only; do not treat them as Kramers by default.")


if __name__ == "__main__":
    main()


```

使用方法：

```shell
python cf_final_hr_win_edges.py --Ef -1.4631 --out_prefix cfA2
```

(2) 第二步


这是一个用于分析材料中元素局域能级分布的Python脚本。其核心功能是对前序脚本（如 cf_final hr_win_edges.py）生成的 *_levels.csv文件进行高阶统计分析。

脚本首先对原始数据进行清洗，过滤掉主导轨道标签不可靠的能级（如轨道权重过低或为空），确保分析质量。随后，按“元素-主导轨道”组合进行分组，计算每个组内能级能量的详细统计量，
包括均值、标准差、分位数等。最重要的，脚本会汇总每个元素在不同轨道上的中位数能级，并通过计算其极差，生成一个衡量“晶场分裂强度”或“轨道能级展开”的实用摘要。

最终，脚本会输出三个CSV文件：一份包含所有有效能级的详细长表、一份每个元素-轨道组合的统计量表，以及一份汇总了各元素轨道能级展开范围的核心摘要，从而将原子尺度的复杂数据提炼
为元素层面的清晰物理图像。

```python

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
from pathlib import Path
import pandas as pd
import numpy as np


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--levels", default="cfA2_levels.csv", help="*_levels.csv from the CF script")
    ap.add_argument("--out_prefix", default="cf_elem", help="output prefix")
    ap.add_argument("--min_dom_w", type=float, default=0.20,
                    help="discard levels whose dominant_w < this threshold (label too mixed)")
    args = ap.parse_args()

    levels_path = Path(args.levels)
    if not levels_path.exists():
        raise FileNotFoundError(levels_path.resolve())

    df = pd.read_csv(levels_path)

    need = {"atom_id", "elem", "n_wf", "level_index", "E_rel", "dominant_orb", "dominant_w"}
    miss = need - set(df.columns)
    if miss:
        raise RuntimeError(f"levels file missing columns: {sorted(list(miss))}")

    # clean
    df["dominant_orb"] = df["dominant_orb"].astype(str).str.strip()
    df = df[df["dominant_orb"] != ""].copy()

    # filter mixed labels
    df_f = df[df["dominant_w"] >= args.min_dom_w].copy()

    # long table output
    long_out = f"{args.out_prefix}_elem_orb_levels_long.csv"
    df_f.to_csv(long_out, index=False)

    # base stats
    stats = (
        df_f.groupby(["elem", "dominant_orb"])["E_rel"]
        .agg(count="count", mean="mean", std="std", min="min", max="max")
        .reset_index()
    )

    # quantiles (robust way)
    qs = (
        df_f.groupby(["elem", "dominant_orb"])["E_rel"]
        .quantile([0.25, 0.50, 0.75])
        .unstack(level=-1)
        .reset_index()
        .rename(columns={0.25: "q25", 0.50: "q50", 0.75: "q75"})
    )

    stats = stats.merge(qs, on=["elem", "dominant_orb"], how="left")

    # per-element splitting summary (range of orbital medians)
    med = stats.pivot_table(index="elem", columns="dominant_orb", values="q50", aggfunc="first")

    elem_split = []
    for elem in med.index:
        vals = med.loc[elem].dropna().to_numpy()
        if len(vals) == 0:
            continue
        elem_split.append({
            "elem": elem,
            "orbital_median_range": float(vals.max() - vals.min()),
            "orbital_median_min": float(vals.min()),
            "orbital_median_max": float(vals.max()),
            "n_orbitals": int(len(vals))
        })
    elem_split_df = pd.DataFrame(elem_split).sort_values(["elem"])

    # write outputs
    stats_out = f"{args.out_prefix}_elem_orb_stats.csv"
    split_out = f"{args.out_prefix}_elem_splitting_summary.csv"

    stats.sort_values(["elem", "dominant_orb"]).to_csv(stats_out, index=False)
    elem_split_df.to_csv(split_out, index=False)

    print("[OK] wrote:")
    print(" ", long_out)
    print(" ", stats_out)
    print(" ", split_out)
    print(f"[INFO] kept {len(df_f)} / {len(df)} levels (min_dom_w={args.min_dom_w})")

    if len(elem_split_df) > 0:
        print("\n[Summary] per-element orbital median range (q50 max-min):")
        for _, r in elem_split_df.iterrows():
            print(f"  {r['elem']}: range={r['orbital_median_range']:.4f} eV over {r['n_orbitals']} orbitals")


if __name__ == "__main__":
    main()


```


使用方法：

```python

python collect_elem_cf_stats.py --levels cfA2_levels.csv --out_prefix elemCF --min_dom_w 0.20

```

(3) 第三步

该脚本（cfFINAL_export_fullweights.py）在材料电子结构分析中实现了关键增强。它首先读取三个必要文件：wannier90_hr.dat（构建on-site哈密顿量）、edges.csv（建立Wannier
函数到原子的唯一映射）和wannier90.win（确定投影轨道顺序），并严格校验WF顺序与投影顺序的一致性以保证后续标签准确。

其核心计算分为两步：对每个原子的on-site哈密顿量子块进行对角化，得到局域能级；然后，对每条能级的本征态，计算其在所有基轨道（如px、py、pz、dxy等）上的详细权重分布。这是相
较于旧版工具的最大提升——旧版仅输出主导轨道，而新版会导出所有轨道的精确占比。

脚本最终输出三个CSV文件，其中最重要的*_levels_fullweights.csv文件完整记录了每条能级的全轨道权重。这使得用户可以直接提取特定元素（如硒Se）的所有px、py、pz轨道权重，进
而进行加权的分布统计、绘制能级展开图等深入分析，而不再局限于之前仅基于主导轨道的简化统计。


```python

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
from pathlib import Path
import numpy as np
import pandas as pd


def parse_hr_dat(hr_path: Path):
    lines = hr_path.read_text(encoding="utf-8", errors="ignore").splitlines()
    num_wann = int(lines[1].strip())
    nrpts = int(lines[2].strip())
    degen = []
    idx = 3
    while len(degen) < nrpts:
        degen.extend([int(t) for t in lines[idx].split()])
        idx += 1
    rec_lines = lines[idx:]
    return num_wann, rec_lines


def build_H0(num_wann, rec_lines):
    H0 = np.zeros((num_wann, num_wann), dtype=np.complex128)
    for ln in rec_lines:
        if not ln.strip():
            continue
        toks = ln.split()
        if len(toks) < 7:
            continue
        Rx, Ry, Rz = int(toks[0]), int(toks[1]), int(toks[2])
        if (Rx, Ry, Rz) != (0, 0, 0):
            continue
        m = int(toks[3]) - 1
        n = int(toks[4]) - 1
        H0[m, n] = float(toks[5]) + 1j * float(toks[6])
    return 0.5 * (H0 + H0.conj().T)


def read_edges_wf_map(edges_path: Path):
    df = pd.read_csv(edges_path)
    needed = ["m", "n", "atom_m", "atom_n", "elem_m", "elem_n"]
    for c in needed:
        if c not in df.columns:
            raise RuntimeError(f"edges.csv missing required column: {c}")

    wf_to_pairs = {}
    for _, r in df.iterrows():
        m = int(r["m"]); n = int(r["n"])
        am = int(r["atom_m"]); an = int(r["atom_n"])
        em = str(r["elem_m"]); en = str(r["elem_n"])
        wf_to_pairs.setdefault(m, set()).add((am, em))
        wf_to_pairs.setdefault(n, set()).add((an, en))

    bad = []
    wf_map = {}
    for wf, s in wf_to_pairs.items():
        if len(s) != 1:
            bad.append((wf, sorted(list(s))))
        else:
            wf_map[wf] = next(iter(s))
    if bad:
        msg = ["[ERROR] WF->(atom,elem) inconsistent in edges.csv"]
        for wf, pairs in bad[:30]:
            msg.append(f"  wf={wf}: {pairs}")
        raise RuntimeError("\n".join(msg))
    return wf_map


def atoms_by_element(wf_map, num_wann):
    elem_to_atoms = {}
    for wf in range(1, num_wann + 1):
        aid, elem = wf_map[wf]
        elem_to_atoms.setdefault(elem, set()).add(aid)
    return {e: sorted(list(s)) for e, s in elem_to_atoms.items()}


def norm_orb(tok: str) -> str:
    t = tok.strip().lower()
    t = t.replace("dx2y2", "dx2-y2").replace("dx2_y2", "dx2-y2")
    t = t.replace("d(z2)", "dz2").replace("d(z^2)", "dz2").replace("d(z**2)", "dz2")
    t = t.replace("d(x2-y2)", "dx2-y2")
    return t


def expand_token(tok: str):
    t = norm_orb(tok)
    if t == "d":
        return ["dxy", "dyz", "dxz", "dz2", "dx2-y2"]
    if t == "p":
        return ["px", "py", "pz"]
    if t == "s":
        return ["s"]
    return [t]


def parse_win_projections(win_path: Path):
    lines = win_path.read_text(encoding="utf-8", errors="ignore").splitlines()
    in_proj = False
    proj_lines = []
    for ln in lines:
        s = ln.strip()
        low = s.lower()
        if low.startswith("begin projections"):
            in_proj = True
            continue
        if low.startswith("end projections"):
            in_proj = False
            continue
        if not in_proj:
            continue
        if (not s) or s.startswith("#") or s.startswith("!"):
            continue
        proj_lines.append(s)

    out = []
    for raw in proj_lines:
        if ":" not in raw:
            continue
        elem, rhs = raw.split(":", 1)
        elem = elem.strip()
        rhs = rhs.replace(",", " ").replace(";", " ")
        toks = [t for t in rhs.split() if t.strip()]
        orbs = []
        for t in toks:
            orbs += expand_token(t)
        out.append((elem, orbs))
    if not out:
        raise RuntimeError("[ERROR] No projections in win.")
    return out


def build_expected_labels(proj_spec, elem_to_atoms, num_wann):
    spatial = []
    for elem, orbs in proj_spec:
        if elem not in elem_to_atoms:
            raise RuntimeError(f"[ERROR] elem {elem} in win not found in edges-derived elements {list(elem_to_atoms.keys())}")
        for aid in elem_to_atoms[elem]:
            for orb in orbs:
                spatial.append((aid, elem, orb))
    if len(spatial) == num_wann:
        return spatial, False
    if 2 * len(spatial) == num_wann:
        spinor = []
        for x in spatial:
            spinor.append(x); spinor.append(x)
        return spinor, True
    raise RuntimeError(f"[ERROR] projection count mismatch: spatial={len(spatial)} num_wann={num_wann}")


def diag_local(H0, wf_list_1based):
    idx = [w - 1 for w in wf_list_1based]
    sub = H0[np.ix_(idx, idx)]
    evals, evecs = np.linalg.eigh(sub)
    order = np.argsort(np.real(evals))
    return np.real(evals[order]), evecs[:, order]


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--hr", default="wannier90_hr.dat")
    ap.add_argument("--edges", default="edges.csv")
    ap.add_argument("--win", default="wannier90.win")
    ap.add_argument("--Ef", type=float, default=0.0)
    ap.add_argument("--out_prefix", default="cfFW")
    args = ap.parse_args()

    hr_path = Path(args.hr)
    edges_path = Path(args.edges)
    win_path = Path(args.win)
    for p in [hr_path, edges_path, win_path]:
        if not p.exists():
            raise FileNotFoundError(p.resolve())

    num_wann, rec_lines = parse_hr_dat(hr_path)
    H0 = build_H0(num_wann, rec_lines)

    wf_map = read_edges_wf_map(edges_path)
    elem_to_atoms = atoms_by_element(wf_map, num_wann)

    proj_spec = parse_win_projections(win_path)
    expected_labels, spinor_dup = build_expected_labels(proj_spec, elem_to_atoms, num_wann)

    # verify WF ordering matches expected atom labels
    mism = []
    for wf in range(1, num_wann + 1):
        aid_true, elem_true = wf_map[wf]
        aid_e, elem_e, _ = expected_labels[wf - 1]
        if (aid_true != aid_e) or (elem_true != elem_e):
            mism.append((wf, (aid_true, elem_true), (aid_e, elem_e)))
    if mism:
        print("[WARN] WF ordering mismatch with win expansion ordering; cannot label safely.")
        for it in mism[:20]:
            print("  wf", it[0], "edges", it[1], "expected", it[2])
        raise RuntimeError("WF ordering mismatch; stop.")

    # WF -> orbital label (basis orbital)
    wf_orb = {wf: expected_labels[wf - 1][2] for wf in range(1, num_wann + 1)}

    # group WFs by atom
    atom_to_wfs = {}
    atom_to_elem = {}
    for wf in range(1, num_wann + 1):
        aid, elem = wf_map[wf]
        atom_to_wfs.setdefault(aid, []).append(wf)
        atom_to_elem[aid] = elem
    for aid in atom_to_wfs:
        atom_to_wfs[aid] = sorted(atom_to_wfs[aid])

    # outputs
    out_levels = f"{args.out_prefix}_levels.csv"
    out_full = f"{args.out_prefix}_levels_fullweights.csv"
    out_order = f"{args.out_prefix}_orbital_order.csv"

    rows_levels = []
    rows_full = []
    lowest = {}

    for aid in sorted(atom_to_wfs.keys()):
        elem = atom_to_elem[aid]
        wfs = atom_to_wfs[aid]
        evals, evecs = diag_local(H0, wfs)
        evals_rel = evals - args.Ef

        basis_orbs = [wf_orb[wf] for wf in wfs]
        unique_orbs = sorted(list(set(basis_orbs)))

        for j in range(len(evals_rel)):
            c2 = np.abs(evecs[:, j])**2
            orb_w = {o: 0.0 for o in unique_orbs}
            for i, o in enumerate(basis_orbs):
                orb_w[o] += float(c2[i])

            dom_orb = max(orb_w.keys(), key=lambda k: orb_w[k])
            dom_w = orb_w[dom_orb]

            rows_levels.append({
                "atom_id": aid, "elem": elem, "n_wf": len(wfs),
                "level_index": j+1, "E_rel": float(evals_rel[j]),
                "dominant_orb": dom_orb, "dominant_w": float(dom_w)
            })

            r = {"atom_id": aid, "elem": elem, "n_wf": len(wfs),
                 "level_index": j+1, "E_rel": float(evals_rel[j])}
            # add all orbital weights as columns
            for o in unique_orbs:
                r[f"w_{o}"] = float(orb_w[o])
            rows_full.append(r)

            key = (aid, elem, dom_orb)
            lowest[key] = min(lowest.get(key, 1e30), float(evals_rel[j]))

    pd.DataFrame(rows_levels).to_csv(out_levels, index=False)
    pd.DataFrame(rows_full).to_csv(out_full, index=False)

    out_rows = []
    for (aid, elem, orb), E0 in sorted(lowest.items(), key=lambda x: (x[0][1], x[0][0], x[0][2])):
        out_rows.append({"atom_id": aid, "elem": elem, "orbital": orb, "lowest_E_rel": float(E0)})
    pd.DataFrame(out_rows).to_csv(out_order, index=False)

    print("[OK] wrote:")
    print(" ", out_levels)
    print(" ", out_full)
    print(" ", out_order)
    print("[INFO] spinor_dup =", spinor_dup)


if __name__ == "__main__":
    main()


```

```shell

python cfFINAL_export_fullweights.py --out_prefix cfFW

```

(4) 第四步


collect_elem_orbital_distributions.py脚本是用于分析材料中元素轨道能量分布的专业工具。其核心功能是基于包含全轨道权重的能级数据（*_levels_fullweights.csv），对每个元
素的各个轨道（如 Se 的 px、py、pz）进行加权统计，这完美解决了你之前提出的“分析Se的px/py/pz整体分布”的需求。

具体而言，它对每个元素和轨道（如 w_px）执行以下计算：首先，将每条能级的相对能量 E_rel视为样本值，并将其在该轨道上的权重 w_orb作为样本权重；接着，过滤掉权重过小的噪声数
据；然后，计算加权平均值、标准差、分位数等统计量；最后，生成加权的能量分布直方图数据。

脚本默认输出两个CSV文件：一个汇总每个元素-轨道组合的加权统计数据（*_weighted_stats.csv），另一个则提供用于绘制分布曲线或热图的直方图数据点（*_hist.csv）。

与此前按主导轨道进行“硬分类”的脚本 collect_elem_cf_stats.py不同，本脚本利用全轨道权重进行“软分解”统计，能更精确地刻画每个轨道对整体能级的贡献分布。

```python

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
from pathlib import Path
import numpy as np
import pandas as pd


def weighted_quantile(values, weights, qs):
    """values, weights: 1D arrays; qs in [0,1]"""
    values = np.asarray(values, dtype=float)
    weights = np.asarray(weights, dtype=float)
    m = (weights > 0) & np.isfinite(values)
    values = values[m]; weights = weights[m]
    if len(values) == 0:
        return [np.nan for _ in qs]
    s = np.argsort(values)
    v = values[s]; w = weights[s]
    cw = np.cumsum(w)
    cw /= cw[-1]
    return [float(np.interp(q, cw, v)) for q in qs]


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--full", default="cfFW_levels_fullweights.csv",
                    help="*_levels_fullweights.csv")
    ap.add_argument("--out_prefix", default="elemDist")
    ap.add_argument("--bins", type=int, default=60)
    ap.add_argument("--emin", type=float, default=None)
    ap.add_argument("--emax", type=float, default=None)
    ap.add_argument("--min_weight", type=float, default=1e-6,
                    help="discard contributions with orbital weight < this")
    args = ap.parse_args()

    p = Path(args.full)
    if not p.exists():
        raise FileNotFoundError(p.resolve())

    df = pd.read_csv(p)

    # find weight columns
    wcols = [c for c in df.columns if c.startswith("w_")]
    if not wcols:
        raise RuntimeError("No w_* columns found. Use cfFINAL_export_fullweights.py first.")

    # energy range
    E = df["E_rel"].to_numpy(dtype=float)
    emin = np.nanmin(E) if args.emin is None else args.emin
    emax = np.nanmax(E) if args.emax is None else args.emax
    edges = np.linspace(emin, emax, args.bins + 1)

    stats_rows = []
    hist_rows = []

    # for each element + orbital: treat each level contributes weight w_orb at energy E
    for elem in sorted(df["elem"].unique()):
        sub = df[df["elem"] == elem].copy()
        Es = sub["E_rel"].to_numpy(dtype=float)

        for wc in wcols:
            orb = wc[2:]  # remove "w_"
            ws = sub[wc].to_numpy(dtype=float)

            m = ws >= args.min_weight
            if m.sum() == 0:
                continue

            v = Es[m]
            w = ws[m]

            wsum = float(w.sum())
            mean = float(np.sum(w * v) / wsum)
            var = float(np.sum(w * (v - mean)**2) / wsum)
            std = float(np.sqrt(var))

            q25, q50, q75 = weighted_quantile(v, w, [0.25, 0.50, 0.75])
            vmin = float(np.min(v))
            vmax = float(np.max(v))

            stats_rows.append({
                "elem": elem, "orbital": orb,
                "weight_sum": wsum,
                "wmean_E": mean,
                "wstd_E": std,
                "wmin_E": vmin,
                "wmax_E": vmax,
                "wq25_E": q25,
                "wq50_E": q50,
                "wq75_E": q75
            })

            # weighted histogram
            hist, _ = np.histogram(v, bins=edges, weights=w)
            centers = 0.5 * (edges[:-1] + edges[1:])
            for x, h in zip(centers, hist):
                hist_rows.append({
                    "elem": elem, "orbital": orb,
                    "E_center": float(x),
                    "weighted_count": float(h)
                })

    stats_df = pd.DataFrame(stats_rows).sort_values(["elem", "orbital"])
    hist_df = pd.DataFrame(hist_rows).sort_values(["elem", "orbital", "E_center"])

    out_stats = f"{args.out_prefix}_elem_orb_weighted_stats.csv"
    out_hist = f"{args.out_prefix}_elem_orb_hist.csv"
    stats_df.to_csv(out_stats, index=False)
    hist_df.to_csv(out_hist, index=False)

    print("[OK] wrote:")
    print(" ", out_stats)
    print(" ", out_hist)
    print(f"[INFO] energy range: [{emin:.6f}, {emax:.6f}] eV, bins={args.bins}")


if __name__ == "__main__":
    main()


```

使用方法：

```shell

python collect_elem_orbital_distributions.py --full cfFW_levels_fullweights.csv --out_prefix elemCFdist --bins 60.

```


(5) 第五步 绘图

配套的绘图工具，用于将前序统计分析的结果进行可视化。

plot_elem_level_diagram.py​ 用于绘制元素的轨道能级示意图。它读取包含加权统计量（如中位数、四分位数）的文件，为指定元素的每个轨道绘制一条以加权中位能量为中心的水平线
段，并可选择添加四分位距区间以示意能级分布宽度。其产出类似一张简明的“能级图”，直观展示各轨道能量的中心位置与离散程度。

plot_elem_orb_distributions.py​ 则用于绘制元素-轨道的能量分布曲线图。它基于直方图数据文件，为指定元素的每个轨道绘制一条完整的分布曲线，横轴为能量，纵轴为加权强度，从而
完整呈现每个轨道在能量空间中的分布形状、峰位和展宽。

两者的核心区别在于信息压缩程度：前者将每个轨道的信息压缩为一个代表性能级（可带误差棒），产出示意图；后者则保留并展示完整的分布轮廓，产出细节曲线图。它们从不同维度呈现同一
组数据，共同服务于对材料中元素局域电子结构的深入理解与展示。

```python

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


def orbital_sort_key(orb):
    # order p then d if mixed; within sets use common order
    p_order = ["px", "py", "pz"]
    d_order = ["dxy", "dyz", "dxz", "dz2", "dx2-y2"]
    if orb in p_order:
        return (0, p_order.index(orb))
    if orb in d_order:
        return (1, d_order.index(orb))
    return (2, orb)


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--stats", default="elemCFdist_elem_orb_weighted_stats.csv")
    ap.add_argument("--elem", required=True, help="Element symbol, e.g. Se / Tc / Ir / Ge")
    ap.add_argument("--out", default=None, help="Output image path (png/pdf). If omitted, auto name.")
    ap.add_argument("--title", default=None)
    ap.add_argument("--band", action="store_true",
                    help="Draw q25-q75 interval as a vertical band around the median energy.")
    ap.add_argument("--ymin", type=float, default=None)
    ap.add_argument("--ymax", type=float, default=None)
    ap.add_argument("--dpi", type=int, default=300)
    args = ap.parse_args()

    p = Path(args.stats)
    if not p.exists():
        raise FileNotFoundError(p.resolve())

    df = pd.read_csv(p)
    need = {"elem", "orbital", "wq25_E", "wq50_E", "wq75_E", "weight_sum"}
    miss = need - set(df.columns)
    if miss:
        raise RuntimeError(f"Missing columns in stats csv: {sorted(list(miss))}")

    sub = df[df["elem"].astype(str).str.strip() == args.elem].copy()
    if len(sub) == 0:
        raise RuntimeError(f"No rows for elem={args.elem} in {p.name}")

    sub["orbital"] = sub["orbital"].astype(str).str.strip()
    sub = sub.sort_values("orbital", key=lambda s: s.map(orbital_sort_key))

    orbs = sub["orbital"].tolist()
    y50 = sub["wq50_E"].to_numpy(float)
    y25 = sub["wq25_E"].to_numpy(float)
    y75 = sub["wq75_E"].to_numpy(float)

    # x positions: 1..N
    xs = np.arange(1, len(orbs) + 1)

    plt.figure(figsize=(4.8, 5.5))

    # draw median level as horizontal line segments
    for x, e in zip(xs, y50):
        plt.hlines(e, x - 0.35, x + 0.35, linewidth=2)

    # optional: draw q25-q75 as vertical band
    if args.band:
        for x, lo, hi in zip(xs, y25, y75):
            plt.vlines(x, lo, hi, linewidth=2, alpha=0.7)

    plt.xticks(xs, orbs, rotation=0)
    plt.ylabel("Energy (E_rel) [eV]")

    ttl = args.title if args.title else f"{args.elem}: orbital level positions (weighted q50)"
    plt.title(ttl)

    if args.ymin is not None or args.ymax is not None:
        plt.ylim(args.ymin, args.ymax)

    plt.tight_layout()

    out = args.out if args.out else f"{args.elem}_level_diagram.png"
    plt.savefig(out, dpi=args.dpi)
    print("[OK] saved:", out)


if __name__ == "__main__":
    main()


```

```python

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


def orbital_sort_key(orb):
    p_order = ["px", "py", "pz"]
    d_order = ["dxy", "dyz", "dxz", "dz2", "dx2-y2"]
    if orb in p_order:
        return (0, p_order.index(orb))
    if orb in d_order:
        return (1, d_order.index(orb))
    return (2, orb)


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--hist", default="elemCFdist_elem_orb_hist.csv")
    ap.add_argument("--elem", required=True)
    ap.add_argument("--out", default=None)
    ap.add_argument("--title", default=None)
    ap.add_argument("--normalize", action="store_true",
                    help="Normalize each orbital curve by its total area (sum of weighted_count).")
    ap.add_argument("--xmin", type=float, default=None)
    ap.add_argument("--xmax", type=float, default=None)
    ap.add_argument("--dpi", type=int, default=300)
    args = ap.parse_args()

    p = Path(args.hist)
    if not p.exists():
        raise FileNotFoundError(p.resolve())

    df = pd.read_csv(p)
    need = {"elem", "orbital", "E_center", "weighted_count"}
    miss = need - set(df.columns)
    if miss:
        raise RuntimeError(f"Missing columns in hist csv: {sorted(list(miss))}")

    sub = df[df["elem"].astype(str).str.strip() == args.elem].copy()
    if len(sub) == 0:
        raise RuntimeError(f"No rows for elem={args.elem} in {p.name}")

    sub["orbital"] = sub["orbital"].astype(str).str.strip()

    orbs = sorted(sub["orbital"].unique(), key=orbital_sort_key)

    plt.figure(figsize=(5.6, 4.8))

    for orb in orbs:
        s = sub[sub["orbital"] == orb].sort_values("E_center")
        x = s["E_center"].to_numpy(float)
        y = s["weighted_count"].to_numpy(float)

        if args.normalize:
            area = float(np.sum(y))
            if area > 0:
                y = y / area

        plt.plot(x, y, linewidth=1.8, label=orb)

    plt.xlabel("Energy (E_rel) [eV]")
    plt.ylabel("Weighted intensity" + (" (normalized)" if args.normalize else ""))

    ttl = args.title if args.title else f"{args.elem}: orbital-weighted energy distributions"
    plt.title(ttl)

    if args.xmin is not None or args.xmax is not None:
        plt.xlim(args.xmin, args.xmax)

    plt.legend()
    plt.tight_layout()

    out = args.out if args.out else f"{args.elem}_orbital_distributions.png"
    plt.savefig(out, dpi=args.dpi)
    print("[OK] saved:", out)


if __name__ == "__main__":
    main()


```

使用方法
```shell

python plot_elem_level_diagram.py --stats elemCFdist_elem_orb_weighted_stats.csv --elem Se --out Se_levels.png
python plot_elem_level_diagram.py --stats elemCFdist_elem_orb_weighted_stats.csv --elem Tc --out Tc_levels.png
python plot_elem_level_diagram.py --stats elemCFdist_elem_orb_weighted_stats.csv --elem Ir --out Ir_levels.png
python plot_elem_level_diagram.py --stats elemCFdist_elem_orb_weighted_stats.csv --elem Ge --out Ge_levels.png


python plot_elem_orb_distributions.py --hist elemCFdist_elem_orb_hist.csv --elem Se --out Se_dist.png
python plot_elem_orb_distributions.py --hist elemCFdist_elem_orb_hist.csv --elem Tc --out Tc_dist.png
python plot_elem_orb_distributions.py --hist elemCFdist_elem_orb_hist.csv --elem Ir --out Ir_dist.png
python plot_elem_orb_distributions.py --hist elemCFdist_elem_orb_hist.csv --elem Ge --out Ge_dist.png

```


(6)第六步 旋转坐标系

第五步的脚本相对于eg和t2g这种经典情况理论上是适用的，因为没有旋转局域坐标系，我的材料并不是通常的畸变情况，所以不适用，因此下方脚本是针对这个问题而写的。

```python

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
from pathlib import Path
import numpy as np
import pandas as pd


# ============================================================
# 1) POSCAR parser (VASP5/6)
# ============================================================
def read_poscar(poscar_path: Path):
    lines = poscar_path.read_text(encoding="utf-8", errors="ignore").splitlines()
    if len(lines) < 8:
        raise RuntimeError("POSCAR too short")

    comment = lines[0].strip()
    scale = float(lines[1].split()[0])
    lattice = np.array([[float(x) for x in lines[i].split()[:3]] for i in range(2, 5)], dtype=float) * scale

    # VASP5+: line 5 symbols, line 6 counts
    # VASP4: line 5 counts directly
    tokens5 = lines[5].split()
    tokens6 = lines[6].split()

    def is_all_int(toks):
        try:
            for t in toks:
                int(t)
            return True
        except Exception:
            return False

    if is_all_int(tokens5):
        # VASP4 style: no symbols line
        species = [f"X{i+1}" for i in range(len(tokens5))]
        counts = [int(x) for x in tokens5]
        coord_line = 6
    else:
        species = tokens5
        counts = [int(x) for x in tokens6]
        coord_line = 7

    coord_type = lines[coord_line].strip().lower()
    if coord_type.startswith("s"):
        # selective dynamics present
        coord_line += 1
        coord_type = lines[coord_line].strip().lower()

    direct = coord_type.startswith("d")

    n_atoms = sum(counts)
    start = coord_line + 1
    coord_lines = lines[start:start + n_atoms]
    if len(coord_lines) < n_atoms:
        raise RuntimeError("POSCAR missing atomic coordinates")

    frac = []
    for ln in coord_lines:
        toks = ln.split()
        frac.append([float(toks[0]), float(toks[1]), float(toks[2])])
    frac = np.array(frac, dtype=float)

    if not direct:
        # Cartesian -> convert to fractional
        # cart = frac * scale? already included scale in lattice, so treat as Å
        cart = frac.copy()
        frac = cart @ np.linalg.inv(lattice)

    # element per atom list (1-based atom_id)
    elems = []
    for sp, ct in zip(species, counts):
        elems += [sp] * ct

    return {
        "comment": comment,
        "lattice": lattice,   # (3,3) row vectors in Å
        "frac": frac,         # (N,3)
        "elems": elems,       # length N, 1-based id -> elems[id-1]
        "species": species,
        "counts": counts
    }


def frac_to_cart(frac, lattice):
    # lattice row vectors: a,b,c
    return frac @ lattice


def min_image_frac(dfrac):
    # wrap to [-0.5, 0.5)
    return dfrac - np.round(dfrac)


# ============================================================
# 2) hr.dat onsite H(R=0)
# ============================================================
def parse_hr_dat(hr_path: Path):
    lines = hr_path.read_text(encoding="utf-8", errors="ignore").splitlines()
    if len(lines) < 4:
        raise RuntimeError("hr.dat too short")
    num_wann = int(lines[1].strip())
    nrpts = int(lines[2].strip())

    degen = []
    idx = 3
    while len(degen) < nrpts:
        degen.extend([int(t) for t in lines[idx].split()])
        idx += 1
    rec_lines = lines[idx:]
    return num_wann, rec_lines


def build_H0(num_wann, rec_lines):
    H0 = np.zeros((num_wann, num_wann), dtype=np.complex128)
    for ln in rec_lines:
        if not ln.strip():
            continue
        toks = ln.split()
        if len(toks) < 7:
            continue
        Rx, Ry, Rz = int(toks[0]), int(toks[1]), int(toks[2])
        if (Rx, Ry, Rz) != (0, 0, 0):
            continue
        m = int(toks[3]) - 1
        n = int(toks[4]) - 1
        H0[m, n] = float(toks[5]) + 1j * float(toks[6])
    return 0.5 * (H0 + H0.conj().T)


# ============================================================
# 3) edges.csv WF -> (atom_id, elem)
# ============================================================
def read_edges_wf_map(edges_path: Path):
    df = pd.read_csv(edges_path)
    needed = ["m", "n", "atom_m", "atom_n", "elem_m", "elem_n"]
    for c in needed:
        if c not in df.columns:
            raise RuntimeError(f"edges.csv missing column: {c}")

    wf_to_pairs = {}
    for _, r in df.iterrows():
        m = int(r["m"]); n = int(r["n"])
        am = int(r["atom_m"]); an = int(r["atom_n"])
        em = str(r["elem_m"]); en = str(r["elem_n"])
        wf_to_pairs.setdefault(m, set()).add((am, em))
        wf_to_pairs.setdefault(n, set()).add((an, en))

    wf_map = {}
    bad = []
    for wf, s in wf_to_pairs.items():
        if len(s) != 1:
            bad.append((wf, sorted(list(s))))
        else:
            wf_map[wf] = next(iter(s))
    if bad:
        msg = ["[ERROR] WF labeling inconsistent in edges.csv (one WF -> multiple atom/elem)."]
        for wf, pairs in bad[:30]:
            msg.append(f"  wf={wf}: {pairs}")
        raise RuntimeError("\n".join(msg))
    return wf_map


def atoms_by_element_from_edges(wf_map, num_wann):
    elem_to_atoms = {}
    for wf in range(1, num_wann + 1):
        aid, elem = wf_map[wf]
        elem_to_atoms.setdefault(elem, set()).add(aid)
    return {e: sorted(list(s)) for e, s in elem_to_atoms.items()}


# ============================================================
# 4) parse win projections => expected WF orbital labels
# ============================================================
def norm_orb(tok: str) -> str:
    t = tok.strip().lower()
    t = t.replace("dx2y2", "dx2-y2").replace("dx2_y2", "dx2-y2")
    t = t.replace("d(z2)", "dz2").replace("d(z^2)", "dz2").replace("d(z**2)", "dz2")
    t = t.replace("d(x2-y2)", "dx2-y2")
    return t


def expand_token(tok: str):
    t = norm_orb(tok)
    if t == "d":
        return ["dxy", "dyz", "dxz", "dz2", "dx2-y2"]
    if t == "p":
        return ["px", "py", "pz"]
    if t == "s":
        return ["s"]
    return [t]


def parse_win_projections(win_path: Path):
    lines = win_path.read_text(encoding="utf-8", errors="ignore").splitlines()
    in_proj = False
    proj_lines = []
    for ln in lines:
        s = ln.strip()
        low = s.lower()
        if low.startswith("begin projections"):
            in_proj = True
            continue
        if low.startswith("end projections"):
            in_proj = False
            continue
        if not in_proj:
            continue
        if (not s) or s.startswith("#") or s.startswith("!"):
            continue
        proj_lines.append(s)

    out = []
    for raw in proj_lines:
        if ":" not in raw:
            continue
        elem, rhs = raw.split(":", 1)
        elem = elem.strip()
        rhs = rhs.replace(",", " ").replace(";", " ")
        toks = [t for t in rhs.split() if t.strip()]
        orbs = []
        for t in toks:
            orbs += expand_token(t)
        out.append((elem, orbs))
    if not out:
        raise RuntimeError("[ERROR] No projections found in wannier90.win.")
    return out


def build_expected_labels(proj_spec, elem_to_atoms, num_wann):
    spatial = []
    for elem, orbs in proj_spec:
        if elem not in elem_to_atoms:
            raise RuntimeError(f"[ERROR] elem '{elem}' in win not found in edges-derived elements: {list(elem_to_atoms.keys())}")
        for aid in elem_to_atoms[elem]:
            for orb in orbs:
                spatial.append((aid, elem, orb))

    if len(spatial) == num_wann:
        return spatial, False
    if 2 * len(spatial) == num_wann:
        spinor = []
        for x in spatial:
            spinor.append(x); spinor.append(x)
        return spinor, True

    raise RuntimeError(f"[ERROR] Projection count mismatch: spatial={len(spatial)}, num_wann={num_wann}")


# ============================================================
# 5) Build Tc 5x5 onsite from spinor-duplicated 10x10
#    by averaging over the two duplicates per orbital label.
# ============================================================
D_ORBS = ["dxy", "dyz", "dxz", "dz2", "dx2-y2"]

def build_orbital_indices_for_atom(atom_id, expected_labels, wf_map, target_orbs):
    """
    Returns: dict orb -> list of wf indices (1-based) belonging to atom_id with that orbital label
    Requires WF ordering to match expected_labels (checked).
    """
    orb2wfs = {o: [] for o in target_orbs}
    for wf in range(1, len(expected_labels) + 1):
        aid_true, elem_true = wf_map[wf]
        aid_e, elem_e, orb_e = expected_labels[wf - 1]
        if (aid_true != aid_e) or (elem_true != elem_e):
            # ordering mismatch
            return None
        if aid_true == atom_id and orb_e in orb2wfs:
            orb2wfs[orb_e].append(wf)
    return orb2wfs


def average_subblock(H0, wfs_a, wfs_b):
    # average over all pairs (a,b)
    A = np.array([w - 1 for w in wfs_a], dtype=int)
    B = np.array([w - 1 for w in wfs_b], dtype=int)
    sub = H0[np.ix_(A, B)]
    return np.mean(sub)


def build_H5_for_atom(H0, orb2wfs):
    # 5x5 in orbital-label space
    H5 = np.zeros((5, 5), dtype=np.complex128)
    for i, oi in enumerate(D_ORBS):
        for j, oj in enumerate(D_ORBS):
            wi = orb2wfs.get(oi, [])
            wj = orb2wfs.get(oj, [])
            if len(wi) == 0 or len(wj) == 0:
                raise RuntimeError(f"Missing WF indices for orb {oi} or {oj}. Check win projections.")
            H5[i, j] = average_subblock(H0, wi, wj)
    return 0.5 * (H5 + H5.conj().T)


# ============================================================
# 6) Local axis from octahedron (PCA on neighbor vectors)
# ============================================================
def pca_local_axes(vectors, weight_mode="inv_r2"):
    """
    vectors: (6,3) Cartesian vectors from center to neighbors (Å)
    weight_mode: 'none' | 'inv_r2'
    Return R (3,3) with columns = local x',y',z' in global coords (right-handed).
    """
    V = np.array(vectors, dtype=float)
    r = np.linalg.norm(V, axis=1)
    if weight_mode == "inv_r2":
        w = 1.0 / np.maximum(r*r, 1e-12)
    else:
        w = np.ones_like(r)

    M = np.zeros((3, 3), dtype=float)
    for vi, wi in zip(V, w):
        M += wi * np.outer(vi, vi)

    evals, evecs = np.linalg.eigh(M)  # ascending
    # choose axis order: largest eigenvalue => "principal" axis (often distortion axis)
    order = np.argsort(evals)[::-1]
    evecs = evecs[:, order]

    # enforce right-handed
    if np.linalg.det(evecs) < 0:
        evecs[:, 2] *= -1.0

    return evecs, evals[order]


# ============================================================
# 7) Rotate real d-orbital basis using quadratic-form trick
#    f(x)=x^T Q x. Under x = R x', Q' = R^T Q R.
# ============================================================
def d_quadratic_matrices():
    """
    Real cubic harmonics basis as traceless symmetric matrices Q:
      dxy ~ xy
      dyz ~ yz
      dxz ~ xz
      dx2-y2 ~ x^2 - y^2
      dz2 ~ 2 z^2 - x^2 - y^2  (proportional to 3z^2-r^2)
    """
    Q = {}
    Q["dxy"] = np.array([[0, 0.5, 0],
                         [0.5, 0, 0],
                         [0, 0, 0]], float)
    Q["dyz"] = np.array([[0, 0, 0],
                         [0, 0, 0.5],
                         [0, 0.5, 0]], float)
    Q["dxz"] = np.array([[0, 0, 0.5],
                         [0, 0, 0],
                         [0.5, 0, 0]], float)
    Q["dx2-y2"] = np.array([[1, 0, 0],
                            [0, -1, 0],
                            [0, 0, 0]], float)
    Q["dz2"] = np.array([[-1, 0, 0],
                         [0, -1, 0],
                         [0, 0, 2]], float)
    return Q


def vec_sym_traceless(Qm):
    """
    Map symmetric matrix to 5D vector in our basis space using inner products.
    We'll solve coefficients by least squares against basis matrices.
    """
    # flatten symmetric components (xx,yy,zz,xy,xz,yz) for robustness
    return np.array([Qm[0, 0], Qm[1, 1], Qm[2, 2], Qm[0, 1], Qm[0, 2], Qm[1, 2]], float)


def build_d_rotation_matrix(R):
    """
    Given 3x3 rotation R (columns are local axes in global coords),
    construct 5x5 matrix C such that coefficients transform as:
      v_local = C v_global   (in the D_ORBS order)
    Using Q'_a = R^T Q_a R and project onto basis.
    """
    Q = d_quadratic_matrices()
    B = np.stack([vec_sym_traceless(Q[o]) for o in D_ORBS], axis=1)  # (6,5)
    # solve for each rotated basis matrix
    C = np.zeros((5, 5), float)
    for a, orb in enumerate(D_ORBS):
        Qp = R.T @ Q[orb] @ R   # expressed in local coords
        vp = vec_sym_traceless(Qp)  # (6,)
        # least squares: B c = vp
        c, *_ = np.linalg.lstsq(B, vp, rcond=None)
        C[:, a] = c
    # Orthonormalize C (numerical)
    # Use QR on C^T then transpose back
    Qt, Rt = np.linalg.qr(C.T)
    C_ortho = Qt.T
    return C_ortho


# ============================================================
# 8) Nearest-neighbor search Tc->6 Se (PBC)
# ============================================================
def get_cart_vectors_to_neighbors(frac, lattice, center_idx0, neighbor_indices0):
    """
    center_idx0: 0-based index
    neighbor_indices0: list of 0-based indices
    return vectors (Nn,3) in Cartesian using minimum-image in fractional space
    """
    fc = frac[center_idx0]
    out = []
    for j0 in neighbor_indices0:
        df = frac[j0] - fc
        df = min_image_frac(df)
        out.append(df @ lattice)
    return np.array(out, float)


def find_k_nearest_neighbors(frac, lattice, center_idx0, candidate_idx0, k=6):
    fc = frac[center_idx0]
    dists = []
    for j0 in candidate_idx0:
        df = frac[j0] - fc
        df = min_image_frac(df)
        v = df @ lattice
        d = float(np.linalg.norm(v))
        dists.append((d, j0))
    dists.sort(key=lambda x: x[0])
    return dists[:k]


# ============================================================
# 9) Main
# ============================================================
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--poscar", default="POSCAR")
    ap.add_argument("--hr", default="wannier90_hr.dat")
    ap.add_argument("--edges", default="edges.csv")
    ap.add_argument("--win", default="wannier90.win")
    ap.add_argument("--center", default="Tc")
    ap.add_argument("--neighbor", default="Se")
    ap.add_argument("--center_atom_id", type=int, default=None, help="If set, analyze only this POSCAR atom id (1-based)")
    ap.add_argument("--Ef", type=float, default=0.0, help="Shift energies as E_rel = E - Ef")
    ap.add_argument("--weight_mode", default="inv_r2", choices=["inv_r2", "none"])
    ap.add_argument("--out_prefix", default="TcCF")
    args = ap.parse_args()

    poscar_path = Path(args.poscar)
    hr_path = Path(args.hr)
    edges_path = Path(args.edges)
    win_path = Path(args.win)
    for p in [poscar_path, hr_path, edges_path, win_path]:
        if not p.exists():
            raise FileNotFoundError(p.resolve())

    # POSCAR
    S = read_poscar(poscar_path)
    frac = S["frac"]
    lattice = S["lattice"]
    elems = S["elems"]

    # center/neighbor indices in POSCAR (0-based)
    center_idx0 = [i for i, e in enumerate(elems) if e == args.center]
    neigh_idx0 = [i for i, e in enumerate(elems) if e == args.neighbor]
    if not center_idx0:
        raise RuntimeError(f"No center element '{args.center}' found in POSCAR element list {set(elems)}")
    if not neigh_idx0:
        raise RuntimeError(f"No neighbor element '{args.neighbor}' found in POSCAR element list {set(elems)}")

    if args.center_atom_id is not None:
        i0 = args.center_atom_id - 1
        if i0 < 0 or i0 >= len(elems):
            raise RuntimeError("center_atom_id out of range")
        if elems[i0] != args.center:
            raise RuntimeError(f"center_atom_id={args.center_atom_id} is element {elems[i0]}, not {args.center}")
        center_idx0 = [i0]

    # hr onsite
    num_wann, rec_lines = parse_hr_dat(hr_path)
    H0 = build_H0(num_wann, rec_lines)

    # WF mapping
    wf_map = read_edges_wf_map(edges_path)
    elem_to_atoms_edges = atoms_by_element_from_edges(wf_map, num_wann)
    proj_spec = parse_win_projections(win_path)
    expected_labels, spinor_dup = build_expected_labels(proj_spec, elem_to_atoms_edges, num_wann)

    # Output rows
    rows_axes = []
    rows_levels = []
    rows_neighbors = []

    for c0 in center_idx0:
        atom_id = c0 + 1  # POSCAR 1-based

        # find 6 nearest Se around this Tc
        nn = find_k_nearest_neighbors(frac, lattice, c0, neigh_idx0, k=6)
        nn_ids0 = [j0 for _, j0 in nn]
        nn_d = [d for d, _ in nn]

        # vectors center->Se
        vecs = get_cart_vectors_to_neighbors(frac, lattice, c0, nn_ids0)

        # PCA local axes
        Rloc, evals = pca_local_axes(vecs, weight_mode=args.weight_mode)  # columns: x',y',z' in global coords

        # Rotation in d space (global->local coefficients)
        C = build_d_rotation_matrix(Rloc)  # 5x5 real
        # NOTE: coefficients v_local = C v_global

        # Build Tc H5 (global d basis) from wf_map/expected_labels using atom_id in edges atom indexing.
        # Important: edges atom_id indexing must match POSCAR atom_id (you already established this mapping earlier).
        orb2wfs = build_orbital_indices_for_atom(atom_id, expected_labels, wf_map, D_ORBS)
        if orb2wfs is None:
            raise RuntimeError("WF ordering mismatch vs win expansion; cannot build orbital indices safely.")

        H5g = build_H5_for_atom(H0, orb2wfs)  # 5x5 complex hermitian
        # Rotate to local basis (treat C as real orthonormal): H_local = C H_global C^T
        H5l = C @ H5g @ C.T

        # Diagonalize local 5x5 (energies; eigenvectors in local orbital basis)
        evals_d, evecs_d = np.linalg.eigh(H5l)
        order = np.argsort(np.real(evals_d))
        evals_d = np.real(evals_d[order]) - args.Ef
        evecs_d = evecs_d[:, order]

        # record neighbor table
        for rank, (d, j0) in enumerate(nn, start=1):
            rows_neighbors.append({
                "center_atom_id": atom_id,
                "neighbor_rank": rank,
                "neighbor_atom_id": j0 + 1,
                "neighbor_elem": elems[j0],
                "distance_A": float(d)
            })

        # record axes
        rows_axes.append({
            "center_atom_id": atom_id,
            "center_elem": args.center,
            "pca_eval1": float(evals[0]),
            "pca_eval2": float(evals[1]),
            "pca_eval3": float(evals[2]),
            "xprime_global": ",".join([f"{x:.6f}" for x in Rloc[:, 0]]),
            "yprime_global": ",".join([f"{x:.6f}" for x in Rloc[:, 1]]),
            "zprime_global": ",".join([f"{x:.6f}" for x in Rloc[:, 2]]),
            "nn6_distances_A": ",".join([f"{x:.4f}" for x in nn_d]),
            "nn6_mean_A": float(np.mean(nn_d)),
            "nn6_std_A": float(np.std(nn_d)),
            "nn6_min_A": float(np.min(nn_d)),
            "nn6_max_A": float(np.max(nn_d)),
            "spinor_dup": bool(spinor_dup)
        })

        # record local CF levels and orbital characters in local basis
        for i in range(5):
            v = evecs_d[:, i]
            w = np.abs(v)**2
            dom = int(np.argmax(w))
            rows_levels.append({
                "center_atom_id": atom_id,
                "level_index": i + 1,
                "E_rel_eV": float(evals_d[i]),
                "dom_orb_local": D_ORBS[dom] + "'",
                "w_dxy_p": float(w[0]),
                "w_dyz_p": float(w[1]),
                "w_dxz_p": float(w[2]),
                "w_dz2_p": float(w[3]),
                "w_dx2y2_p": float(w[4]),
            })

    # write outputs
    out_axes = f"{args.out_prefix}_Tc_local_axes.csv"
    out_nn = f"{args.out_prefix}_Tc_Se6_neighbors.csv"
    out_lv = f"{args.out_prefix}_Tc_local_d_levels.csv"

    pd.DataFrame(rows_axes).to_csv(out_axes, index=False)
    pd.DataFrame(rows_neighbors).to_csv(out_nn, index=False)
    pd.DataFrame(rows_levels).to_csv(out_lv, index=False)

    print("[OK] wrote:")
    print(" ", out_axes)
    print(" ", out_nn)
    print(" ", out_lv)
    print("[NOTE]")
    print(" - Energies are from Tc onsite 5x5 effective d Hamiltonian (R=0), rotated into local octahedron axes by PCA.")
    print(" - dom_orb_local is defined in the local axes (x',y',z'), hence meaningful for distorted octahedron CF discussion.")


if __name__ == "__main__":
    main()


```

使用方法：

```shell

python local_cf_octa_Tc.py --poscar POSCAR --hr wannier90_hr.dat --edges edges.csv --win wannier90.win --center Tc --neighbor Se --out_prefix TcCF

python local_cf_octa_Tc.py --poscar POSCAR --hr wannier90_hr.dat --edges edges.csv --win wannier90.win --center Tc --neighbor Se --center_atom_id 2 --out_prefix Tc2CF

```

这个脚本确实有效，但是由于wannier考虑soc是一种较为”混乱“的情况，包含了杂化等等，所以即使旋转了坐标系，也是不能实现和dft计算中的pdos同样的简并情况。

(7)第七步 通过pdos中的d轨道

计算bandcenters，得到准确的轨道能级分布。

输入文件，dft计算dos后，用vaspkit导出的数据文档。

```python

PDOS_Tc_DW.dat
PDOS_Tc_UP.dat

```

代码如下：

```python

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

COLS = ["E","s","py","pz","px","dxy","dyz","dz2","dxz","dx2y2","tot"]
D_COLS = ["dxy","dyz","dz2","dxz","dx2y2"]

GROUPS = [
    ("(dxy, dx2-y2)", ["dxy", "dx2y2"]),
    ("(dz2)",         ["dz2"]),
    ("(dxz, dyz)",    ["dxz", "dyz"]),
]

def load_pdos(path: Path):
    # 第一行是表头（以 # 开头），后面是数据
    df = pd.read_csv(path, sep=r"\s+", comment="#", names=COLS, skiprows=1)
    return df

def trapz(E, Y):
    return np.trapz(Y, E)

def band_center(E, D, Emin, Emax):
    m = (E >= Emin) & (E <= Emax)
    Ew = E[m]; Dw = D[m]
    denom = trapz(Ew, Dw)
    if abs(denom) < 1e-14:
        return np.nan, 0.0
    num = trapz(Ew, Ew * Dw)
    return num/denom, denom

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--up", required=True, help="PDOS_Tc_UP.dat")
    ap.add_argument("--dw", required=True, help="PDOS_Tc_DW.dat (negative for plotting is ok)")
    ap.add_argument("--Emin", type=float, default=-1.0, help="window min (EF=0)")
    ap.add_argument("--Emax", type=float, default=0.0, help="window max (EF=0)")
    ap.add_argument("--out_prefix", default="TcPDOS")
    ap.add_argument("--shift_min_to_zero", action="store_true")
    args = ap.parse_args()

    up = load_pdos(Path(args.up))
    dw = load_pdos(Path(args.dw))

    if not np.allclose(up["E"].values, dw["E"].values, atol=1e-10):
        raise RuntimeError("Energy grids of UP/DW differ; cannot combine directly.")

    E = up["E"].to_numpy(float)

    # DW 文件通常为负号作图，这里取绝对值再与 UP 相加
    D_up = up[D_COLS].to_numpy(float)
    D_dw = np.abs(dw[D_COLS].to_numpy(float))
    D_tot = D_up + D_dw

    # ---- per-orbital stats ----
    orb_rows = []
    for j, orb in enumerate(D_COLS):
        Ec, Nw = band_center(E, D_tot[:, j], args.Emin, args.Emax)
        orb_rows.append({"orb": orb, "E_center": Ec, "N_window": Nw})
    df_orb = pd.DataFrame(orb_rows).sort_values("E_center")
    df_orb.to_csv(f"{args.out_prefix}_orb_centers.csv", index=False)

    # ---- grouped stats ----
    grp_rows = []
    for gname, orbs in GROUPS:
        idx = [D_COLS.index(o) for o in orbs]
        Dg = D_tot[:, idx].sum(axis=1)
        Ec, Nw = band_center(E, Dg, args.Emin, args.Emax)
        grp_rows.append({"group": gname, "orbs": ",".join(orbs), "E_center": Ec, "N_window": Nw})
    df_grp = pd.DataFrame(grp_rows).sort_values("E_center")
    df_grp.to_csv(f"{args.out_prefix}_group_centers.csv", index=False)

    # ---- plot: grouped level diagram ----
    names = df_grp["group"].tolist()
    centers = df_grp["E_center"].to_numpy(float)
    if args.shift_min_to_zero:
        centers = centers - np.min(centers)
        ylab = "PDOS band-center (shifted min→0) [eV]"
        title = f"Tc-d grouped PDOS band-centers  (window {args.Emin}~{args.Emax} eV)"
    else:
        ylab = "PDOS band-center [eV]"
        title = f"Tc-d grouped PDOS band-centers  (window {args.Emin}~{args.Emax} eV)"

    xs = np.arange(1, len(names)+1)
    plt.figure(figsize=(6.2, 4.6))
    for x, e in zip(xs, centers):
        plt.hlines(e, x-0.35, x+0.35, linewidth=3)
        plt.text(x, e, f"{e:+.4f}", ha="center", va="bottom", fontsize=10)

    plt.xticks(xs, names)
    plt.ylabel(ylab)
    plt.title(title)
    ymin, ymax = float(np.min(centers)), float(np.max(centers))
    pad = 0.25 * (ymax - ymin + 1e-12)
    plt.ylim(ymin - pad, ymax + pad)
    plt.tight_layout()
    out_png = f"{args.out_prefix}_group_levels.png"
    plt.savefig(out_png, dpi=300)

    print("[OK] wrote:",
          f"{args.out_prefix}_orb_centers.csv, {args.out_prefix}_group_centers.csv, {out_png}")

    # print summary
    print("\n[Grouped band-centers]")
    print(df_grp.to_string(index=False))

if __name__ == "__main__":
    main()


```

运行代码,能量范围-1到0，价带顶：

```shell

python pdos_cf_from_updw.py --up PDOS_Tc_UP.dat --dw PDOS_Tc_DW.dat --Emin -1.0 --Emax 0.0 --out_prefix TcPDOS

```


