---
layout: post
title: "(原创自研)利用wannier计算超超交换过程中的轨道杂化"
subtitle: "Experience Sharing"
background: '/img/bg-sci-note.jpg'
categories: sci-note
permalink: /sci-note_posts/20260213-dp
---

## <center>说明</center>

为了严谨，参考一篇CPL文章给出的超交换示意图，以及d-p杂化轨道的情况，本篇文章写脚本来提取自己体系的杂化情况。

## <center>代码</center>  

(1) 获取edges.csv


```python

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
1step.py (Index-based robust version)  [SOC-ready + output re/im]

输出 edges CSV 现在包含：
  absH_eV, reH_eV, imH_eV, phase_rad, phase_deg
便于后续做：
  - 轨道杂化 (absH / absH^2)
  - 2×2 SOC spinor block Frobenius norm / singular values (更接近文献“hopping幅度”口径)
"""

import re
import csv
import math
import argparse
from collections import defaultdict, namedtuple
import numpy as np

# -------------------------
# Edge record
# -------------------------
Edge = namedtuple(
    "Edge",
    "pair shell dist absH reH imH phase_rad Rx Ry Rz m n atom_m atom_n elem_m elem_n group_m group_n orb_m orb_n"
)

# =========================
# Parse wannier90.win
# =========================
def _extract_block(txt, block_name):
    m = re.search(rf"begin\s+{block_name}(.*?)end\s+{block_name}", txt, re.S | re.I)
    return None if not m else m.group(1).strip()

def parse_win_lattice_and_atoms(win_path):
    with open(win_path, "r", encoding="utf-8", errors="ignore") as f:
        txt = f.read()

    cell_block = _extract_block(txt, "unit_cell_cart")
    if cell_block is None:
        raise RuntimeError("Cannot find unit_cell_cart block in wannier90.win")

    lines = [ln.strip() for ln in cell_block.splitlines() if ln.strip()]
    if re.match(r"^(ang|angstrom|bohr)\b", lines[0], re.I):
        unit = lines[0].lower()
        vec_lines = lines[1:4]
    else:
        unit = "ang"
        vec_lines = lines[0:3]

    a1 = np.array([float(x) for x in vec_lines[0].split()[:3]], dtype=float)
    a2 = np.array([float(x) for x in vec_lines[1].split()[:3]], dtype=float)
    a3 = np.array([float(x) for x in vec_lines[2].split()[:3]], dtype=float)

    if "bohr" in unit:
        bohr_to_ang = 0.52917721092
        a1 *= bohr_to_ang
        a2 *= bohr_to_ang
        a3 *= bohr_to_ang

    A = np.stack([a1, a2, a3], axis=1)  # columns are lattice vectors

    atoms_block = _extract_block(txt, "atoms_cart")
    if atoms_block is None:
        raise RuntimeError("Cannot find atoms_cart block in wannier90.win")

    atoms = []
    for idx, ln in enumerate([x for x in atoms_block.splitlines() if x.strip()], start=1):
        parts = ln.split()
        if len(parts) < 4:
            continue
        elem = parts[0]
        r = np.array([float(parts[1]), float(parts[2]), float(parts[3])], dtype=float)
        atoms.append({"id": idx, "elem": elem, "r": r})

    if not atoms:
        raise RuntimeError("atoms_cart parsed but got 0 atoms")

    return A, atoms

# =========================
# Parse centres.xyz (WF-only)
# =========================
def parse_wf_centres_from_xyz(xyz_path, natoms, nwann):
    with open(xyz_path, "r", encoding="utf-8", errors="ignore") as f:
        lines = [ln.strip() for ln in f if ln.strip()]

    N = int(lines[0])
    if N < natoms + nwann:
        raise RuntimeError(f"centres.xyz N={N} < natoms+nwann={natoms+nwann}")

    start = 2 + natoms
    end = start + nwann
    if len(lines) < end:
        raise RuntimeError(f"centres.xyz insufficient lines: need >= {end}, got {len(lines)}")

    wf_centers = [None] * (nwann + 1)
    for i in range(nwann):
        parts = lines[start + i].split()
        if len(parts) < 4:
            raise RuntimeError(f"Bad WF centre line: {lines[start+i]}")
        wf_centers[i + 1] = np.array([float(parts[1]), float(parts[2]), float(parts[3])], dtype=float)

    return wf_centers

# =========================
# Parse hr.dat
# =========================
def read_hr_header_num_wann(hr_path):
    with open(hr_path, "r", encoding="utf-8", errors="ignore") as f:
        f.readline()
        return int(f.readline().strip())

def parse_hr_dat(hr_path):
    """
    Yield (Rx,Ry,Rz,m,n,reH,imH) from wannier90_hr.dat. m,n are 1-based.
    """
    with open(hr_path, "r", encoding="utf-8", errors="ignore") as f:
        _ = f.readline()
        _num_wann = int(f.readline().strip())
        nrpts = int(f.readline().strip())

        deg = []
        while len(deg) < nrpts:
            ln = f.readline()
            if not ln:
                raise RuntimeError("Unexpected EOF while reading degeneracy list")
            ln = ln.strip()
            if not ln:
                continue
            deg += [int(x) for x in ln.split()]

        for ln in f:
            ln = ln.strip()
            if not ln:
                continue
            parts = ln.split()
            if len(parts) < 7:
                continue
            Rx, Ry, Rz = int(parts[0]), int(parts[1]), int(parts[2])
            m, n = int(parts[3]), int(parts[4])
            reH, imH = float(parts[5]), float(parts[6])
            yield Rx, Ry, Rz, m, n, reH, imH

# =========================
# Group/order helpers
# =========================
def pair_name(g1, g2, order_map):
    o1 = order_map.get(g1, 999)
    o2 = order_map.get(g2, 999)
    if o1 < o2:
        return f"{g1}<->{g2}"
    if o2 < o1:
        return f"{g2}<->{g1}"
    return f"{min(g1, g2)}<->{max(g1, g2)}"

# =========================
# Index-based WF mapping
# =========================
D_ORBS = ["dxy", "dyz", "dxz", "dz2", "dx2-y2"]
P_ORBS = ["px", "py", "pz"]

def build_index_mapping(atoms, num_wann):
    if num_wann != 68:
        raise RuntimeError(
            f"Expected num_wann=68 for your system, but got {num_wann}. "
            f"If your projections changed, update index blocks accordingly."
        )

    idx_Tc = [i for i,a in enumerate(atoms) if a["elem"] == "Tc"]
    idx_Ir = [i for i,a in enumerate(atoms) if a["elem"] == "Ir"]
    idx_Se = [i for i,a in enumerate(atoms) if a["elem"] == "Se"]
    idx_Ge = [i for i,a in enumerate(atoms) if a["elem"] == "Ge"]

    if len(idx_Tc) != 1 or len(idx_Ir) != 1 or len(idx_Se) != 6 or len(idx_Ge) != 2:
        raise RuntimeError(
            f"Atom counts mismatch: Tc={len(idx_Tc)}, Ir={len(idx_Ir)}, "
            f"Se={len(idx_Se)}, Ge={len(idx_Ge)}. Check atoms_cart."
        )

    wf_atom = [None] * (num_wann + 1)
    wf_elem = [None] * (num_wann + 1)
    wf_group = [None] * (num_wann + 1)
    wf_orb = [None] * (num_wann + 1)

    for wf in range(1, 11):
        wf_atom[wf] = idx_Tc[0]
        wf_elem[wf] = "Tc"
        wf_group[wf] = "Tc_d"
        i = (wf - 1) // 2
        wf_orb[wf] = D_ORBS[i]

    for wf in range(11, 21):
        wf_atom[wf] = idx_Ir[0]
        wf_elem[wf] = "Ir"
        wf_group[wf] = "Ir_d"
        i = (wf - 11) // 2
        wf_orb[wf] = D_ORBS[i]

    start = 21
    for a_i in range(6):
        atom_idx = idx_Se[a_i]
        for local in range(6):
            wf = start + a_i * 6 + local
            wf_atom[wf] = atom_idx
            wf_elem[wf] = "Se"
            wf_group[wf] = "Se_p"
            j = local // 2
            wf_orb[wf] = P_ORBS[j]

    start = 57
    for a_i in range(2):
        atom_idx = idx_Ge[a_i]
        for local in range(6):
            wf = start + a_i * 6 + local
            wf_atom[wf] = atom_idx
            wf_elem[wf] = "Ge"
            wf_group[wf] = "Ge_p"
            j = local // 2
            wf_orb[wf] = P_ORBS[j]

    return wf_atom, wf_elem, wf_group, wf_orb

# =========================
# Main
# =========================
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--win", required=True, help="wannier90.win")
    ap.add_argument("--centres", required=True, help="wannier90_centres.xyz")
    ap.add_argument("--hr", required=True, help="wannier90_hr.dat")
    ap.add_argument("--tol", type=float, default=0.10, help="distance bin size (Ang)")
    ap.add_argument("--min_absH", type=float, default=1e-4, help="min |H| kept (eV)")
    ap.add_argument("--topk", type=int, default=30, help="topK edges per (pair,shell)")
    ap.add_argument("--skip_same_atom_R0", action="store_true")
    ap.add_argument("--skip_diag_R0", action="store_true")
    ap.add_argument("--pairs", default="ALL", help="comma-separated allowed pairs like Tc_d<->Se_p, or ALL")
    ap.add_argument("--out_summary", default="hopping_summary_by_shell.csv")
    ap.add_argument("--out_edges", default="", help="optional edges csv output")
    ap.add_argument("--group_order", default="Tc_d,Ir_d,Se_p,Ge_p,OTHER")

    args = ap.parse_args()

    order_list = [x.strip() for x in args.group_order.split(",") if x.strip()]
    order_map = {g: i for i, g in enumerate(order_list, start=1)}

    A, atoms = parse_win_lattice_and_atoms(args.win)
    natoms = len(atoms)
    num_wann = read_hr_header_num_wann(args.hr)
    wf_centers = parse_wf_centres_from_xyz(args.centres, natoms=natoms, nwann=num_wann)

    wf_atom_id, wf_elem, wf_group, wf_orb = build_index_mapping(atoms, num_wann)

    counts = defaultdict(int)
    for wf in range(1, num_wann + 1):
        counts[wf_elem[wf]] += 1
    print("[INFO] num_wann:", num_wann, " natoms:", natoms)
    print("[INFO] WF counts by elem (index-based):", dict(sorted(counts.items())))

    allowed_pairs = None
    if args.pairs.strip().upper() != "ALL":
        allowed_pairs = set([p.strip() for p in args.pairs.split(",") if p.strip()])

    cnt = defaultdict(int)
    sumsq = defaultdict(float)
    maxv = defaultdict(float)
    top_edges = defaultdict(list)
    filtered_edges = []

    a1 = A[:, 0]; a2 = A[:, 1]; a3 = A[:, 2]
    def R_to_T(Rx, Ry, Rz):
        return Rx * a1 + Ry * a2 + Rz * a3

    for Rx, Ry, Rz, m, n, reH, imH in parse_hr_dat(args.hr):
        absH = math.hypot(reH, imH)
        if absH < args.min_absH:
            continue

        atom_m = wf_atom_id[m]
        atom_n = wf_atom_id[n]
        elem_m = wf_elem[m]
        elem_n = wf_elem[n]
        gm = wf_group[m]
        gn = wf_group[n]
        if gm == "OTHER" or gn == "OTHER":
            continue

        pair = pair_name(gm, gn, order_map=order_map)
        if allowed_pairs is not None and pair not in allowed_pairs:
            continue

        if (Rx, Ry, Rz) == (0, 0, 0):
            if args.skip_diag_R0 and (m == n):
                continue
            if args.skip_same_atom_R0 and (atom_m == atom_n):
                continue

        T = R_to_T(Rx, Ry, Rz)
        dr = (wf_centers[n] + T) - wf_centers[m]
        dist = float(np.linalg.norm(dr))
        shell = round(dist / args.tol) * args.tol

        # complex phase
        phase_rad = math.atan2(imH, reH) if absH > 0 else 0.0

        key = (pair, shell)
        cnt[key] += 1
        sumsq[key] += absH * absH
        if absH > maxv[key]:
            maxv[key] = absH

        e = Edge(
            pair, shell, dist, absH, reH, imH, phase_rad,
            Rx, Ry, Rz, m, n,
            atom_m, atom_n, elem_m, elem_n,
            gm, gn, wf_orb[m], wf_orb[n]
        )

        lst = top_edges[key]
        lst.append(e)
        lst.sort(key=lambda x: x.absH, reverse=True)
        if len(lst) > args.topk:
            lst[:] = lst[:args.topk]

        if args.out_edges:
            filtered_edges.append(e)

    # ---- summary ----
    with open(args.out_summary, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow([
            "pair", "shell_A", "count", "max_absH_eV", "rms_absH_eV",
            f"top{args.topk}_edges(m,n,atom_m,atom_n,group_m,group_n,orb_m,orb_n,R,absH,dist,phase)"
        ])
        for (pair, shell) in sorted(cnt.keys(), key=lambda x: (x[0], x[1])):
            c = cnt[(pair, shell)]
            rms = math.sqrt(sumsq[(pair, shell)] / c) if c else 0.0
            tops = top_edges[(pair, shell)]
            tops_str = "; ".join([
                f"{e.m}-{e.n}|a{e.atom_m+1}-a{e.atom_n+1}"
                f"|{e.group_m}-{e.group_n}|{e.orb_m}-{e.orb_n}"
                f"@({e.Rx},{e.Ry},{e.Rz})|{e.absH:.6g}|d={e.dist:.3f}|ph={e.phase_rad:.3f}"
                for e in tops
            ])
            w.writerow([pair, f"{shell:.3f}", c, f"{maxv[(pair, shell)]:.6g}", f"{rms:.6g}", tops_str])

    # ---- edges ----
    if args.out_edges:
        with open(args.out_edges, "w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow([
                "pair", "shell_A", "dist_A",
                "absH_eV", "reH_eV", "imH_eV", "phase_rad", "phase_deg",
                "Rx", "Ry", "Rz", "m", "n",
                "atom_m", "atom_n", "elem_m", "elem_n",
                "group_m", "group_n", "orb_m", "orb_n"
            ])
            for e in filtered_edges:
                w.writerow([
                    e.pair, f"{e.shell:.3f}", f"{e.dist:.6f}",
                    f"{e.absH:.10g}", f"{e.reH:.10g}", f"{e.imH:.10g}",
                    f"{e.phase_rad:.10g}", f"{(e.phase_rad * 180.0 / math.pi):.10g}",
                    e.Rx, e.Ry, e.Rz, e.m, e.n,
                    e.atom_m + 1, e.atom_n + 1, e.elem_m, e.elem_n,
                    e.group_m, e.group_n, e.orb_m, e.orb_n
                ])

    print("Done.")
    print("Summary:", args.out_summary)
    if args.out_edges:
        print("Edges :", args.out_edges)

if __name__ == "__main__":
    main()


```

使用方法：需要去除掉所有的换行符：

```shell

python 1step.py ^
  --win wannier90.win ^
  --centres wannier90_centres.xyz ^
  --hr wannier90_hr.dat ^
  --tol 0.10 --min_absH 1e-4 --topk 2000 ^
  --skip_same_atom_R0 ^
  --out_summary hopping_summary_by_shell.csv ^
  --out_edges edges_with_orb_reim.csv


```

(2) 提取想要的原子对之间的杂化情况


```python

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

D_ORBS = ["dxy", "dyz", "dxz", "dz2", "dx2-y2"]
P_ORBS = ["px", "py", "pz"]

# ---------------- POSCAR ----------------
def read_poscar(path: str):
    lines = Path(path).read_text(encoding="utf-8", errors="ignore").splitlines()
    lines = [ln.strip() for ln in lines if ln.strip()]

    scale = float(lines[1].split()[0])
    a1 = np.array(list(map(float, lines[2].split()[:3]))) * scale
    a2 = np.array(list(map(float, lines[3].split()[:3]))) * scale
    a3 = np.array(list(map(float, lines[4].split()[:3]))) * scale
    A = np.stack([a1, a2, a3], axis=1)  # columns

    species = lines[5].split()
    counts  = list(map(int, lines[6].split()))
    if len(species) != len(counts):
        raise RuntimeError("POSCAR species/counts mismatch")

    idx = 7
    if lines[idx].lower().startswith("selective"):
        idx += 1

    mode_line = lines[idx].lower()
    if mode_line.startswith("d"):
        mode = "Direct"
    elif mode_line.startswith("c") or mode_line.startswith("k"):
        mode = "Cartesian"
    else:
        raise RuntimeError(f"Unknown coordinate mode: {lines[idx]}")
    idx += 1

    nat = sum(counts)
    coords = np.array([list(map(float, lines[idx+i].split()[:3])) for i in range(nat)], dtype=float)
    if mode == "Cartesian":
        frac = np.linalg.solve(A, coords.T).T
    else:
        frac = coords.copy()

    elems_by_atom = []
    for sp, c in zip(species, counts):
        elems_by_atom += [sp] * c

    return A, np.asarray(frac), elems_by_atom

def bond_distance_with_R(A, frac, i0, j0, R):
    df = (frac[j0] + np.array(R, dtype=float)) - frac[i0]
    dr = A @ df
    return float(np.linalg.norm(dr))

# ---------------- plotting ----------------
def safe_mkdir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def heatmap_with_values(mat, xlabels, ylabels, title, out_png: Path, cbar_label="t_eff"):
    fig = plt.figure(figsize=(6.6, 5.0), dpi=180)
    ax = fig.add_subplot(111)
    im = ax.imshow(mat, aspect="auto", interpolation="nearest")

    ax.set_xticks(np.arange(len(xlabels)))
    ax.set_yticks(np.arange(len(ylabels)))
    ax.set_xticklabels(xlabels, rotation=45, ha="right")
    ax.set_yticklabels(ylabels)
    ax.set_title(title)

    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    cbar.set_label(cbar_label)

    finite = np.isfinite(mat)
    vmax = float(np.max(mat[finite])) if np.any(finite) else 1.0
    thresh = 0.55 * vmax

    for i in range(mat.shape[0]):
        for j in range(mat.shape[1]):
            v = mat[i, j]
            s = "0" if abs(v) < 1e-15 else f"{v:.3g}"
            color = "white" if v >= thresh else "black"
            ax.text(j, i, s, ha="center", va="center", color=color, fontsize=9)

    ax.set_xticks(np.arange(-.5, len(xlabels), 1), minor=True)
    ax.set_yticks(np.arange(-.5, len(ylabels), 1), minor=True)
    ax.grid(which="minor", linestyle="-", linewidth=0.6)
    ax.tick_params(which="minor", bottom=False, left=False)

    fig.tight_layout()
    fig.savefig(out_png, bbox_inches="tight")
    plt.close(fig)

# ---------------- orientation / filter ----------------
def orient_to_AB(df, A, B):
    """keep (A,B) or (B,A), swap BA->AB so elem_m=A, elem_n=B"""
    df = df.copy()
    mAB = (df["elem_m"] == A) & (df["elem_n"] == B)
    mBA = (df["elem_m"] == B) & (df["elem_n"] == A)
    df_AB = df.loc[mAB].copy()
    df_BA = df.loc[mBA].copy()

    if len(df_BA) > 0:
        for c1, c2 in [("m","n"),("atom_m","atom_n"),("elem_m","elem_n"),("orb_m","orb_n")]:
            tmp = df_BA[c1].copy()
            df_BA[c1] = df_BA[c2].values
            df_BA[c2] = tmp.values

    return pd.concat([df_AB, df_BA], ignore_index=True)

def filter_kind(df, kind):
    D = set(D_ORBS); P = set(P_ORBS)
    if kind == "dd":
        return df[df["orb_m"].isin(D) & df["orb_n"].isin(D)].copy(), D_ORBS, D_ORBS
    if kind == "pp":
        return df[df["orb_m"].isin(P) & df["orb_n"].isin(P)].copy(), P_ORBS, P_ORBS
    # dp after AB: m is d, n is p
    return df[df["orb_m"].isin(D) & df["orb_n"].isin(P)].copy(), D_ORBS, P_ORBS

# ---------------- SOC pairing & t_eff ----------------
def build_spin_of_wf(dfAB, strict=False):
    """
    Determine SOC Kramers pair (2 WFs) for each (atom_id, spatial_orb).
    Assign spin index 0/1 by sorting WF ids.
    """
    groups = {}
    for side in ["m", "n"]:
        wf = dfAB[side].astype(int).to_numpy()
        atom = dfAB[f"atom_{side}"].astype(int).to_numpy()
        orb  = dfAB[f"orb_{side}"].astype(str).to_numpy()
        for w,a,o in zip(wf, atom, orb):
            key = (int(a), str(o))
            groups.setdefault(key, set()).add(int(w))

    spin_of_wf = {}
    bad = []
    for (a,o), s in groups.items():
        wfs = sorted(list(s))
        if len(wfs) != 2:
            bad.append(((a,o), wfs))
            continue
        spin_of_wf[wfs[0]] = 0
        spin_of_wf[wfs[1]] = 1

    if bad:
        msg = ["[WARN] Some (atom,orb) do not have exactly 2 WFs (SOC)."]
        for (a,o), wfs in bad[:30]:
            msg.append(f"  atom={a}, orb={o}, wfs={wfs}")
        msg.append("  (showing up to 30)")
        print("\n".join(msg))
        if strict:
            raise RuntimeError("strict_spinor enabled: found (atom,orb) with !=2 WFs.")

    return spin_of_wf

def teff_block(df_block, spin_of_wf):
    """
    df_block: fixed bond image + fixed (orb_m, orb_n).
    Construct 2×2 complex H and compute Frobenius norm.
    """
    H = np.zeros((2,2), dtype=np.complex128)
    for _, r in df_block.iterrows():
        m = int(r["m"]); n = int(r["n"])
        sm = spin_of_wf.get(m, None)
        sn = spin_of_wf.get(n, None)
        if sm is None or sn is None:
            continue
        H[sm, sn] = complex(float(r["reH_eV"]), float(r["imH_eV"]))
    return float(np.sqrt(np.sum(np.abs(H)**2)))

def build_teff_matrix_for_bond(df_bond, A_orbs, B_orbs, spin_of_wf):
    mat = np.zeros((len(A_orbs), len(B_orbs)), dtype=float)
    ia = {o:i for i,o in enumerate(A_orbs)}
    ib = {o:i for i,o in enumerate(B_orbs)}
    for (om, on), g in df_bond.groupby(["orb_m","orb_n"]):
        if om not in ia or on not in ib:
            continue
        mat[ia[om], ib[on]] = teff_block(g, spin_of_wf)
    return mat

def long_table_orb_pairs(df_bond, spin_of_wf):
    """
    Produce a long table like paper: each row is (orb_m, orb_n, t_eff, and the 2x2 complex matrix entries).
    """
    rows = []
    for (om, on), g in df_bond.groupby(["orb_m","orb_n"]):
        H = np.zeros((2,2), dtype=np.complex128)
        for _, r in g.iterrows():
            m = int(r["m"]); n = int(r["n"])
            sm = spin_of_wf.get(m, None)
            sn = spin_of_wf.get(n, None)
            if sm is None or sn is None:
                continue
            H[sm, sn] = complex(float(r["reH_eV"]), float(r["imH_eV"]))
        teff = float(np.sqrt(np.sum(np.abs(H)**2)))
        rows.append({
            "orb_m": om, "orb_n": on, "t_eff": teff,
            "H00_re": H[0,0].real, "H00_im": H[0,0].imag,
            "H01_re": H[0,1].real, "H01_im": H[0,1].imag,
            "H10_re": H[1,0].real, "H10_im": H[1,0].imag,
            "H11_re": H[1,1].real, "H11_im": H[1,1].imag,
        })
    dfL = pd.DataFrame(rows)
    if not dfL.empty:
        dfL.sort_values("t_eff", ascending=False, inplace=True)
    return dfL

# ---------------- main ----------------
def main():
    ap = argparse.ArgumentParser(
        description="One-shot, maximum literature-like reproduction of Fig.5(b)/(d): bond-resolved NN selection + SOC 2×2 spinor Frobenius t_eff + outputs for plotting."
    )
    ap.add_argument("--poscar", required=True)
    ap.add_argument("--edges", required=True, help="edges_with_orb_reim.csv (must include reH_eV/imH_eV)")
    ap.add_argument("--outdir", required=True)

    ap.add_argument("--A", required=True, help="e.g. Tc")
    ap.add_argument("--B", required=True, help="e.g. Se")
    ap.add_argument("--kind", default="dp", choices=["dd","dp","pp"])

    ap.add_argument("--dmin", type=float, required=True, help="NN distance window min (Å)")
    ap.add_argument("--dmax", type=float, required=True, help="NN distance window max (Å)")

    ap.add_argument("--min_absH", type=float, default=0.0, help="pre-filter edges by absH_eV")
    ap.add_argument("--top_bonds", type=int, default=10, help="export top N NN bond-images")
    ap.add_argument("--strict_spinor", action="store_true", help="require exactly 2 WFs per (atom,orb)")

    args = ap.parse_args()

    outdir = Path(args.outdir)
    safe_mkdir(outdir)

    A_lat, frac, elems = read_poscar(args.poscar)
    nat = len(elems)

    df = pd.read_csv(args.edges)
    need = ["elem_m","elem_n","atom_m","atom_n","orb_m","orb_n","absH_eV",
            "reH_eV","imH_eV","Rx","Ry","Rz","m","n"]
    miss = [c for c in need if c not in df.columns]
    if miss:
        raise ValueError(f"edges missing columns: {miss}")

    if args.min_absH > 0:
        df = df[df["absH_eV"] >= args.min_absH].copy()

    # orient + kind filter
    dfAB = orient_to_AB(df, args.A, args.B)
    dfAB, A_orbs, B_orbs = filter_kind(dfAB, args.kind)
    if dfAB.empty:
        raise RuntimeError("No edges after A/B/kind filtering.")

    # SOC pairing
    spin_of_wf = build_spin_of_wf(dfAB, strict=args.strict_spinor)

    # compute bond distance per edge (atom_m -> atom_n + R)
    am = dfAB["atom_m"].astype(int).to_numpy()
    an = dfAB["atom_n"].astype(int).to_numpy()
    Rx = dfAB["Rx"].astype(int).to_numpy()
    Ry = dfAB["Ry"].astype(int).to_numpy()
    Rz = dfAB["Rz"].astype(int).to_numpy()

    if am.min() < 1 or an.min() < 1 or am.max() > nat or an.max() > nat:
        raise RuntimeError("atom indices out of POSCAR range")

    d_atom = np.empty(len(dfAB), dtype=float)
    for i in range(len(dfAB)):
        d_atom[i] = bond_distance_with_R(A_lat, frac, am[i]-1, an[i]-1, (Rx[i], Ry[i], Rz[i]))

    dfAB = dfAB.copy()
    dfAB["d_atom_R_A"] = d_atom

    # NN window selection (paper-like)
    dfNN = dfAB[(dfAB["d_atom_R_A"] >= args.dmin) & (dfAB["d_atom_R_A"] <= args.dmax)].copy()
    if dfNN.empty:
        raise RuntimeError("No edges in NN window. Widen dmin/dmax or check POSCAR/edges.")

    # bond-image identity
    bond_cols = ["atom_m","atom_n","Rx","Ry","Rz"]
    bond_rows = []
    mats_cache = {}

    for key, g in dfNN.groupby(bond_cols):
        mat = build_teff_matrix_for_bond(g, A_orbs, B_orbs, spin_of_wf)
        score = float(mat.sum())
        dval = float(g["d_atom_R_A"].mean())
        bond_rows.append((*key, dval, len(g), score))
        mats_cache[key] = (mat, g)

    bond_sum = pd.DataFrame(
        bond_rows,
        columns=["atom_m","atom_n","Rx","Ry","Rz","d_atom_A","n_edges","sum_teff"]
    ).sort_values("sum_teff", ascending=False)

    # export global ranking (this is your "choose representative bond like paper")
    bond_rank = outdir / f"BONDS_{args.A}-{args.B}_{args.kind}_TEFF_d{args.dmin}-{args.dmax}.csv"
    bond_sum.to_csv(bond_rank, index=False)

    # export top bonds with full fig5-like package
    topN = min(args.top_bonds, len(bond_sum))
    summary = []

    for bi in range(topN):
        r = bond_sum.iloc[bi]
        key = (int(r["atom_m"]), int(r["atom_n"]), int(r["Rx"]), int(r["Ry"]), int(r["Rz"]))
        mat, g = mats_cache[key]
        a_m, a_n, Rx0, Ry0, Rz0 = key
        dval = float(r["d_atom_A"])

        tag = f"{args.A}{a_m}-{args.B}{a_n}_R{Rx0}{Ry0}{Rz0}_{args.kind}_TEFF"
        bdir = outdir / tag
        safe_mkdir(bdir)

        # (1) matrix + heatmap (paper-like panel)
        pd.DataFrame(mat, index=A_orbs, columns=B_orbs).to_csv(bdir / f"{tag}_matrix.csv")
        title = f"{tag} | d={dval:.3f}Å | Σ t_eff={mat.sum():.4g}"
        heatmap_with_values(mat, B_orbs, A_orbs, title, bdir / f"{tag}.png",
                            cbar_label=r"$t^{\mathrm{eff}}=\|H^{2\times2}\|_F$ (eV)")

        # (2) long-table orbital pairs (paper-like list)
        dfL = long_table_orb_pairs(g, spin_of_wf)
        dfL.to_csv(bdir / f"{tag}_orbital_pairs_long.csv", index=False)

        # (3) top pairs list
        flat = []
        for i, ao in enumerate(A_orbs):
            for j, bo in enumerate(B_orbs):
                flat.append((ao, bo, mat[i, j]))
        flat.sort(key=lambda x: x[2], reverse=True)
        pd.DataFrame(flat, columns=["A_orb","B_orb","t_eff"]).to_csv(
            bdir / f"{tag}_orbital_pairs_matrix_entries.csv", index=False
        )

        # (4) raw edges used (traceability)
        g.to_csv(bdir / f"{tag}_edges.csv", index=False)

        summary.append({
            "rank": bi+1,
            "bond_tag": tag,
            "atom_m": a_m, "atom_n": a_n,
            "Rx": Rx0, "Ry": Ry0, "Rz": Rz0,
            "d_atom_A": dval,
            "sum_teff": float(mat.sum()),
            "max_teff": float(mat.max()),
            "argmax_pair": f"{flat[0][0]}-{flat[0][1]}" if flat else "NA",
        })

    pd.DataFrame(summary).to_csv(outdir / f"SUMMARY_top{topN}_bonds_TEFF.csv", index=False)

    print(f"[OK] NN edges: {len(dfNN)} in window [{args.dmin},{args.dmax}] Å")
    print(f"[OK] unique NN bond-images: {len(bond_sum)}")
    print(f"[OK] bond ranking: {bond_rank}")
    print(f"[OK] exported top bond packages: {topN}")
    print(f"[OK] summary: {outdir / f'SUMMARY_top{topN}_bonds_TEFF.csv'}")


if __name__ == "__main__":
    main()


```


使用方法：

```shell

python paper_fig5_like_soc.py ^
  --poscar POSCAR ^
  --edges edges_with_orb_reim.csv ^
  --outdir Fig5_like_TcSe ^
  --A Tc --B Se --kind dp ^
  --dmin 2.43 --dmax 2.73 ^
  --top_bonds 6 ^
  --strict_spinor


```


(3) 使用脚本，计算dd轨道杂化情况

```python

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
paper_fig5_like_soc.py  (final, with --unique_bonds)

One-shot, maximum literature-like reproduction of Fig.5(b)/(d):
- bond-resolved NN selection by atom distance window (POSCAR + Rx,Ry,Rz)
- SOC spinor 2×2 block -> Frobenius norm t_eff
- export: matrix heatmap + long-table + raw edges for top bond-images
- NEW: --unique_bonds to avoid double counting when A==B (e.g., Tc-Tc)
"""

import argparse
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

D_ORBS = ["dxy", "dyz", "dxz", "dz2", "dx2-y2"]
P_ORBS = ["px", "py", "pz"]

# ---------------- POSCAR ----------------
def read_poscar(path: str):
    lines = Path(path).read_text(encoding="utf-8", errors="ignore").splitlines()
    lines = [ln.strip() for ln in lines if ln.strip()]

    scale = float(lines[1].split()[0])
    a1 = np.array(list(map(float, lines[2].split()[:3]))) * scale
    a2 = np.array(list(map(float, lines[3].split()[:3]))) * scale
    a3 = np.array(list(map(float, lines[4].split()[:3]))) * scale
    A = np.stack([a1, a2, a3], axis=1)  # columns

    species = lines[5].split()
    counts  = list(map(int, lines[6].split()))
    if len(species) != len(counts):
        raise RuntimeError("POSCAR species/counts mismatch")

    idx = 7
    if lines[idx].lower().startswith("selective"):
        idx += 1

    mode_line = lines[idx].lower()
    if mode_line.startswith("d"):
        mode = "Direct"
    elif mode_line.startswith("c") or mode_line.startswith("k"):
        mode = "Cartesian"
    else:
        raise RuntimeError(f"Unknown coordinate mode: {lines[idx]}")
    idx += 1

    nat = sum(counts)
    coords = np.array([list(map(float, lines[idx+i].split()[:3])) for i in range(nat)], dtype=float)
    if mode == "Cartesian":
        frac = np.linalg.solve(A, coords.T).T
    else:
        frac = coords.copy()

    elems_by_atom = []
    for sp, c in zip(species, counts):
        elems_by_atom += [sp] * c

    return A, np.asarray(frac), elems_by_atom

def bond_distance_with_R(A, frac, i0, j0, R):
    df = (frac[j0] + np.array(R, dtype=float)) - frac[i0]
    dr = A @ df
    return float(np.linalg.norm(dr))

# ---------------- plotting ----------------
def safe_mkdir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def heatmap_with_values(mat, xlabels, ylabels, title, out_png: Path, cbar_label="t_eff"):
    fig = plt.figure(figsize=(6.6, 5.0), dpi=180)
    ax = fig.add_subplot(111)
    im = ax.imshow(mat, aspect="auto", interpolation="nearest")

    ax.set_xticks(np.arange(len(xlabels)))
    ax.set_yticks(np.arange(len(ylabels)))
    ax.set_xticklabels(xlabels, rotation=45, ha="right")
    ax.set_yticklabels(ylabels)
    ax.set_title(title)

    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    cbar.set_label(cbar_label)

    finite = np.isfinite(mat)
    vmax = float(np.max(mat[finite])) if np.any(finite) else 1.0
    thresh = 0.55 * vmax

    for i in range(mat.shape[0]):
        for j in range(mat.shape[1]):
            v = mat[i, j]
            s = "0" if abs(v) < 1e-15 else f"{v:.3g}"
            color = "white" if v >= thresh else "black"
            ax.text(j, i, s, ha="center", va="center", color=color, fontsize=9)

    ax.set_xticks(np.arange(-.5, len(xlabels), 1), minor=True)
    ax.set_yticks(np.arange(-.5, len(ylabels), 1), minor=True)
    ax.grid(which="minor", linestyle="-", linewidth=0.6)
    ax.tick_params(which="minor", bottom=False, left=False)

    fig.tight_layout()
    fig.savefig(out_png, bbox_inches="tight")
    plt.close(fig)

# ---------------- orientation / filter ----------------
def orient_to_AB(df, A, B):
    """
    Keep (A,B) or (B,A). Swap BA->AB so elem_m=A, elem_n=B.
    Note: If A==B, this will keep both directions as-is; use --unique_bonds to avoid double counting.
    """
    df = df.copy()
    mAB = (df["elem_m"] == A) & (df["elem_n"] == B)
    mBA = (df["elem_m"] == B) & (df["elem_n"] == A)
    df_AB = df.loc[mAB].copy()
    df_BA = df.loc[mBA].copy()

    if len(df_BA) > 0 and A != B:
        for c1, c2 in [("m","n"),("atom_m","atom_n"),("elem_m","elem_n"),("orb_m","orb_n")]:
            tmp = df_BA[c1].copy()
            df_BA[c1] = df_BA[c2].values
            df_BA[c2] = tmp.values

    return pd.concat([df_AB, df_BA], ignore_index=True)

def filter_kind(df, kind):
    D = set(D_ORBS); P = set(P_ORBS)
    if kind == "dd":
        return df[df["orb_m"].isin(D) & df["orb_n"].isin(D)].copy(), D_ORBS, D_ORBS
    if kind == "pp":
        return df[df["orb_m"].isin(P) & df["orb_n"].isin(P)].copy(), P_ORBS, P_ORBS
    # dp after AB: m is d, n is p
    return df[df["orb_m"].isin(D) & df["orb_n"].isin(P)].copy(), D_ORBS, P_ORBS

# ---------------- SOC pairing & t_eff ----------------
def build_spin_of_wf(dfAB, strict=False):
    """
    Determine SOC Kramers pair (2 WFs) for each (atom_id, spatial_orb).
    Assign spin index 0/1 by sorting WF ids.
    """
    groups = {}
    for side in ["m", "n"]:
        wf = dfAB[side].astype(int).to_numpy()
        atom = dfAB[f"atom_{side}"].astype(int).to_numpy()
        orb  = dfAB[f"orb_{side}"].astype(str).to_numpy()
        for w, a, o in zip(wf, atom, orb):
            key = (int(a), str(o))
            groups.setdefault(key, set()).add(int(w))

    spin_of_wf = {}
    bad = []
    for (a, o), s in groups.items():
        wfs = sorted(list(s))
        if len(wfs) != 2:
            bad.append(((a, o), wfs))
            continue
        spin_of_wf[wfs[0]] = 0
        spin_of_wf[wfs[1]] = 1

    if bad:
        msg = ["[WARN] Some (atom,orb) do not have exactly 2 WFs (SOC)."]
        for (a, o), wfs in bad[:30]:
            msg.append(f"  atom={a}, orb={o}, wfs={wfs}")
        msg.append("  (showing up to 30)")
        print("\n".join(msg))
        if strict:
            raise RuntimeError("strict_spinor enabled: found (atom,orb) with !=2 WFs.")

    return spin_of_wf

def teff_block(df_block, spin_of_wf):
    """
    df_block: fixed bond image + fixed (orb_m, orb_n).
    Construct 2×2 complex H and compute Frobenius norm.
    """
    H = np.zeros((2, 2), dtype=np.complex128)
    for _, r in df_block.iterrows():
        m = int(r["m"]); n = int(r["n"])
        sm = spin_of_wf.get(m, None)
        sn = spin_of_wf.get(n, None)
        if sm is None or sn is None:
            continue
        H[sm, sn] = complex(float(r["reH_eV"]), float(r["imH_eV"]))
    return float(np.sqrt(np.sum(np.abs(H) ** 2)))

def build_teff_matrix_for_bond(df_bond, A_orbs, B_orbs, spin_of_wf):
    mat = np.zeros((len(A_orbs), len(B_orbs)), dtype=float)
    ia = {o: i for i, o in enumerate(A_orbs)}
    ib = {o: i for i, o in enumerate(B_orbs)}
    for (om, on), g in df_bond.groupby(["orb_m", "orb_n"]):
        if om not in ia or on not in ib:
            continue
        mat[ia[om], ib[on]] = teff_block(g, spin_of_wf)
    return mat

def long_table_orb_pairs(df_bond, spin_of_wf):
    """
    Long table: each row is (orb_m, orb_n, t_eff, and the 2x2 complex matrix entries).
    """
    rows = []
    for (om, on), g in df_bond.groupby(["orb_m", "orb_n"]):
        H = np.zeros((2, 2), dtype=np.complex128)
        for _, r in g.iterrows():
            m = int(r["m"]); n = int(r["n"])
            sm = spin_of_wf.get(m, None)
            sn = spin_of_wf.get(n, None)
            if sm is None or sn is None:
                continue
            H[sm, sn] = complex(float(r["reH_eV"]), float(r["imH_eV"]))
        teff = float(np.sqrt(np.sum(np.abs(H) ** 2)))
        rows.append({
            "orb_m": om, "orb_n": on, "t_eff": teff,
            "H00_re": H[0, 0].real, "H00_im": H[0, 0].imag,
            "H01_re": H[0, 1].real, "H01_im": H[0, 1].imag,
            "H10_re": H[1, 0].real, "H10_im": H[1, 0].imag,
            "H11_re": H[1, 1].real, "H11_im": H[1, 1].imag,
        })
    dfL = pd.DataFrame(rows)
    if not dfL.empty:
        dfL.sort_values("t_eff", ascending=False, inplace=True)
    return dfL

# ---------------- main ----------------
def main():
    ap = argparse.ArgumentParser(
        description="Fig.5-like (bond-resolved) orbital hybridization with SOC t_eff (Frobenius norm of 2x2 spinor block)."
    )
    ap.add_argument("--poscar", required=True)
    ap.add_argument("--edges", required=True, help="edges_with_orb_reim.csv (must include reH_eV/imH_eV)")
    ap.add_argument("--outdir", required=True)

    ap.add_argument("--A", required=True, help="e.g. Tc")
    ap.add_argument("--B", required=True, help="e.g. Se")
    ap.add_argument("--kind", default="dp", choices=["dd", "dp", "pp"])

    ap.add_argument("--dmin", type=float, required=True, help="NN distance window min (Å)")
    ap.add_argument("--dmax", type=float, required=True, help="NN distance window max (Å)")

    ap.add_argument("--min_absH", type=float, default=0.0, help="pre-filter edges by absH_eV")
    ap.add_argument("--top_bonds", type=int, default=10, help="export top N NN bond-images")
    ap.add_argument("--strict_spinor", action="store_true", help="require exactly 2 WFs per (atom,orb)")
    # ---- NEW ----
    ap.add_argument("--unique_bonds", action="store_true",
                    help="When A==B, keep only atom_m < atom_n to avoid double counting.")

    args = ap.parse_args()

    outdir = Path(args.outdir)
    safe_mkdir(outdir)

    A_lat, frac, elems = read_poscar(args.poscar)
    nat = len(elems)

    df = pd.read_csv(args.edges)
    need = ["elem_m", "elem_n", "atom_m", "atom_n", "orb_m", "orb_n", "absH_eV",
            "reH_eV", "imH_eV", "Rx", "Ry", "Rz", "m", "n"]
    miss = [c for c in need if c not in df.columns]
    if miss:
        raise ValueError(f"edges missing columns: {miss}")

    if args.min_absH > 0:
        df = df[df["absH_eV"] >= args.min_absH].copy()

    # orient + kind filter
    dfAB = orient_to_AB(df, args.A, args.B)
    dfAB, A_orbs, B_orbs = filter_kind(dfAB, args.kind)
    if dfAB.empty:
        raise RuntimeError("No edges after A/B/kind filtering.")

    # SOC pairing
    spin_of_wf = build_spin_of_wf(dfAB, strict=args.strict_spinor)

    # compute atom distance per edge (atom_m -> atom_n + R)
    am = dfAB["atom_m"].astype(int).to_numpy()
    an = dfAB["atom_n"].astype(int).to_numpy()
    Rx = dfAB["Rx"].astype(int).to_numpy()
    Ry = dfAB["Ry"].astype(int).to_numpy()
    Rz = dfAB["Rz"].astype(int).to_numpy()

    if am.min() < 1 or an.min() < 1 or am.max() > nat or an.max() > nat:
        raise RuntimeError("atom indices out of POSCAR range")

    d_atom = np.empty(len(dfAB), dtype=float)
    for i in range(len(dfAB)):
        d_atom[i] = bond_distance_with_R(A_lat, frac, am[i]-1, an[i]-1, (Rx[i], Ry[i], Rz[i]))

    dfAB = dfAB.copy()
    dfAB["d_atom_R_A"] = d_atom

    # NN window selection (paper-like)
    dfNN = dfAB[(dfAB["d_atom_R_A"] >= args.dmin) & (dfAB["d_atom_R_A"] <= args.dmax)].copy()
    if dfNN.empty:
        raise RuntimeError("No edges in NN window. Widen dmin/dmax or check POSCAR/edges.")

    # ---- NEW: avoid double counting for A==B ----
    if args.unique_bonds and args.A == args.B:
        dfNN = dfNN[dfNN["atom_m"].astype(int) < dfNN["atom_n"].astype(int)].copy()
        if dfNN.empty:
            raise RuntimeError("After --unique_bonds filtering, no edges left. Check distance window.")

    # bond-image identity
    bond_cols = ["atom_m", "atom_n", "Rx", "Ry", "Rz"]
    bond_rows = []
    mats_cache = {}

    for key, g in dfNN.groupby(bond_cols):
        mat = build_teff_matrix_for_bond(g, A_orbs, B_orbs, spin_of_wf)
        score = float(mat.sum())
        dval = float(g["d_atom_R_A"].mean())
        bond_rows.append((*key, dval, len(g), score))
        mats_cache[key] = (mat, g)

    bond_sum = pd.DataFrame(
        bond_rows,
        columns=["atom_m", "atom_n", "Rx", "Ry", "Rz", "d_atom_A", "n_edges", "sum_teff"]
    ).sort_values("sum_teff", ascending=False)

    # export global ranking
    bond_rank = outdir / f"BONDS_{args.A}-{args.B}_{args.kind}_TEFF_d{args.dmin}-{args.dmax}.csv"
    bond_sum.to_csv(bond_rank, index=False)

    # export top bonds
    topN = min(args.top_bonds, len(bond_sum))
    summary = []

    for bi in range(topN):
        r = bond_sum.iloc[bi]
        key = (int(r["atom_m"]), int(r["atom_n"]), int(r["Rx"]), int(r["Ry"]), int(r["Rz"]))
        mat, g = mats_cache[key]
        a_m, a_n, Rx0, Ry0, Rz0 = key
        dval = float(r["d_atom_A"])

        tag = f"{args.A}{a_m}-{args.B}{a_n}_R{Rx0}{Ry0}{Rz0}_{args.kind}_TEFF"
        bdir = outdir / tag
        safe_mkdir(bdir)

        # (1) matrix + heatmap
        pd.DataFrame(mat, index=A_orbs, columns=B_orbs).to_csv(bdir / f"{tag}_matrix.csv")
        title = f"{tag} | d={dval:.3f}Å | Σ t_eff={mat.sum():.4g}"
        heatmap_with_values(
            mat, B_orbs, A_orbs, title, bdir / f"{tag}.png",
            cbar_label=r"$t^{\mathrm{eff}}=\|H^{2\times2}\|_F$ (eV)"
        )

        # (2) long-table orbital pairs
        dfL = long_table_orb_pairs(g, spin_of_wf)
        dfL.to_csv(bdir / f"{tag}_orbital_pairs_long.csv", index=False)

        # (3) flat list from matrix (all entries)
        flat = []
        for i, ao in enumerate(A_orbs):
            for j, bo in enumerate(B_orbs):
                flat.append((ao, bo, mat[i, j]))
        flat.sort(key=lambda x: x[2], reverse=True)
        pd.DataFrame(flat, columns=["A_orb", "B_orb", "t_eff"]).to_csv(
            bdir / f"{tag}_orbital_pairs_matrix_entries.csv", index=False
        )

        # (4) raw edges used
        g.to_csv(bdir / f"{tag}_edges.csv", index=False)

        summary.append({
            "rank": bi + 1,
            "bond_tag": tag,
            "atom_m": a_m, "atom_n": a_n,
            "Rx": Rx0, "Ry": Ry0, "Rz": Rz0,
            "d_atom_A": dval,
            "sum_teff": float(mat.sum()),
            "max_teff": float(mat.max()),
            "argmax_pair": f"{flat[0][0]}-{flat[0][1]}" if flat else "NA",
        })

    pd.DataFrame(summary).to_csv(outdir / f"SUMMARY_top{topN}_bonds_TEFF.csv", index=False)

    print(f"[OK] NN edges: {len(dfNN)} in window [{args.dmin},{args.dmax}] Å")
    print(f"[OK] unique NN bond-images: {len(bond_sum)}")
    print(f"[OK] bond ranking: {bond_rank}")
    print(f"[OK] exported top bond packages: {topN}")
    print(f"[OK] summary: {outdir / f'SUMMARY_top{topN}_bonds_TEFF.csv'}")

if __name__ == "__main__":
    main()


```

使用方法：

```shell

python r3.py  --poscar POSCAR --edges edges_with_orb_reim.csv --outdir Fig5_like_TcTc --A Tc --B Tc --kind dd --dmin 6.4 --dmax 6.6 --top_bonds 6
--strict_spinor --unique_bonds


```

前面的文章提到了有原子序号的超超交换路径，直接找序号，然后选择最小的dist_atom即可，就是最近的这个键的杂化情况。

今天白天开了1400公里的车，凌晨2：07完成此篇文章。