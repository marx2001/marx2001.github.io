---
layout: post
title: "利用wannier验证超超交换路径(2)"
subtitle: "Experience Sharing"
background: '/img/bg-sci-note.jpg'
categories: sci-note
permalink: /sci-note_posts/20260204-super2
---

## <center>说明</center>
上一篇文章是固定d-p-d-p-d的超超交换路径，dp杂化，而不能排除d-p-p-p-d杂化的可能，因此本篇文章通过修改脚本主要探究d-p-p-p-d杂化与d-p-d-p-d哪个更占据主导。

## <center>代码</center>  

1. 此次要在同一个子空间内对比Ge-d和Ge-p的超超交换路径评分，且考虑SOC，因此考虑下方的轨道：

```python
num_wann = 88

Tc: d
Se : p
Ge : d;p
Ir: d
```

运行一次成功的wannier拟合之后，进入第二步

2. 我们需要重新修改一下代码，因为之前的代码是硬编码wannier centres和元素轨道对应。

```python

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
1step.py  (Python 3.8/3.9 compatible)

NEW: export edges csv (2step.csv) from wannier90_hr.dat, like your old 1step.py did,
but now robust for num_wann=88 with projections:
  Tc: d
  Se: p
  Ge: d;p
  Ir: d

Core features
-------------
(1) Parse wannier90.win: unit_cell_cart, atoms_cart, projections
(2) Parse wannier90.wout: Final State WF centres & spreads (authoritative)
(3) Parse wannier90_hr.dat: all hopping matrix elements H_{mn}(R)
(4) Assign each WF to an element using quota-balanced assignment so element counts
    match expected from projections & atom counts (handles bond-centred WFs).
(5) Split Ge WFs into Ge_d / Ge_p using spread / onsite / hybrid.
(6) Export directed edges to CSV with columns compatible with your downstream 2step.py:
    shell, dist, absH, re, im, Rx,Ry,Rz, u_wf,v_wf, u_atom,v_atom, u_elem,v_elem, pair

Optional:
- Export a summary CSV per shell.

Usage (export edges)
--------------------
python 1step.py --win wannier90.win --wout wannier90.wout --hr wannier90_hr.dat \
  --out_edges 2step.csv --tol 0.10 --min_absH 1e-6

If you want fewer edges:
  --topk_per_shell 400

Ge d/p splitting mode:
  --ge_split spread   (default)
  --ge_split onsite
  --ge_split hybrid
"""

import argparse
import csv
import json
import re
from collections import defaultdict, Counter
from pathlib import Path

import numpy as np


# =========================
# WIN parsing
# =========================
def _extract_block(txt, name):
    m = re.search(r"begin\s+%s(.*?)end\s+%s" % (re.escape(name), re.escape(name)), txt, re.S | re.I)
    return None if not m else m.group(1).strip()


def parse_win(win_path):
    txt = Path(win_path).read_text(encoding="utf-8", errors="ignore")

    blk = _extract_block(txt, "unit_cell_cart")
    if blk is None:
        raise RuntimeError("Cannot find begin/end unit_cell_cart in win.")
    lines = [ln.strip() for ln in blk.splitlines() if ln.strip()]
    if lines and lines[0].lower().startswith("ang"):
        lines = lines[1:]
    if len(lines) < 3:
        raise RuntimeError("unit_cell_cart must contain 3 lattice vectors.")
    A = np.array([[float(x) for x in lines[i].split()[:3]] for i in range(3)], dtype=float)

    blk = _extract_block(txt, "atoms_cart")
    if blk is None:
        raise RuntimeError("Cannot find begin/end atoms_cart in win.")
    atoms = []
    for ln in blk.splitlines():
        ln = ln.strip()
        if not ln:
            continue
        parts = ln.split()
        if len(parts) < 4:
            continue
        elem = parts[0].strip()
        x, y, z = map(float, parts[1:4])
        atoms.append((elem, np.array([x, y, z], float)))
    if not atoms:
        raise RuntimeError("No atoms parsed from atoms_cart.")

    blk = _extract_block(txt, "projections")
    if blk is None:
        raise RuntimeError("Cannot find begin/end projections in win.")
    proj = defaultdict(set)
    for ln in blk.splitlines():
        ln = ln.strip()
        if not ln or ln.startswith("#") or ln.startswith("!"):
            continue
        if ":" not in ln:
            continue
        lhs, rhs = ln.split(":", 1)
        elem = lhs.strip()
        rhs = rhs.strip().replace(" ", "")
        for orb in rhs.split(";"):
            if orb:
                proj[elem].add(orb.lower())
    if not proj:
        raise RuntimeError("Parsed empty projections. Check win projections block.")

    return A, atoms, proj


# =========================
# WOUT parsing (Final State)
# =========================
def parse_wout_final_centres_spreads(wout_path):
    txt = Path(wout_path).read_text(encoding="utf-8", errors="ignore")
    idx = txt.lower().rfind("final state")
    if idx < 0:
        raise RuntimeError("Cannot find 'Final State' in wannier90.wout")

    centres = {}
    spreads = {}

    pat = re.compile(
        r"^\s*WF centre and spread\s+(\d+)\s+\(\s*([-\d\.]+)\s*,\s*([-\d\.]+)\s*,\s*([-\d\.]+)\s*\)\s+([-\d\.]+)\s*$"
    )

    for ln in txt[idx:].splitlines():
        m = pat.match(ln)
        if not m:
            continue
        i = int(m.group(1))
        x, y, z = float(m.group(2)), float(m.group(3)), float(m.group(4))
        s = float(m.group(5))
        centres[i] = np.array([x, y, z], float)
        spreads[i] = s

    if not centres:
        raise RuntimeError("No WF centres parsed from Final State section.")
    num_wann = max(centres.keys())
    return num_wann, centres, spreads


# =========================
# HR parsing
# =========================
def read_hr_all(hr_path):
    """
    Return:
      num_wann, onsite(dict), hoppings(list of tuples)
    where hoppings are (Rx,Ry,Rz,m,n,re,im)
    """
    raw = Path(hr_path).read_text(encoding="utf-8", errors="ignore").splitlines()
    if len(raw) < 4:
        raise RuntimeError("hr.dat too short")
    num_wann = int(raw[1].split()[0])
    nrpts = int(raw[2].split()[0])

    deg = []
    i = 3
    while len(deg) < nrpts:
        deg.extend([int(x) for x in raw[i].split()])
        i += 1
        if i >= len(raw):
            raise RuntimeError("Unexpected EOF while reading degeneracies.")

    onsite = {}
    hops = []
    for ln in raw[i:]:
        parts = ln.split()
        if len(parts) < 7:
            continue
        Rx, Ry, Rz = map(int, parts[0:3])
        m = int(parts[3])
        n = int(parts[4])
        re_ = float(parts[5])
        im_ = float(parts[6])
        hops.append((Rx, Ry, Rz, m, n, re_, im_))
        if Rx == 0 and Ry == 0 and Rz == 0 and m == n:
            onsite[m] = re_
    return num_wann, onsite, hops


# =========================
# PBC helpers
# =========================
def pbc_dist(A, r1, r2):
    invA = np.linalg.inv(A)
    f1 = r1 @ invA
    f2 = r2 @ invA
    df = f2 - f1
    df -= np.round(df)
    dr = df @ A
    return float(np.linalg.norm(dr))


def shift_by_R(A, vec, Rx, Ry, Rz):
    return vec + Rx * A[0] + Ry * A[1] + Rz * A[2]


# =========================
# Expected WF counts from projections + atoms_cart
# =========================
def expected_counts(atoms, proj, num_wann):
    atom_counts = Counter([e for e, _ in atoms])
    orb_dim = {"s": 1, "p": 3, "d": 5, "f": 7}

    base_total = 0
    elem_orb = defaultdict(dict)
    elem_total = {}

    for elem, orbs in proj.items():
        n = atom_counts.get(elem, 0)
        tot = 0
        for o in orbs:
            if o not in orb_dim:
                raise RuntimeError("Unknown orbital '%s' in projections for %s" % (o, elem))
            tot += orb_dim[o] * n
        elem_total[elem] = tot
        base_total += tot
        for o in orbs:
            elem_orb[elem][o] = orb_dim[o] * n

    factor = 2 if num_wann == 2 * base_total else 1

    elem_total = {k: v * factor for k, v in elem_total.items()}
    for elem in list(elem_orb.keys()):
        for o in list(elem_orb[elem].keys()):
            elem_orb[elem][o] *= factor

    s = sum(elem_total.values())
    if s != num_wann:
        raise RuntimeError(
            "Expected counts sum %d != num_wann %d. Check atoms_cart/projections or SOC/spinor." % (s, num_wann)
        )
    return factor, elem_total, elem_orb


# =========================
# Quota-balanced WF->element assignment
# =========================
def assign_wfs_to_elements_quota(A, atoms, centres, elem_quota):
    elem_atoms = defaultdict(list)
    for ai, (elem, pos) in enumerate(atoms, start=1):
        elem_atoms[elem].append((ai, pos))

    wf_choices = {}
    for wf, c in centres.items():
        opts = []
        for elem, lst in elem_atoms.items():
            bestd = 1e18
            best_ai = None
            for ai, pos in lst:
                d = pbc_dist(A, c, pos)
                if d < bestd:
                    bestd = d
                    best_ai = ai
            opts.append((bestd, elem, best_ai))
        opts.sort(key=lambda x: x[0])
        wf_choices[wf] = opts

    order = sorted(centres.keys(), key=lambda wf: wf_choices[wf][0][0])

    quota = dict(elem_quota)
    wf_elem = {}
    wf_atom = {}

    for wf in order:
        assigned = False
        for d, elem, ai in wf_choices[wf]:
            if quota.get(elem, 0) > 0:
                wf_elem[wf] = elem
                wf_atom[wf] = ai
                quota[elem] -= 1
                assigned = True
                break
        if not assigned:
            wf_elem[wf] = wf_choices[wf][0][1]
            wf_atom[wf] = wf_choices[wf][0][2]

    remain = {k: v for k, v in quota.items() if v != 0}
    if remain:
        raise RuntimeError("Quota assignment failed, remaining quota: %s" % remain)
    return wf_elem, wf_atom


# =========================
# Split Ge into d/p
# =========================
def split_ge_dp(wfs_ge, spreads, onsite, n_ge_d, n_ge_p, mode):
    wfs_ge = list(wfs_ge)
    if len(wfs_ge) != n_ge_d + n_ge_p:
        raise RuntimeError("Ge WF count %d != expected %d" % (len(wfs_ge), n_ge_d + n_ge_p))

    if mode == "spread":
        order = sorted(wfs_ge, key=lambda i: spreads[i])
    elif mode == "onsite":
        order = sorted(wfs_ge, key=lambda i: onsite.get(i, 0.0))
    elif mode == "hybrid":
        sp = np.array([spreads[i] for i in wfs_ge], float)
        ep = np.array([onsite.get(i, 0.0) for i in wfs_ge], float)
        spz = (sp - sp.mean()) / (sp.std() + 1e-12)
        epz = (ep - ep.mean()) / (ep.std() + 1e-12)
        score = spz + epz
        order = [w for _, w in sorted(zip(score, wfs_ge), key=lambda x: x[0])]
    else:
        raise ValueError("Unknown ge_split mode: %s" % mode)

    ge_d = set(order[:n_ge_d])
    ge_p = set(order[n_ge_d:n_ge_d + n_ge_p])
    return ge_d, ge_p


# =========================
# Shell clustering by distance
# =========================
def assign_shells_by_tol(dist_list, tol):
    """
    Given distances, cluster into shells by tolerance tol.
    Return:
      shell_id per dist index, and shell_centers list
    """
    if not dist_list:
        return [], []
    order = sorted(range(len(dist_list)), key=lambda i: dist_list[i])
    centers = []
    shell_id = [None] * len(dist_list)

    for idx in order:
        d = dist_list[idx]
        placed = False
        for s, c in enumerate(centers):
            if abs(d - c) <= tol:
                shell_id[idx] = s + 1
                # update center (simple running average)
                centers[s] = (c + d) * 0.5
                placed = True
                break
        if not placed:
            centers.append(d)
            shell_id[idx] = len(centers)
    return shell_id, centers


# =========================
# Main: export edges
# =========================
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--win", required=True)
    ap.add_argument("--wout", required=True)
    ap.add_argument("--hr", required=True)

    ap.add_argument("--out_edges", default="", help="Output edges CSV (e.g. 2step.csv). If empty, no edges exported.")
    ap.add_argument("--out_summary", default="", help="Optional per-shell summary CSV.")

    ap.add_argument("--min_absH", type=float, default=1e-6)
    ap.add_argument("--tol", type=float, default=0.10, help="Distance tolerance for shell clustering (Ang).")
    ap.add_argument("--topk_per_shell", type=int, default=0,
                    help="If >0, keep only topK edges (by absH) per shell.")

    ap.add_argument("--ge_split", default="spread", choices=["spread", "onsite", "hybrid"])
    ap.add_argument("--dump_groups", default="", help="Optional JSON to dump WF index sets.")

    args = ap.parse_args()

    A, atoms, proj = parse_win(args.win)
    num_wann_wout, centres, spreads = parse_wout_final_centres_spreads(args.wout)
    num_wann_hr, onsite, hops = read_hr_all(args.hr)

    if num_wann_wout != num_wann_hr:
        raise RuntimeError("num_wann mismatch: wout=%d, hr=%d. Use matching files." % (num_wann_wout, num_wann_hr))
    num_wann = num_wann_wout

    factor, elem_total, elem_orb = expected_counts(atoms, proj, num_wann)
    print("[INFO] num_wann=%d  factor=%d" % (num_wann, factor))
    print("[INFO] expected per-element totals: %s" % dict(elem_total))
    print("[INFO] expected per-element-orbital: %s" % {k: dict(v) for k, v in elem_orb.items()})

    wf_elem, wf_atom = assign_wfs_to_elements_quota(A, atoms, centres, elem_total)

    elem_wfs = defaultdict(list)
    for wf in range(1, num_wann + 1):
        elem_wfs[wf_elem[wf]].append(wf)

    Tc_set = set(elem_wfs.get("Tc", []))
    Se_set = set(elem_wfs.get("Se", []))
    Ir_set = set(elem_wfs.get("Ir", []))
    Ge_set = set(elem_wfs.get("Ge", []))

    # split Ge into d/p (only if Ge has both in projections)
    n_ge_d = elem_orb.get("Ge", {}).get("d", 0)
    n_ge_p = elem_orb.get("Ge", {}).get("p", 0)
    if n_ge_d > 0 and n_ge_p > 0:
        Ge_d, Ge_p = split_ge_dp(list(Ge_set), spreads, onsite, n_ge_d, n_ge_p, mode=args.ge_split)
    else:
        Ge_d, Ge_p = set(Ge_set), set()

    print("[INFO] derived sizes: Tc=%d Se=%d Ir=%d Ge=%d (Ge_d=%d Ge_p=%d)" %
          (len(Tc_set), len(Se_set), len(Ir_set), len(Ge_set), len(Ge_d), len(Ge_p)))

    if args.dump_groups:
        out = {
            "Tc": sorted(Tc_set),
            "Se": sorted(Se_set),
            "Ir": sorted(Ir_set),
            "Ge": sorted(Ge_set),
            "Ge_d": sorted(Ge_d),
            "Ge_p": sorted(Ge_p),
        }
        Path(args.dump_groups).write_text(json.dumps(out, indent=2), encoding="utf-8")
        print("[INFO] dumped group WF indices -> %s" % args.dump_groups)

    # Build edges from hr.dat
    rows = []
    dist_list = []

    for (Rx, Ry, Rz, m, n, re_, im_) in hops:
        if m == n and Rx == 0 and Ry == 0 and Rz == 0:
            continue
        if m < 1 or m > num_wann or n < 1 or n > num_wann:
            continue
        absH = float(np.hypot(re_, im_))
        if absH < args.min_absH:
            continue

        # distance between WF centers (n shifted by lattice R)
        cm = centres[m]
        cn = shift_by_R(A, centres[n], Rx, Ry, Rz)
        dist = float(np.linalg.norm(cn - cm))

        u_elem = wf_elem[m]
        v_elem = wf_elem[n]
        u_atom = wf_atom[m]
        v_atom = wf_atom[n]

        # pair label (for downstream filtering/analysis)
        # Additionally label Ge as Ge_d / Ge_p when possible.
        def label_elem_wf(elem, wf):
            if elem == "Ge":
                if wf in Ge_d:
                    return "Ge_d"
                if wf in Ge_p:
                    return "Ge_p"
                return "Ge"
            if elem == "Tc":
                return "Tc_d"   # your projections are Tc:d
            if elem == "Se":
                return "Se_p"   # Se:p
            if elem == "Ir":
                return "Ir_d"   # Ir:d
            return elem

        u_tag = label_elem_wf(u_elem, m)
        v_tag = label_elem_wf(v_elem, n)
        pair = "%s-%s" % (u_tag, v_tag)

        rows.append({
            "shell": 0,  # fill later
            "dist": dist,
            "absH": absH,
            "re": re_,
            "im": im_,
            "Rx": Rx, "Ry": Ry, "Rz": Rz,
            "u_wf": m, "v_wf": n,
            "u_atom": u_atom, "v_atom": v_atom,
            "u_elem": u_elem, "v_elem": v_elem,
            "pair": pair
        })
        dist_list.append(dist)

    if not rows:
        raise RuntimeError("No edges passed filters. Lower --min_absH or check hr.dat.")

    # Assign shells
    shell_id, centers = assign_shells_by_tol(dist_list, tol=args.tol)
    for i, sid in enumerate(shell_id):
        rows[i]["shell"] = sid

    # Optionally keep topK per shell
    if args.topk_per_shell and args.topk_per_shell > 0:
        by_shell = defaultdict(list)
        for r in rows:
            by_shell[r["shell"]].append(r)
        new_rows = []
        for sh, lst in by_shell.items():
            lst.sort(key=lambda x: x["absH"], reverse=True)
            new_rows.extend(lst[:args.topk_per_shell])
        rows = new_rows
        print("[INFO] after topk_per_shell=%d: edges=%d" % (args.topk_per_shell, len(rows)))

    # Write edges
    if args.out_edges:
        outp = Path(args.out_edges)
        outp.parent.mkdir(parents=True, exist_ok=True)
        fieldnames = ["pair", "shell", "dist", "absH", "re", "im",
                      "Rx", "Ry", "Rz", "u_wf", "v_wf", "u_atom", "v_atom", "u_elem", "v_elem"]
        with outp.open("w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=fieldnames)
            w.writeheader()
            for r in rows:
                w.writerow({k: r[k] for k in fieldnames})
        print("[INFO] wrote edges -> %s  (rows=%d)" % (str(outp), len(rows)))

    # Optional summary per shell
    if args.out_summary:
        by_shell = defaultdict(list)
        for r in rows:
            by_shell[r["shell"]].append(r)

        summ = []
        for sh in sorted(by_shell.keys()):
            lst = by_shell[sh]
            dmean = float(np.mean([x["dist"] for x in lst]))
            amax = float(np.max([x["absH"] for x in lst]))
            asum = float(np.sum([x["absH"] for x in lst]))
            n = len(lst)
            summ.append({
                "shell": sh,
                "N_edges": n,
                "dist_mean": dmean,
                "absH_max": amax,
                "absH_sum": asum
            })

        outp = Path(args.out_summary)
        outp.parent.mkdir(parents=True, exist_ok=True)
        with outp.open("w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=list(summ[0].keys()))
            w.writeheader()
            for r in summ:
                w.writerow(r)
        print("[INFO] wrote summary -> %s" % str(outp))


if __name__ == "__main__":
    main()


```

运行指令

```python
python 1step.py --win wannier90.win --centres wannier90_centres.xyz --hr wannier90_hr.dat --tol 0.10 --min_absH 1e-3 --topk 30 --out_summary 1step_up.csv --pairs ALL --skip_same_atom_R0 --out_edges 2step_up.csv
```
得到文件

```shell
Summary: 1step_up.csv
Edges: 2step_up.csv
```

```python

import csv
import argparse
from dataclasses import dataclass
from collections import defaultdict
from typing import Tuple, List, Dict, Optional


@dataclass(frozen=True)
class DirEdge:
    # directed edge: u(cell_shift) -> v(cell_shift + R)
    u_atom: int
    v_atom: int
    u_elem: str
    v_elem: str
    u_wf: int   # 1-based wf index
    v_wf: int   # 1-based wf index
    Rx: int
    Ry: int
    Rz: int
    absH: float
    dist: float
    pair: str = ""   # optional: "Se_p-Ge_d", "Se_p-Ir_d", ...
    shell: int = 0   # optional


def _pick(row: dict, *keys: str, default=None):
    for k in keys:
        if k in row and row[k] not in (None, ""):
            return row[k]
    return default


def parse_edges_csv(path: str, add_reverse: bool = False) -> List[DirEdge]:
    """
    Read edges csv produced by:
      - old 1step: dist_A, absH_eV, Rx,Ry,Rz, m,n, atom_m,atom_n, elem_m,elem_n
      - new 1step: dist, absH, Rx,Ry,Rz, u_wf,v_wf, u_atom,v_atom, u_elem,v_elem, pair(optional), shell(optional)

    For new 1step outputs, edges are already directed and include both directions in hr.dat,
    so add_reverse should usually be False to avoid doubling.
    """
    edges: List[DirEdge] = []
    with open(path, "r", encoding="utf-8", errors="ignore", newline="") as f:
        r = csv.DictReader(f)
        fns = r.fieldnames or []
        fset = set(fns)

        is_new = ("u_wf" in fset and "v_wf" in fset and "u_atom" in fset and "v_atom" in fset)
        is_old = ("m" in fset and "n" in fset and "atom_m" in fset and "atom_n" in fset)

        if not (is_new or is_old):
            raise RuntimeError(f"Unrecognized edges schema in {path}. Found columns: {fns}")

        for row in r:
            try:
                dist = float(_pick(row, "dist", "dist_A"))
                absH = float(_pick(row, "absH", "absH_eV"))
                Rx, Ry, Rz = int(_pick(row, "Rx")), int(_pick(row, "Ry")), int(_pick(row, "Rz"))

                u_wf = int(_pick(row, "u_wf", "m"))
                v_wf = int(_pick(row, "v_wf", "n"))

                u_atom = int(_pick(row, "u_atom", "atom_m"))
                v_atom = int(_pick(row, "v_atom", "atom_n"))

                u_elem = str(_pick(row, "u_elem", "elem_m")).strip()
                v_elem = str(_pick(row, "v_elem", "elem_n")).strip()

                pair = str(_pick(row, "pair", default="") or "")
                shell = int(_pick(row, "shell", default="0") or 0)
            except Exception:
                continue

            e = DirEdge(u_atom, v_atom, u_elem, v_elem, u_wf, v_wf, Rx, Ry, Rz, absH, dist, pair=pair, shell=shell)
            edges.append(e)

            if add_reverse:
                edges.append(DirEdge(
                    v_atom, u_atom, v_elem, u_elem,
                    v_wf, u_wf, -Rx, -Ry, -Rz,
                    absH, dist, pair=pair, shell=shell
                ))

    return edges


def load_onsite_from_hr(hr_path: str, num_wann: Optional[int] = None) -> Dict[int, float]:
    """Extract onsite energies eps[m] = Re(H_mm(R=0)) from wannier90_hr.dat."""
    with open(hr_path, "r", encoding="utf-8", errors="ignore") as f:
        lines = f.readlines()
    if len(lines) < 4:
        raise RuntimeError(f"hr.dat too short: {hr_path}")

    try:
        nw = int(lines[1].split()[0])
    except Exception:
        raise RuntimeError("Failed to parse num_wann from hr.dat line2.")
    if num_wann is not None and nw != num_wann:
        print(f"[WARN] num_wann mismatch: hr={nw} vs --num_wann={num_wann}. Using hr value.")
    num_wann = nw

    try:
        nrpts = int(lines[2].split()[0])
    except Exception:
        raise RuntimeError("Failed to parse nrpts from hr.dat line3.")

    deg = []
    idx = 3
    while idx < len(lines) and len(deg) < nrpts:
        for p in lines[idx].split():
            try:
                deg.append(int(p))
            except Exception:
                pass
        idx += 1
    if len(deg) < nrpts:
        raise RuntimeError("Failed to read full degeneracy list from hr.dat.")

    eps: Dict[int, float] = {}
    for j in range(idx, len(lines)):
        parts = lines[j].split()
        if len(parts) < 7:
            continue
        try:
            Rx, Ry, Rz = int(parts[0]), int(parts[1]), int(parts[2])
            m, n = int(parts[3]), int(parts[4])
            re = float(parts[5])
        except Exception:
            continue
        if Rx == 0 and Ry == 0 and Rz == 0 and m == n:
            eps[m] = re

    if len(eps) < num_wann:
        print(f"[WARN] onsite found {len(eps)}/{num_wann}. Some WFs missing onsite.")
    else:
        print(f"[INFO] onsite found {len(eps)}/{num_wann}.")
    return eps


def add_shift(s: Tuple[int, int, int], R: Tuple[int, int, int]) -> Tuple[int, int, int]:
    return (s[0] + R[0], s[1] + R[1], s[2] + R[2])


def abs_R_L1(R: Tuple[int, int, int]) -> int:
    return abs(R[0]) + abs(R[1]) + abs(R[2])


def keep_top_per_key(d: Dict[int, List[DirEdge]], topn: int):
    for k in list(d.keys()):
        lst = d[k]
        lst.sort(key=lambda x: x.absH, reverse=True)
        if len(lst) > topn:
            d[k] = lst[:topn]


def delta_pair(eps: Dict[int, float], wf_a: int, wf_b: int, floor: float) -> float:
    ea = eps.get(wf_a, None)
    eb = eps.get(wf_b, None)
    if ea is None or eb is None:
        return float("nan")
    d = abs(ea - eb)
    return max(d, floor)


def print_panel_summary(tag: str, uniq_sorted: List[tuple], panel_topk: int):
    """
    uniq_sorted elements: (score, N, D, d1, d2, d3, sig, ...)
    """
    if not uniq_sorted:
        print(f"[PANEL] {tag}: no paths")
        return

    k = min(panel_topk, len(uniq_sorted))
    top1 = uniq_sorted[0][0]
    sumk = sum(x[0] for x in uniq_sorted[:k])

    # also show N and D behavior (helps diagnose delta blow-up)
    N1, D1 = uniq_sorted[0][1], uniq_sorted[0][2]
    Nk = [x[1] for x in uniq_sorted[:k]]
    Dk = [x[2] for x in uniq_sorted[:k]]

    def _safe_mean(arr):
        return sum(arr) / max(1, len(arr))

    print(f"[PANEL] {tag}")
    print(f"  unique_paths : {len(uniq_sorted)}")
    print(f"  top1 score   : {top1:.6g}    (N={N1:.6g}, D={D1:.6g})")
    print(f"  sum top{k}   : {sumk:.6g}")
    print(f"  top{k} <N>   : {_safe_mean(Nk):.6g}")
    print(f"  top{k} <D>   : {_safe_mean(Dk):.6g}")


def main():
    ap = argparse.ArgumentParser(
        description="Rank Tc–Se–X–Se–Tc paths by (product |t|) / (product Δ) using onsite energies from hr.dat."
    )
    ap.add_argument("--edges", required=True, help="Input edges csv (from 1step.py)")
    ap.add_argument("--hr", required=True, help="wannier90_hr.dat path for onsite extraction")
    ap.add_argument("--out", default="top_paths_ratio.csv", help="Output ranked paths csv")

    ap.add_argument("--mediators", default="Ir,Ge", help="Comma-separated mediators, e.g. Ir,Ge or Ir only")
    ap.add_argument("--d_tcse", type=float, default=3.0, help="Max distance for Tc–Se edges (Ang)")
    ap.add_argument("--d_sex", type=float, default=3.0, help="Max distance for Se–X edges (Ang)")
    ap.add_argument("--min_absH", type=float, default=1e-6, help="Min |t| (eV) used for edges in path building")

    # channel filter using 'pair' column
    ap.add_argument("--X_orb", choices=["auto", "Ir_d", "Ge_d", "Ge_p", "Ge_all"],
                    default="auto",
                    help=("Filter mediator orbital channel using edge 'pair' labels from 1step.py. "
                          "auto: Ir->Ir_d, Ge->Ge_all."))

    ap.add_argument("--top_per_se_tc", type=int, default=60)
    ap.add_argument("--top_per_se_x", type=int, default=120)
    ap.add_argument("--top_per_x", type=int, default=120)
    ap.add_argument("--top_paths", type=int, default=2000, help="How many top paths to output after ranking")

    ap.add_argument("--require_same_tc_atom", action="store_true")
    ap.add_argument("--max_netR_L1", type=int, default=6, help="Filter by |dRx|+|dRy|+|dRz| <= this (999 disables)")
    ap.add_argument("--exclude_netR0", action="store_true", help="Drop net_dR=(0,0,0) loops")

    ap.add_argument("--delta_mode", choices=["sequential", "pairwise"], default="sequential")
    ap.add_argument("--delta_floor", type=float, default=1e-3)

    ap.add_argument("--add_reverse_edges", action="store_true",
                    help="Also add reversed edges. Usually OFF for new 1step edges (already directed).")

    # NEW: panel printing
    ap.add_argument("--panel_topk", type=int, default=100,
                    help="In-shell panel: report sum of topK scores (default 100).")

    args = ap.parse_args()

    mediators = [x.strip() for x in args.mediators.split(",") if x.strip()]
    if not mediators:
        raise RuntimeError("No mediators specified.")

    eps = load_onsite_from_hr(args.hr)
    all_dir = parse_edges_csv(args.edges, add_reverse=args.add_reverse_edges)

    tc_atoms = sorted({e.u_atom for e in all_dir if e.u_elem == "Tc"} | {e.v_atom for e in all_dir if e.v_elem == "Tc"})
    se_atoms = sorted({e.u_atom for e in all_dir if e.u_elem == "Se"} | {e.v_atom for e in all_dir if e.v_elem == "Se"})
    if not tc_atoms:
        raise RuntimeError("No Tc atoms found.")
    if not se_atoms:
        raise RuntimeError("No Se atoms found.")

    print(f"[INFO] Tc atoms: {tc_atoms}")
    print(f"[INFO] Se atoms: {se_atoms}")
    for M in mediators:
        med_atoms = sorted({e.u_atom for e in all_dir if e.u_elem == M} | {e.v_atom for e in all_dir if e.v_elem == M})
        print(f"[INFO] {M} atoms: {med_atoms}")

    def edge_pass_channel(e: DirEdge, M: str) -> bool:
        if not e.pair:
            return True
        p = e.pair
        if M == "Ir":
            # Ir is d in your setting
            return ("Ir_d" in p)
        if M == "Ge":
            if args.X_orb == "Ge_d":
                return ("Ge_d" in p)
            if args.X_orb == "Ge_p":
                return ("Ge_p" in p)
            if args.X_orb in ("Ge_all", "auto"):
                return ("Ge_d" in p) or ("Ge_p" in p) or ("Ge" in p)
            return True
        return True

    tc_to_se = defaultdict(list)
    se_to_tc = defaultdict(list)
    se_to_x = {M: defaultdict(list) for M in mediators}
    x_to_se = {M: defaultdict(list) for M in mediators}

    for e in all_dir:
        if e.absH < args.min_absH:
            continue

        if e.u_elem == "Tc" and e.v_elem == "Se" and e.dist <= args.d_tcse:
            tc_to_se[e.u_atom].append(e)
            continue
        if e.u_elem == "Se" and e.v_elem == "Tc" and e.dist <= args.d_tcse:
            se_to_tc[e.u_atom].append(e)
            continue

        if e.dist <= args.d_sex:
            for M in mediators:
                if e.u_elem == "Se" and e.v_elem == M and edge_pass_channel(e, M):
                    se_to_x[M][e.u_atom].append(e)
                elif e.u_elem == M and e.v_elem == "Se" and edge_pass_channel(e, M):
                    x_to_se[M][e.u_atom].append(e)

    keep_top_per_key(tc_to_se, topn=max(args.top_per_se_tc, 30))
    keep_top_per_key(se_to_tc, topn=max(args.top_per_se_tc, 30))
    for M in mediators:
        keep_top_per_key(se_to_x[M], topn=max(args.top_per_se_x, 50))
        keep_top_per_key(x_to_se[M], topn=max(args.top_per_x, 50))

    def compute_denoms(tc_wf, se1_wf, x_wf, se2_wf):
        floor = args.delta_floor
        if args.delta_mode == "sequential":
            d1 = delta_pair(eps, se1_wf, tc_wf, floor)
            d2 = delta_pair(eps, x_wf,  se1_wf, floor)
            d3 = delta_pair(eps, se2_wf, x_wf,  floor)
        else:
            d1 = delta_pair(eps, se1_wf, tc_wf, floor)
            d2 = delta_pair(eps, x_wf,   tc_wf, floor)
            d3 = delta_pair(eps, se2_wf, tc_wf, floor)

        if any([d != d for d in (d1, d2, d3)]):
            return float("nan"), d1, d2, d3
        return d1 * d2 * d3, d1, d2, d3

    # ---------- enumerate ----------
    paths_by_M = {M: [] for M in mediators}

    for tc0 in tc_atoms:
        shift0 = (0, 0, 0)

        for e1 in tc_to_se.get(tc0, []):
            se1 = e1.v_atom
            shift_se1 = add_shift(shift0, (e1.Rx, e1.Ry, e1.Rz))

            for M in mediators:
                for e2 in se_to_x[M].get(se1, []):
                    x = e2.v_atom
                    shift_x = add_shift(shift_se1, (e2.Rx, e2.Ry, e2.Rz))

                    for e3 in x_to_se[M].get(x, []):
                        se2 = e3.v_atom
                        shift_se2 = add_shift(shift_x, (e3.Rx, e3.Ry, e3.Rz))

                        for e4 in se_to_tc.get(se2, []):
                            tc1 = e4.v_atom
                            shift_tc1 = add_shift(shift_se2, (e4.Rx, e4.Ry, e4.Rz))

                            if args.require_same_tc_atom and (tc1 != tc0):
                                continue

                            netR = shift_tc1
                            if args.exclude_netR0 and netR == (0, 0, 0):
                                continue
                            if args.max_netR_L1 < 999 and abs_R_L1(netR) > args.max_netR_L1:
                                continue

                            N = e1.absH * e2.absH * e3.absH * e4.absH
                            D, d1, d2, d3 = compute_denoms(e1.u_wf, e1.v_wf, e2.v_wf, e3.v_wf)
                            if D != D:
                                continue
                            score = N / D

                            sig = (
                                M,
                                tc0, se1, x, se2, tc1,
                                netR[0], netR[1], netR[2],
                                (e1.u_wf, e1.v_wf, e1.Rx, e1.Ry, e1.Rz),
                                (e2.u_wf, e2.v_wf, e2.Rx, e2.Ry, e2.Rz),
                                (e3.u_wf, e3.v_wf, e3.Rx, e3.Ry, e3.Rz),
                                (e4.u_wf, e4.v_wf, e4.Rx, e4.Ry, e4.Rz),
                            )

                            paths_by_M[M].append((score, N, D, d1, d2, d3, sig,
                                                  M, tc0, se1, x, se2, tc1, netR, e1, e2, e3, e4))

    # ---------- de-dup within each mediator ----------
    uniq_by_M = {}
    for M, paths in paths_by_M.items():
        if not paths:
            uniq_by_M[M] = []
            continue
        best = {}
        for item in paths:
            sc, sig = item[0], item[6]
            if sig not in best or sc > best[sig][0]:
                best[sig] = item
        uniq = list(best.values())
        uniq.sort(key=lambda x: x[0], reverse=True)
        uniq_by_M[M] = uniq

    # ---------- panel print ----------
    print("\n===== SHELL PANEL SUMMARY =====")
    for M in mediators:
        tag = f"{M} (X_orb={args.X_orb})"
        print_panel_summary(tag, uniq_by_M[M], panel_topk=args.panel_topk)

    # ---------- merge for output CSV ----------
    merged = []
    for M in mediators:
        merged.extend(uniq_by_M[M])

    if not merged:
        raise RuntimeError("No paths found after applying constraints. Try loosening windows or lowering --min_absH.")

    merged.sort(key=lambda x: x[0], reverse=True)
    merged = merged[:args.top_paths]

    # ---------- write output ----------
    with open(args.out, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow([
            "mediator", "X_orb",
            "score_num_over_denom", "N_prod_absH", "D_prod_delta",
            "delta1", "delta2", "delta3",
            "Tc0_atom", "Se1_atom", "X_atom", "Se2_atom", "Tc1_atom",
            "net_dRx", "net_dRy", "net_dRz",
            "t1_absH", "d1_A", "wf1_u", "wf1_v", "R1x", "R1y", "R1z",
            "t2_absH", "d2_A", "wf2_u", "wf2_v", "R2x", "R2y", "R2z",
            "t3_absH", "d3_A", "wf3_u", "wf3_v", "R3x", "R3y", "R3z",
            "t4_absH", "d4_A", "wf4_u", "wf4_v", "R4x", "R4y", "R4z",
        ])

        for score, N, D, d1, d2, d3, sig, M, tc0, se1, x, se2, tc1, netR, e1, e2, e3, e4 in merged:
            w.writerow([
                M, args.X_orb,
                f"{score:.10g}", f"{N:.10g}", f"{D:.10g}",
                f"{d1:.10g}", f"{d2:.10g}", f"{d3:.10g}",
                tc0, se1, x, se2, tc1,
                netR[0], netR[1], netR[2],
                f"{e1.absH:.10g}", f"{e1.dist:.6f}", e1.u_wf, e1.v_wf, e1.Rx, e1.Ry, e1.Rz,
                f"{e2.absH:.10g}", f"{e2.dist:.6f}", e2.u_wf, e2.v_wf, e2.Rx, e2.Ry, e2.Rz,
                f"{e3.absH:.10g}", f"{e3.dist:.6f}", e3.u_wf, e3.v_wf, e3.Rx, e3.Ry, e3.Rz,
                f"{e4.absH:.10g}", f"{e4.dist:.6f}", e4.u_wf, e4.v_wf, e4.Rx, e4.Ry, e4.Rz,
            ])

    print(f"\n[DONE] wrote ranked paths to: {args.out}")
    print(f"[INFO] delta_mode={args.delta_mode}, delta_floor={args.delta_floor} eV")


if __name__ == "__main__":
    main()


```

运行指令

```python

python 2step.py --edges 2step_up.csv --out up.csv --mediators Ir --d_tcse 3.0 --d_sex 3.0 --min_absH 1e-3 --top_paths 2000 --require_same_tc_atom --max_netR_L1 6 --exclude_netR0 --delta_mode sequential --delta_floor 1e-3 --hr wannier90_hr.dat

```
重复运行上面指令，得到Ge-d和Ge-p的评分


然后增大delta_floor确保数值稳定，如果出现不一致，那么说明脚本或者公式存在问题，需要重新考虑。

目前我的材料体系得到的结果是Ge-d相比于Ir-d和Ge-p，Ge-d参与的评分更高，但是dos和投影能带指出，费米能级附近的主要贡献是Ge-p，所以考虑Ge-d轨道是不合适的。

所以真正的结论是，这是一个d-p-d-p-d的超超交换路径。


最终结论，经过详实的对超超交换路径的评分，落实了d-p杂化导致的强铁磁基态，且是四面体配位的Se原子路径。