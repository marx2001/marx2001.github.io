---
layout: post
title: "(原创自研)利用wannier验证超超交换路径(3)"
subtitle: "Experience Sharing"
background: '/img/bg-sci-note.jpg'
categories: sci-note
permalink: /sci-note_posts/20260207-super3
---

## <center>说明</center>
上一篇文章是固定d-p-d-p-d的超超交换路径，dp杂化，而不能排除d-p-p-p-d杂化的可能，因此本篇文章通过修改脚本主要探究d-p-p-p-d杂化与d-p-d-p-d哪个更占据主导。
1step.py 是提取在位能，2step.py是对各个原子进行超超交换路径组合，然后除以对应的在位能，形成一个值，根据这个值的大小判断超超交换路径贡献，找到路径最大的那个。

这篇文章是细化成能用于论文的指标，包括在位能等等。

/public/home/cssong/song/1mrx/9_single_layer/19_ReIrGe2S6/TcIrGeSe/3_static_band/4_wannier/ncl/20260207_detail


## <center>代码</center>  

(1) 第一步
    之前的脚本里虽然已经做了 WF centre → 最近原子（PBC minimum image）的映射（map_wf_to_atoms），但真正用于分组(Tc_d/Ir_d/Se_p/Ge_p)的仍然是硬编码的 WF 序号区间 wf_group。并且在主循环里 gm = wf_group(m), gn = wf_group(n) 也仍然走的是这个硬编码逻辑。这就导致：只要你换了投影顺序、num_wann、spin 通道、或者 Wannier 重排，分组就会错——因此"
之前修改是否正确"的结论是：核心问题没有被真正修掉（最近原子映射算出来了，但没用来分组）。

    下面我把你这份脚本做了"最小但关键"的正确修复：删除 wf_group(wf) 的"WF 序号区间分组"（这是不可靠的根源），改为：对每条 hopping，先用你已计算出的 elem_m/elem_n（来自最近
原子
映射），再通过 elem→group 映射得到 Tc_d/Ir_d/Se_p/Ge_p。新增命令行参数 --group_map 与 --group_order，确保你以后体系变了也不用改代码。pair 仍保持你现在的 Tc_d<->Se_p 这
种无向格式（便于做壳层统计），同时在 out_edges 里额外写出 group_m/group_n，方便你后续 2step 做筛选。
```python

import re
import csv
import math
import argparse
from collections import defaultdict, namedtuple

import numpy as np

Edge = namedtuple(
    "Edge",
    "pair shell dist absH re im Rx Ry Rz m n atom_m atom_n elem_m elem_n group_m group_n"
)

# =========================
# Parse wannier90.win
# =========================
def _extract_block(txt, block_name):
    m = re.search(rf"begin\s+{block_name}(.*?)end\s+{block_name}", txt, re.S | re.I)
    return None if not m else m.group(1).strip()

def parse_win_lattice_and_atoms(win_path):
    """
    Read unit_cell_cart and atoms_cart from wannier90.win.
    Assumes Angstrom if unit not specified.
    Returns:
      A (3x3) with columns = a1,a2,a3 in Angstrom, so cart = A @ frac
      atoms: list of dict {id, elem, r_cart(np.array shape(3))}
    """
    with open(win_path, "r", encoding="utf-8", errors="ignore") as f:
        txt = f.read()

    cell_block = _extract_block(txt, "unit_cell_cart")
    if cell_block is None:
        raise RuntimeError("Cannot find unit_cell_cart block in wannier90.win")

    lines = [ln.strip() for ln in cell_block.splitlines() if ln.strip()]
    if re.match(r"^(ang|angstrom|bohr)\b", lines[0], re.I):
        unit = lines[0].lower()
        vec_lines = lines[1:4]
    else:
        unit = "ang"
        vec_lines = lines[0:3]

    if len(vec_lines) < 3:
        raise RuntimeError("unit_cell_cart has < 3 lattice vectors")

    a1 = np.array([float(x) for x in vec_lines[0].split()[:3]], dtype=float)
    a2 = np.array([float(x) for x in vec_lines[1].split()[:3]], dtype=float)
    a3 = np.array([float(x) for x in vec_lines[2].split()[:3]], dtype=float)

    if "bohr" in unit:
        bohr_to_ang = 0.52917721092
        a1 *= bohr_to_ang
        a2 *= bohr_to_ang
        a3 *= bohr_to_ang

    A = np.stack([a1, a2, a3], axis=1)  # (3,3), columns are lattice vectors

    atoms_block = _extract_block(txt, "atoms_cart")
    if atoms_block is None:
        raise RuntimeError("Cannot find atoms_cart block in wannier90.win")

    atoms = []
    for idx, ln in enumerate([x for x in atoms_block.splitlines() if x.strip()], start=1):
        parts = ln.split()
        if len(parts) < 4:
            continue
        elem = parts[0]
        r = np.array([float(parts[1]), float(parts[2]), float(parts[3])], dtype=float)
        atoms.append({"id": idx, "elem": elem, "r": r})

    if not atoms:
        raise RuntimeError("atoms_cart parsed but got 0 atoms")

    return A, atoms


# =========================
# Parse wannier90_centres.xyz
# =========================
def parse_centres_xyz(xyz_path):
    """
    Parse wannier90_centres.xyz:
    line1 = N
    line2 = comment
    then N lines: <label> x y z
    Return centers list indexed 1..N: centers[wf] = np.array([x,y,z])
    """
    with open(xyz_path, "r", encoding="utf-8", errors="ignore") as f:
        lines = [ln.strip() for ln in f if ln.strip()]

    n = int(lines[0])
    if len(lines) < 2 + n:
        raise RuntimeError(f"centres.xyz incomplete: need {2+n} lines, got {len(lines)}")

    centers = [None] * (n + 1)
    for i in range(n):
        parts = lines[2 + i].split()
        if len(parts) < 4:
            raise RuntimeError(f"Bad centres.xyz line: {lines[2+i]}")
        centers[i + 1] = np.array([float(parts[1]), float(parts[2]), float(parts[3])], dtype=float)
    return centers


# =========================
# Parse wannier90_hr.dat
# =========================
def parse_hr_dat(hr_path):
    """
    Yields (Rx, Ry, Rz, m, n, reH, imH)
    """
    with open(hr_path, "r", encoding="utf-8", errors="ignore") as f:
        _ = f.readline()  # comment
        _num_wann = int(f.readline().strip())
        nrpts = int(f.readline().strip())

        # read degeneracy list
        degen = []
        while len(degen) < nrpts:
            ln = f.readline()
            if not ln:
                raise RuntimeError("Unexpected EOF while reading degeneracy list")
            ln = ln.strip()
            if not ln:
                continue
            degen += [int(x) for x in ln.split()]

        # data
        for ln in f:
            ln = ln.strip()
            if not ln:
                continue
            parts = ln.split()
            if len(parts) < 7:
                continue
            Rx, Ry, Rz = int(parts[0]), int(parts[1]), int(parts[2])
            m, n = int(parts[3]), int(parts[4])
            reH, imH = float(parts[5]), float(parts[6])
            yield Rx, Ry, Rz, m, n, reH, imH


# =========================
# Group mapping: element -> group label
# =========================
def parse_group_map(s):
    """
    "Tc:Tc_d,Ir:Ir_d,Se:Se_p,Ge:Ge_p" -> dict
    """
    mp = {}
    s = (s or "").strip()
    if not s:
        return mp
    for item in s.split(","):
        item = item.strip()
        if not item:
            continue
        if ":" not in item:
            raise ValueError(f"Bad group_map item '{item}', expected Elem:Group")
        k, v = item.split(":", 1)
        mp[k.strip()] = v.strip()
    return mp

def elem_to_group(elem, group_map):
    """
    Default rules (can be overridden by --group_map):
      Tc -> Tc_d, Ir -> Ir_d, Se -> Se_p, Ge -> Ge_p
    """
    if elem in group_map:
        return group_map[elem]
    if elem == "Tc":
        return "Tc_d"
    if elem == "Ir":
        return "Ir_d"
    if elem == "Se":
        return "Se_p"
    if elem == "Ge":
        return "Ge_p"
    return "OTHER"

def pair_name(g1, g2, order_map):
    """
    Deterministic ordering, avoid Se_p<->Tc_d flip.
    """
    o1 = order_map.get(g1, 999)
    o2 = order_map.get(g2, 999)
    if o1 < o2:
        return f"{g1}<->{g2}"
    if o2 < o1:
        return f"{g2}<->{g1}"
    return f"{min(g1,g2)}<->{max(g1,g2)}"


# =========================
# Minimum-image mapping: WF -> nearest atom
# =========================
def cart_to_frac(A, r_cart):
    return np.linalg.solve(A, r_cart)

def frac_to_cart(A, f):
    return A @ f

def wrap_delta_frac(df):
    return df - np.round(df)

def map_wf_to_atoms(A, centers, atoms):
    """
    Returns:
      wf_atom_id[wf] = atom index in atoms list (0-based)
      wf_atom_elem[wf] = element string
    Uses minimum-image distance in fractional space.
    """
    atom_frac = []
    for a in atoms:
        atom_frac.append(cart_to_frac(A, a["r"]))
    atom_frac = np.array(atom_frac)  # (Nat,3)

    wf_atom_id = [None] * len(centers)
    wf_atom_elem = [None] * len(centers)

    for wf in range(1, len(centers)):
        f_w = cart_to_frac(A, centers[wf])

        best_i = None
        best_d = 1e30
        for i, f_a in enumerate(atom_frac):
            df = wrap_delta_frac(f_w - f_a)
            dr = frac_to_cart(A, df)
            d = float(np.linalg.norm(dr))
            if d < best_d:
                best_d = d
                best_i = i

        wf_atom_id[wf] = best_i
        wf_atom_elem[wf] = atoms[best_i]["elem"]

    return wf_atom_id, wf_atom_elem


# =========================
# Main
# =========================
def main():
    ap = argparse.ArgumentParser(
        description="Summarize hoppings by (group-pair, distance shell) using WF centres + lattice translations."
    )
    ap.add_argument("--win", required=True, help="wannier90.win")
    ap.add_argument("--centres", required=True, help="wannier90_centres.xyz")
    ap.add_argument("--hr", required=True, help="wannier90_hr.dat")
    ap.add_argument("--tol", type=float, default=0.10, help="distance bin size (Ang), e.g. 0.05~0.15")
    ap.add_argument("--min_absH", type=float, default=1e-4, help="min |H| kept (eV)")
    ap.add_argument("--topk", type=int, default=30, help="topK edges per (pair,shell)")
    ap.add_argument("--skip_same_atom_R0", action="store_true",
                    help="skip terms where (atom_m==atom_n) AND (R==0). Recommended to remove onsite/local terms.")
    ap.add_argument("--skip_diag_R0", action="store_true",
                    help="skip diagonal terms (m==n) at R==0 (onsite).")
    ap.add_argument("--pairs", default="ALL", help="comma-separated allowed pairs like Tc_d<->Se_p, or ALL")
    ap.add_argument("--out_summary", default="hopping_summary_by_shell.csv")
    ap.add_argument("--out_edges", default="", help="optional: write filtered edges csv (empty=off)")

    # NEW: element -> group mapping (to replace hard-coded wf index ranges)
    ap.add_argument("--group_map", default="Tc:Tc_d,Ir:Ir_d,Se:Se_p,Ge:Ge_p",
                    help="Override element->group mapping, e.g. 'Tc:Tc_d,Ir:Ir_d,Se:Se_p'.")
    ap.add_argument("--group_order", default="Tc_d,Ir_d,Se_p,Ge_p,OTHER",
                    help="Group ordering used to format A<->B deterministically.")

    args = ap.parse_args()

    group_map = parse_group_map(args.group_map)
    order_list = [x.strip() for x in args.group_order.split(",") if x.strip()]
    order_map = {g: i for i, g in enumerate(order_list, start=1)}

    A, atoms = parse_win_lattice_and_atoms(args.win)
    centers = parse_centres_xyz(args.centres)

    wf_atom_id, wf_atom_elem = map_wf_to_atoms(A, centers, atoms)

    # quick sanity print: how many WFs mapped to each element
    elem_counts = defaultdict(int)
    for wf in range(1, len(centers)):
        elem_counts[wf_atom_elem[wf]] += 1
    print("[INFO] WF->nearest-atom element counts:", dict(sorted(elem_counts.items(), key=lambda x: x[0])))

    # Allowed pairs
    allowed_pairs = None
    if args.pairs.strip().upper() != "ALL":
        allowed_pairs = set([p.strip() for p in args.pairs.split(",") if p.strip()])

    # stats
    cnt = defaultdict(int)
    sumsq = defaultdict(float)
    maxv = defaultdict(float)
    top_edges = defaultdict(list)
    filtered_edges = []

    # Precompute lattice vectors for translation
    a1 = A[:, 0]
    a2 = A[:, 1]
    a3 = A[:, 2]

    def R_to_T(Rx, Ry, Rz):
        return Rx * a1 + Ry * a2 + Rz * a3

    for Rx, Ry, Rz, m, n, reH, imH in parse_hr_dat(args.hr):
        absH = math.hypot(reH, imH)
        if absH < args.min_absH:
            continue

        atom_m = wf_atom_id[m]
        atom_n = wf_atom_id[n]
        elem_m = wf_atom_elem[m]
        elem_n = wf_atom_elem[n]

        gm = elem_to_group(elem_m, group_map)
        gn = elem_to_group(elem_n, group_map)

        # drop OTHER by default (consistent with your original behavior)
        if gm == "OTHER" or gn == "OTHER":
            continue

        pair = pair_name(gm, gn, order_map=order_map)
        if allowed_pairs is not None and pair not in allowed_pairs:
            continue

        # Skip onsite/local terms if requested
        if (Rx, Ry, Rz) == (0, 0, 0):
            if args.skip_diag_R0 and (m == n):
                continue
            if args.skip_same_atom_R0 and (atom_m == atom_n):
                continue

        # distance between WF centers considering translation R
        T = R_to_T(Rx, Ry, Rz)
        dr = (centers[n] + T) - centers[m]
        dist = float(np.linalg.norm(dr))

        # bin to shell
        shell = round(dist / args.tol) * args.tol
        key = (pair, shell)

        cnt[key] += 1
        sumsq[key] += absH * absH
        if absH > maxv[key]:
            maxv[key] = absH

        e = Edge(pair, shell, dist, absH, reH, imH, Rx, Ry, Rz, m, n,
                 atom_m, atom_n, elem_m, elem_n, gm, gn)

        lst = top_edges[key]
        lst.append(e)
        lst.sort(key=lambda x: x.absH, reverse=True)
        if len(lst) > args.topk:
            lst[:] = lst[:args.topk]

        if args.out_edges:
            filtered_edges.append(e)

    # write summary
    with open(args.out_summary, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow([
            "pair", "shell_A", "count", "max_absH_eV", "rms_absH_eV",
            f"top{args.topk}_edges(m,n,atom_m,atom_n,elem_m,elem_n,group_m,group_n,Rx,Ry,Rz,absH,dist)"
        ])
        for (pair, shell) in sorted(cnt.keys(), key=lambda x: (x[0], x[1])):
            c = cnt[(pair, shell)]
            rms = math.sqrt(sumsq[(pair, shell)] / c) if c else 0.0
            tops = top_edges[(pair, shell)]
            tops_str = "; ".join([
                f"{e.m}-{e.n}|a{e.atom_m+1}-a{e.atom_n+1}|{e.elem_m}-{e.elem_n}"
                f"|{e.group_m}-{e.group_n}"
                f"@({e.Rx},{e.Ry},{e.Rz})|{e.absH:.6g}|d={e.dist:.3f}"
                for e in tops
            ])
            w.writerow([pair, f"{shell:.3f}", c, f"{maxv[(pair, shell)]:.6g}", f"{rms:.6g}", tops_str])

    # optional edges
    if args.out_edges:
        with open(args.out_edges, "w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow([
                "pair", "shell_A", "dist_A", "absH_eV", "re", "im",
                "Rx", "Ry", "Rz", "m", "n",
                "atom_m", "atom_n", "elem_m", "elem_n",
                "group_m", "group_n"
            ])
            for e in filtered_edges:
                w.writerow([
                    e.pair, f"{e.shell:.3f}", f"{e.dist:.6f}", f"{e.absH:.8g}",
                    f"{e.re:.8g}", f"{e.im:.8g}",
                    e.Rx, e.Ry, e.Rz, e.m, e.n,
                    e.atom_m + 1, e.atom_n + 1, e.elem_m, e.elem_n,
                    e.group_m, e.group_n
                ])

    print("Done.")
    print("Summary:", args.out_summary)
    if args.out_edges:
        print("Edges:", args.out_edges)


if __name__ == "__main__":
    main()


```

运行方法：

```python

python 1step.py --win wannier90.win --centres wannier90_centres.xyz --hr wannier90_hr.dat --skip_same_atom_R0 --out_edges edges.csv --out_summary 1step.csv

```


(2) 第二步是优化2step.py，对应1step.py

    当前脚本虽然已经建立了Wannier函数中心到最近原子的映射，但其用于轨道分组的核心逻辑仍然依赖于硬编码的Wannier函数序号区间。具体来说，在主循环中判断Tc_d、Ir_d等分组依据的
wf_group()函数，正是基于此不可靠的序号范围。这导致一旦体系的投影顺序、Wannier函数数量、自旋通道设置发生改变，或Wannier函数本身经过重排，分组结果必然出错。因此，之前对脚本的
修改并未触及这一根本问题，已计算出的原子映射信息实际上并未被用于关键的分组步骤。

    为解决此问题，需要进行一项关键修复：彻底删除基于序号区间的wf_group()函数。正确的做法是，在处理每条hopping时，直接利用已计算出的elem_m/elem_n（即映射得到的原子元素信
息），再通过一个可配置的、从元素到轨道组的映射关系（例如Tc-> Tc_d），来动态确定分组标签。同时，建议通过新增命令行参数来接收此映射关系及分组顺序，这将使脚本完全脱离对特定体系编
码的依赖，增强通用性。输出方面，pair列可继续保持Tc_d<->Se_p这类无向格式以利于统计，但需在结果中额外输出group_m/group_n两列，以便后续的2step.py脚本能够依据准确的分组信息进行
筛选和计算。

目前的 1step 输出已包含 group_m/group_n列（例如 Tc_d、Se_p 等），因此 2step 的核心改进在于：应优先依据这些明确的分组信息来筛选路径和建边，而不再依赖原始的 elem_m/elem_n
（如 "Tc"、"Se" 等元素标签）。这一调整将彻底避免对不稳定的“Wannier 函数序号范围”的依赖。

为了实现平滑过渡，对 2step 进行如下修改：新版将兼容两种输入格式。当输入文件包含 group_m/group_n列时，程序会优先使用分组信息；若为旧格式文件，则自动回退至基于 elem_m/elem_n
的逻辑。本次改动将在其基础上以“最小侵入”的方式完成，确保核心流程清晰且易于维护。


```python

import csv
import argparse
from dataclasses import dataclass
from collections import defaultdict
from typing import Tuple, List, Dict, Optional


@dataclass(frozen=True)
class DirEdge:
    # directed edge: u(cell_shift) -> v(cell_shift + R)
    u_atom: int
    v_atom: int
    u_elem: str
    v_elem: str
    u_group: str
    v_group: str
    u_wf: int   # 1-based wf index
    v_wf: int   # 1-based wf index
    Rx: int
    Ry: int
    Rz: int
    absH: float
    dist: float


def parse_edges_csv(path: str, prefer_group: bool = True) -> List[DirEdge]:
    """
    Read out_edges csv produced by NEW 1step (recommended) or old format.
    Required columns (minimum):
      dist_A, absH_eV, Rx,Ry,Rz, m,n, atom_m,atom_n, elem_m,elem_n
    Optional (NEW, preferred):
      group_m, group_n

    If group_m/group_n exists and prefer_group=True, use them; otherwise fallback to elem.
    """
    edges: List[DirEdge] = []
    with open(path, "r", encoding="utf-8", errors="ignore", newline="") as f:
        r = csv.DictReader(f)
        required = ["dist_A", "absH_eV", "Rx", "Ry", "Rz",
                    "m", "n", "atom_m", "atom_n", "elem_m", "elem_n"]
        for k in required:
            if k not in r.fieldnames:
                raise RuntimeError(f"Missing column '{k}' in {path}. Found: {r.fieldnames}")

        has_group = ("group_m" in r.fieldnames) and ("group_n" in r.fieldnames)
        if prefer_group and (not has_group):
            print("[WARN] group_m/group_n not found. Falling back to elem_m/elem_n filtering.")

        for row in r:
            try:
                dist = float(row["dist_A"])
                absH = float(row["absH_eV"])
                Rx, Ry, Rz = int(row["Rx"]), int(row["Ry"]), int(row["Rz"])
                m, n = int(row["m"]), int(row["n"])
                atom_m, atom_n = int(row["atom_m"]), int(row["atom_n"])
                elem_m, elem_n = row["elem_m"].strip(), row["elem_n"].strip()
            except Exception:
                continue

            if has_group:
                gm = row["group_m"].strip()
                gn = row["group_n"].strip()
            else:
                gm = elem_m
                gn = elem_n

            # Add both directions to make path enumeration easier.
            edges.append(DirEdge(atom_m, atom_n, elem_m, elem_n, gm, gn, m, n, Rx, Ry, Rz, absH, dist))
            edges.append(DirEdge(atom_n, atom_m, elem_n, elem_m, gn, gm, n, m, -Rx, -Ry, -Rz, absH, dist))

    return edges


def load_onsite_from_hr(hr_path: str, num_wann: Optional[int] = None) -> Dict[int, float]:
    """
    Parse wannier90_hr.dat and extract onsite energies eps[m] = Re(H_mm(R=0)).
    Returns dict: wf_index(1-based) -> eps_eV
    """
    with open(hr_path, "r", encoding="utf-8", errors="ignore") as f:
        lines = f.readlines()

    if len(lines) < 4:
        raise RuntimeError(f"hr.dat too short: {hr_path}")

    try:
        nw = int(lines[1].split()[0])
    except Exception:
        raise RuntimeError("Failed to parse num_wann from hr.dat line2.")
    if num_wann is not None and nw != num_wann:
        print(f"[WARN] num_wann mismatch: hr={nw} vs --num_wann={num_wann}. Using hr value.")
    num_wann = nw

    try:
        nrpts = int(lines[2].split()[0])
    except Exception:
        raise RuntimeError("Failed to parse nrpts from hr.dat line3.")

    deg = []
    idx = 3
    while idx < len(lines) and len(deg) < nrpts:
        parts = lines[idx].split()
        for p in parts:
            try:
                deg.append(int(p))
            except Exception:
                pass
        idx += 1
    if len(deg) < nrpts:
        raise RuntimeError("Failed to read full degeneracy list from hr.dat.")

    eps: Dict[int, float] = {}
    for j in range(idx, len(lines)):
        parts = lines[j].split()
        if len(parts) < 7:
            continue
        try:
            Rx, Ry, Rz = int(parts[0]), int(parts[1]), int(parts[2])
            m, n = int(parts[3]), int(parts[4])
            re = float(parts[5])
        except Exception:
            continue

        if Rx == 0 and Ry == 0 and Rz == 0 and m == n:
            eps[m] = re

    if len(eps) < num_wann:
        print(f"[WARN] onsite found {len(eps)}/{num_wann}. Some WFs missing onsite (unusual).")
    else:
        print(f"[INFO] onsite found {len(eps)}/{num_wann}.")
    return eps


def add_shift(s: Tuple[int, int, int], R: Tuple[int, int, int]) -> Tuple[int, int, int]:
    return (s[0] + R[0], s[1] + R[1], s[2] + R[2])


def abs_R_L1(R: Tuple[int, int, int]) -> int:
    return abs(R[0]) + abs(R[1]) + abs(R[2])


def keep_top_per_key(d: Dict[int, List[DirEdge]], topn: int):
    for k in list(d.keys()):
        lst = d[k]
        lst.sort(key=lambda x: x.absH, reverse=True)
        if len(lst) > topn:
            d[k] = lst[:topn]


def delta_pair(eps: Dict[int, float], wf_a: int, wf_b: int, floor: float) -> float:
    ea = eps.get(wf_a, None)
    eb = eps.get(wf_b, None)
    if ea is None or eb is None:
        return float("nan")
    d = abs(ea - eb)
    return max(d, floor)


def main():
    ap = argparse.ArgumentParser(
        description="Rank Tc–Se–X–Se–Tc paths by (product |t|) / (product Δ) using onsite energies from hr.dat."
    )
    ap.add_argument("--edges", required=True, help="Input edges csv (NEW 1step out_edges is recommended)")
    ap.add_argument("--hr", required=True, help="wannier90_hr.dat path for onsite extraction")
    ap.add_argument("--out", default="top_paths_ratio.csv", help="Output ranked paths csv")

    # NEW: use group labels (Tc_d / Se_p / Ir_d ...) if present
    ap.add_argument("--prefer_group", action="store_true", help="Prefer group_m/group_n if present (recommended).")
    ap.add_argument("--tc_group", default="Tc_d", help="Group label for Tc-centered d-like WFs (default: Tc_d)")
    ap.add_argument("--se_group", default="Se_p", help="Group label for Se-centered p-like WFs (default: Se_p)")
    ap.add_argument("--mediators", default="Ir_d",
                    help="Comma-separated mediator group labels (e.g. Ir_d,Ge_p). If no group cols, use element symbols (Ir,Ge).")

    ap.add_argument("--d_tcse", type=float, default=3.0, help="Max distance for Tc–Se edges (Ang)")
    ap.add_argument("--d_sex", type=float, default=3.0, help="Max distance for Se–X edges (Ang)")
    ap.add_argument("--min_absH", type=float, default=1e-3, help="Min |t| (eV) used for edges in path building")

    ap.add_argument("--top_per_se_tc", type=int, default=60)
    ap.add_argument("--top_per_se_x", type=int, default=120)
    ap.add_argument("--top_per_x", type=int, default=120)
    ap.add_argument("--top_paths", type=int, default=2000, help="How many top paths to output after ranking")

    ap.add_argument("--require_same_tc_atom", action="store_true")
    ap.add_argument("--max_netR_L1", type=int, default=6, help="Filter by |dRx|+|dRy|+|dRz| <= this (999 disables)")
    ap.add_argument("--exclude_netR0", action="store_true", help="Drop net_dR=(0,0,0) loops (recommended for Tc–Tc exchange)")

    ap.add_argument("--delta_mode", choices=["sequential", "pairwise"], default="sequential",
                    help=("How to define Δ product:\n"
                          "  sequential: Δ1=|ε(Se1)-ε(Tc)|, Δ2=|ε(X)-ε(Se1)|, Δ3=|ε(Se2)-ε(X)|\n"
                          "  pairwise:   Δ1=|ε(Se1)-ε(Tc)|, Δ2=|ε(X)-ε(Tc)|,  Δ3=|ε(Se2)-ε(Tc)|\n"
                          "Both are proxies; sequential is closer to step-by-step virtual hopping."))
    ap.add_argument("--delta_floor", type=float, default=1e-3,
                    help="Lower bound for each Δ (eV) to avoid division blow-up when ε nearly equal")

    args = ap.parse_args()

    mediators = [x.strip() for x in args.mediators.split(",") if x.strip()]
    if not mediators:
        raise RuntimeError("No mediators specified.")

    eps = load_onsite_from_hr(args.hr)

    # IMPORTANT: prefer_group=True only makes sense if group_m/group_n exist.
    all_dir = parse_edges_csv(args.edges, prefer_group=args.prefer_group)

    # Decide whether we are filtering by group or elem:
    # If group columns existed, DirEdge.u_group != DirEdge.u_elem in general; else group==elem.
    use_group = True if args.prefer_group else False

    # Helper accessors
    def Utag(e: DirEdge) -> str:
        return e.u_group if use_group else e.u_elem

    def Vtag(e: DirEdge) -> str:
        return e.v_group if use_group else e.v_elem

    tc_tag = args.tc_group if use_group else "Tc"
    se_tag = args.se_group if use_group else "Se"

    # Build node lists
    tc_atoms = sorted({e.u_atom for e in all_dir if Utag(e) == tc_tag} |
                      {e.v_atom for e in all_dir if Vtag(e) == tc_tag})
    se_atoms = sorted({e.u_atom for e in all_dir if Utag(e) == se_tag} |
                      {e.v_atom for e in all_dir if Vtag(e) == se_tag})

    if not tc_atoms:
        raise RuntimeError(f"No Tc nodes found under tag '{tc_tag}'. Check --prefer_group/--tc_group.")
    if not se_atoms:
        raise RuntimeError(f"No Se nodes found under tag '{se_tag}'. Check --prefer_group/--se_group.")

    print(f"[INFO] use_group={use_group}")
    print(f"[INFO] Tc tag: {tc_tag}, Tc atoms: {tc_atoms}")
    print(f"[INFO] Se tag: {se_tag}, Se atoms: {se_atoms}")
    for M in mediators:
        med_atoms = sorted({e.u_atom for e in all_dir if Utag(e) == M} |
                           {e.v_atom for e in all_dir if Vtag(e) == M})
        print(f"[INFO] mediator '{M}' atoms: {med_atoms}")

    # Build filtered edge pools
    tc_to_se = defaultdict(list)  # Tc atom -> edges Tc->Se
    se_to_tc = defaultdict(list)  # Se atom -> edges Se->Tc
    se_to_x = {M: defaultdict(list) for M in mediators}  # Se atom -> edges Se->M
    x_to_se = {M: defaultdict(list) for M in mediators}  # M atom  -> edges M->Se

    for e in all_dir:
        if e.absH < args.min_absH:
            continue

        u = Utag(e)
        v = Vtag(e)

        if u == tc_tag and v == se_tag and e.dist <= args.d_tcse:
            tc_to_se[e.u_atom].append(e)
            continue
        if u == se_tag and v == tc_tag and e.dist <= args.d_tcse:
            se_to_tc[e.u_atom].append(e)
            continue

        if e.dist <= args.d_sex:
            for M in mediators:
                if u == se_tag and v == M:
                    se_to_x[M][e.u_atom].append(e)
                elif u == M and v == se_tag:
                    x_to_se[M][e.u_atom].append(e)

    keep_top_per_key(tc_to_se, topn=max(args.top_per_se_tc, 30))
    keep_top_per_key(se_to_tc, topn=max(args.top_per_se_tc, 30))
    for M in mediators:
        keep_top_per_key(se_to_x[M], topn=max(args.top_per_se_x, 50))
        keep_top_per_key(x_to_se[M], topn=max(args.top_per_x, 50))

    paths = []

    def compute_denoms(tc_wf, se1_wf, x_wf, se2_wf) -> Tuple[float, float, float, float]:
        floor = args.delta_floor
        if args.delta_mode == "sequential":
            d1 = delta_pair(eps, se1_wf, tc_wf, floor)
            d2 = delta_pair(eps, x_wf,  se1_wf, floor)
            d3 = delta_pair(eps, se2_wf, x_wf,  floor)
        else:
            d1 = delta_pair(eps, se1_wf, tc_wf, floor)
            d2 = delta_pair(eps, x_wf,   tc_wf, floor)
            d3 = delta_pair(eps, se2_wf, tc_wf, floor)

        if any([d != d for d in (d1, d2, d3)]):  # NaN
            return float("nan"), d1, d2, d3
        return d1 * d2 * d3, d1, d2, d3

    for tc0 in tc_atoms:
        shift0 = (0, 0, 0)

        for e1 in tc_to_se.get(tc0, []):  # Tc->Se1
            se1 = e1.v_atom
            shift_se1 = add_shift(shift0, (e1.Rx, e1.Ry, e1.Rz))

            for M in mediators:
                for e2 in se_to_x[M].get(se1, []):  # Se1->X
                    x = e2.v_atom
                    shift_x = add_shift(shift_se1, (e2.Rx, e2.Ry, e2.Rz))

                    for e3 in x_to_se[M].get(x, []):  # X->Se2
                        se2 = e3.v_atom
                        shift_se2 = add_shift(shift_x, (e3.Rx, e3.Ry, e3.Rz))

                        for e4 in se_to_tc.get(se2, []):  # Se2->Tc1
                            tc1 = e4.v_atom
                            shift_tc1 = add_shift(shift_se2, (e4.Rx, e4.Ry, e4.Rz))

                            if args.require_same_tc_atom and (tc1 != tc0):
                                continue

                            netR = shift_tc1
                            if args.exclude_netR0 and netR == (0, 0, 0):
                                continue
                            if args.max_netR_L1 < 999 and abs_R_L1(netR) > args.max_netR_L1:
                                continue

                            N = e1.absH * e2.absH * e3.absH * e4.absH
                            D, d1, d2, d3 = compute_denoms(e1.u_wf, e1.v_wf, e2.v_wf, e3.v_wf)
                            if D != D:
                                continue

                            score = N / D

                            sig = (
                                M,
                                tc0, se1, x, se2, tc1,
                                netR[0], netR[1], netR[2],
                                (e1.u_wf, e1.v_wf, e1.Rx, e1.Ry, e1.Rz),
                                (e2.u_wf, e2.v_wf, e2.Rx, e2.Ry, e2.Rz),
                                (e3.u_wf, e3.v_wf, e3.Rx, e3.Ry, e3.Rz),
                                (e4.u_wf, e4.v_wf, e4.Rx, e4.Ry, e4.Rz),
                            )

                            paths.append((score, N, D, d1, d2, d3, sig, M,
                                          tc0, se1, x, se2, tc1, netR, e1, e2, e3, e4))

    if not paths:
        raise RuntimeError("No paths found. Try lowering --min_absH or loosening distance windows "
                           "or check tc/se/mediator tags.")

    best = {}
    for item in paths:
        score = item[0]
        sig = item[6]
        if sig not in best or score > best[sig][0]:
            best[sig] = item
    uniq = list(best.values())
    print(f"[INFO] paths raw={len(paths)}, unique={len(uniq)} (by signature)")

    uniq.sort(key=lambda x: x[0], reverse=True)
    uniq = uniq[:args.top_paths]

    with open(args.out, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow([
            "mediator",
            "score_num_over_denom", "N_prod_absH", "D_prod_delta",
            "delta1", "delta2", "delta3",
            "Tc0_atom", "Se1_atom", "X_atom", "Se2_atom", "Tc1_atom",
            "net_dRx", "net_dRy", "net_dRz",
            "t1_absH", "d1_A", "wf1_u", "wf1_v", "R1x", "R1y", "R1z",
            "t2_absH", "d2_A", "wf2_u", "wf2_v", "R2x", "R2y", "R2z",
            "t3_absH", "d3_A", "wf3_u", "wf3_v", "R3x", "R3y", "R3z",
            "t4_absH", "d4_A", "wf4_u", "wf4_v", "R4x", "R4y", "R4z",
        ])

        for score, N, D, d1, d2, d3, sig, M, tc0, se1, x, se2, tc1, netR, e1, e2, e3, e4 in uniq:
            w.writerow([
                M,
                f"{score:.10g}", f"{N:.10g}", f"{D:.10g}",
                f"{d1:.10g}", f"{d2:.10g}", f"{d3:.10g}",
                tc0, se1, x, se2, tc1,
                netR[0], netR[1], netR[2],
                f"{e1.absH:.10g}", f"{e1.dist:.6f}", e1.u_wf, e1.v_wf, e1.Rx, e1.Ry, e1.Rz,
                f"{e2.absH:.10g}", f"{e2.dist:.6f}", e2.u_wf, e2.v_wf, e2.Rx, e2.Ry, e2.Rz,
                f"{e3.absH:.10g}", f"{e3.dist:.6f}", e3.u_wf, e3.v_wf, e3.Rx, e3.Ry, e3.Rz,
                f"{e4.absH:.10g}", f"{e4.dist:.6f}", e4.u_wf, e4.v_wf, e4.Rx, e4.Ry, e4.Rz,
            ])

    print(f"[DONE] wrote ranked unique paths to: {args.out}")
    print(f"[INFO] delta_mode={args.delta_mode}, delta_floor={args.delta_floor} eV")
    print(f"[INFO] prefer_group={args.prefer_group}, tc_group={args.tc_group}, se_group={args.se_group}, mediators={mediators}")


if __name__ == "__main__":
    main()


```

使用方法：

```python

python 2step.py --edges edges.csv --out Ge.csv --mediators Ge --d_tcse 3.0 --d_sex 3.0 --min_absH 1e-4 --top_paths 2000 --require_same_tc_atom --max_netR_L1 6 --exclude_netR0 --delta_mode sequential --delta_floor 0.5 --hr wannier90_hr.dat

```
(3) 第三步，新增考虑U和J。

下面提供一份完整且可直接替换的 2step.py脚本。该版本在我之前提交的“带 U/JH 分母修正”基础上，进一步针对当前体系的特点做了适配调整。

在参数设置上，默认 hund_coef取值为 2（对应更合理的 U + 2JH形式，这是考虑到体系中 d 轨道呈 2+2+1 分裂的实际情况，因此不建议默认使用 4）。同时，保留了 --hund_coef参数供您
调节，可以运行 1、2、4 等不同取值以进行敏感性分析。

功能方面，保留了 --enable_UJ开关：开启时将使用包含 U 和 JH 的分母进行计算；关闭则回退到仅依赖 on-site 能级差的纯能量分母模式。为了方便论文写作与结果复现，输出 CSV 文件中会
新增记录相关参数，包括：enable_UJ、U_Tc、U_Ir、JH_Tc、JH_Ir以及 hund_coef。

需要说明的是，此版本默认采用元素模式（即识别 Tc、Se、Ir 等元素标签）。如果 edges.csv中已包含 group_m/group_n列（例如 Tc_d、Se_p、Ir_d），并希望依据这些分组信息进行处理，
请运行时添加以下参数


```python
import csv
import argparse
from dataclasses import dataclass
from collections import defaultdict
from typing import Tuple, List, Dict, Optional


@dataclass(frozen=True)
class DirEdge:
    # directed edge: u(cell_shift) -> v(cell_shift + R)
    u_atom: int
    v_atom: int
    u_elem: str
    v_elem: str
    u_group: str
    v_group: str
    u_wf: int   # 1-based wf index
    v_wf: int   # 1-based wf index
    Rx: int
    Ry: int
    Rz: int
    absH: float
    dist: float


def parse_edges_csv(path: str, prefer_group: bool = True) -> Tuple[List[DirEdge], Dict[int, str]]:
    """
    Read edges.csv produced by 1step.py.

    Required columns:
      dist_A, absH_eV, Rx,Ry,Rz, m,n, atom_m,atom_n, elem_m,elem_n
    Optional (recommended):
      group_m, group_n

    Returns:
      dir_edges: directed edges (both directions expanded)
      wf_tag: mapping wf_index -> tag (group if present else element)
    """
    edges: List[DirEdge] = []
    wf_tag: Dict[int, str] = {}

    with open(path, "r", encoding="utf-8", errors="ignore", newline="") as f:
        r = csv.DictReader(f)
        required = ["dist_A", "absH_eV", "Rx", "Ry", "Rz",
                    "m", "n", "atom_m", "atom_n", "elem_m", "elem_n"]
        for k in required:
            if k not in (r.fieldnames or []):
                raise RuntimeError(f"Missing column '{k}' in {path}. Found: {r.fieldnames}")

        has_group = ("group_m" in (r.fieldnames or [])) and ("group_n" in (r.fieldnames or []))
        if prefer_group and (not has_group):
            print("[WARN] group_m/group_n not found. Falling back to elem_m/elem_n filtering.")

        for row in r:
            try:
                dist = float(row["dist_A"])
                absH = float(row["absH_eV"])
                Rx, Ry, Rz = int(row["Rx"]), int(row["Ry"]), int(row["Rz"])
                m, n = int(row["m"]), int(row["n"])
                atom_m, atom_n = int(row["atom_m"]), int(row["atom_n"])
                elem_m, elem_n = row["elem_m"].strip(), row["elem_n"].strip()
            except Exception:
                continue

            if has_group:
                gm = row["group_m"].strip()
                gn = row["group_n"].strip()
            else:
                gm = elem_m
                gn = elem_n

            # wf -> tag (group or element)
            if m not in wf_tag:
                wf_tag[m] = gm
            if n not in wf_tag:
                wf_tag[n] = gn

            # Add both directions
            edges.append(DirEdge(atom_m, atom_n, elem_m, elem_n, gm, gn, m, n, Rx, Ry, Rz, absH, dist))
            edges.append(DirEdge(atom_n, atom_m, elem_n, elem_m, gn, gm, n, m, -Rx, -Ry, -Rz, absH, dist))

    return edges, wf_tag


def load_onsite_from_hr(hr_path: str) -> Dict[int, float]:
    """
    Extract onsite energies eps[m] = Re(H_mm(R=0)) from wannier90_hr.dat.
    """
    with open(hr_path, "r", encoding="utf-8", errors="ignore") as f:
        lines = f.readlines()

    if len(lines) < 4:
        raise RuntimeError(f"hr.dat too short: {hr_path}")

    num_wann = int(lines[1].split()[0])
    nrpts = int(lines[2].split()[0])

    # degeneracy list
    deg = []
    idx = 3
    while idx < len(lines) and len(deg) < nrpts:
        parts = lines[idx].split()
        for p in parts:
            try:
                deg.append(int(p))
            except Exception:
                pass
        idx += 1
    if len(deg) < nrpts:
        raise RuntimeError("Failed to read full degeneracy list from hr.dat.")

    eps: Dict[int, float] = {}
    for j in range(idx, len(lines)):
        parts = lines[j].split()
        if len(parts) < 7:
            continue
        try:
            Rx, Ry, Rz = int(parts[0]), int(parts[1]), int(parts[2])
            m, n = int(parts[3]), int(parts[4])
            re = float(parts[5])
        except Exception:
            continue
        if Rx == 0 and Ry == 0 and Rz == 0 and m == n:
            eps[m] = re

    if len(eps) < num_wann:
        print(f"[WARN] onsite found {len(eps)}/{num_wann}. Some WFs missing onsite.")
    else:
        print(f"[INFO] onsite found {len(eps)}/{num_wann}.")
    return eps


def add_shift(s: Tuple[int, int, int], R: Tuple[int, int, int]) -> Tuple[int, int, int]:
    return (s[0] + R[0], s[1] + R[1], s[2] + R[2])


def abs_R_L1(R: Tuple[int, int, int]) -> int:
    return abs(R[0]) + abs(R[1]) + abs(R[2])


def keep_top_per_key(d: Dict[int, List[DirEdge]], topn: int):
    for k in list(d.keys()):
        lst = d[k]
        lst.sort(key=lambda x: x.absH, reverse=True)
        if len(lst) > topn:
            d[k] = lst[:topn]


def delta_pair_with_UJ(
    eps: Dict[int, float],
    wf_tag: Dict[int, str],
    wf_a: int,
    wf_b: int,
    floor: float,
    enable_UJ: bool,
    U_Tc: float,
    U_Ir: float,
    JH_Tc: float,
    JH_Ir: float,
    hund_coef: float,
    tc_tag_for_UJ: str,
    ir_tag_for_UJ: str,
) -> float:
    """
    Δ(a,b) = max(|εa-εb| + UJ(a,b), floor)

    UJ(a,b) rule (minimal & robust):
      if either endpoint tag matches tc_tag_for_UJ -> add (U_Tc + hund_coef*JH_Tc)
      if either endpoint tag matches ir_tag_for_UJ -> add (U_Ir + hund_coef*JH_Ir)

    NOTE:
      - This is a controlled phenomenological correction.
      - hund_coef default is set to 2.0 (more suitable when d-manifold is not 5-fold degenerate).
    """
    ea = eps.get(wf_a, None)
    eb = eps.get(wf_b, None)
    if ea is None or eb is None:
        return float("nan")

    base = abs(ea - eb)
    if not enable_UJ:
        return max(base, floor)

    ta = wf_tag.get(wf_a, "")
    tb = wf_tag.get(wf_b, "")

    uj = 0.0
    if (ta == tc_tag_for_UJ) or (tb == tc_tag_for_UJ):
        uj += (U_Tc + hund_coef * JH_Tc)
    if (ta == ir_tag_for_UJ) or (tb == ir_tag_for_UJ):
        uj += (U_Ir + hund_coef * JH_Ir)

    return max(base + uj, floor)


def main():
    ap = argparse.ArgumentParser(
        description="Rank Tc–Se–X–Se–Tc paths by (product |t|) / (product Δ). "
                    "Δ can include U + hund_coef*JH when Tc/Ir d tags are involved."
    )
    ap.add_argument("--edges", required=True, help="edges.csv from 1step.py")
    ap.add_argument("--hr", required=True, help="wannier90_hr.dat for onsite extraction")
    ap.add_argument("--out", default="top_paths_ratio.csv", help="Output ranked paths csv")

    # Filtering mode
    ap.add_argument("--prefer_group", action="store_true",
                    help="Prefer group_m/group_n if present. If not, fall back to elem.")
    ap.add_argument("--tc_group", default="Tc_d", help="Tc tag when prefer_group is ON (default Tc_d)")
    ap.add_argument("--se_group", default="Se_p", help="Se tag when prefer_group is ON (default Se_p)")
    ap.add_argument("--mediators", default="Ir_d",
                    help="Comma-separated mediator tags. If prefer_group OFF, use elements Ir/Ge...")

    # Geometry and pruning
    ap.add_argument("--d_tcse", type=float, default=3.0, help="Max distance for Tc–Se edges (Ang)")
    ap.add_argument("--d_sex", type=float, default=3.0, help="Max distance for Se–X edges (Ang)")
    ap.add_argument("--min_absH", type=float, default=1e-4, help="Min |t| (eV) used for edges in path building")
    ap.add_argument("--top_per_se_tc", type=int, default=60)
    ap.add_argument("--top_per_se_x", type=int, default=120)
    ap.add_argument("--top_per_x", type=int, default=120)
    ap.add_argument("--top_paths", type=int, default=2000)

    ap.add_argument("--require_same_tc_atom", action="store_true")
    ap.add_argument("--max_netR_L1", type=int, default=6)
    ap.add_argument("--exclude_netR0", action="store_true")

    # Denominator form
    ap.add_argument("--delta_mode", choices=["sequential", "pairwise"], default="sequential")
    ap.add_argument("--delta_floor", type=float, default=0.1, help="Lower bound for each Δ (eV)")

    # U/JH controls
    ap.add_argument("--enable_UJ", action="store_true",
                    help="Enable adding U + hund_coef*JH to Δ when Tc/Ir tags are involved.")
    ap.add_argument("--U_Tc", type=float, default=3.0, help="Tc Hubbard U (eV). You set: 3.0")
    ap.add_argument("--U_Ir", type=float, default=1.0, help="Ir Hubbard U (eV). You set: 1.0")
    ap.add_argument("--JH_Tc", type=float, default=0.5, help="Tc Hund J_H (eV). Default 0.5")
    ap.add_argument("--JH_Ir", type=float, default=0.4, help="Ir Hund J_H (eV). Default 0.4")

    # IMPORTANT: default 2.0 (recommended for your split d-manifold)
    ap.add_argument("--hund_coef", type=float, default=2.0,
                    help="Coefficient in U + hund_coef*JH. Default 2.0 (recommended). "
                         "Try 1/2/4 for sensitivity test.")

    args = ap.parse_args()

    mediators = [x.strip() for x in args.mediators.split(",") if x.strip()]
    if not mediators:
        raise RuntimeError("No mediators specified.")

    eps = load_onsite_from_hr(args.hr)
    all_dir, wf_tag = parse_edges_csv(args.edges, prefer_group=args.prefer_group)

    # Decide tag space
    use_group = True if args.prefer_group else False

    def Utag(e: DirEdge) -> str:
        return e.u_group if use_group else e.u_elem

    def Vtag(e: DirEdge) -> str:
        return e.v_group if use_group else e.v_elem

    tc_tag = args.tc_group if use_group else "Tc"
    se_tag = args.se_group if use_group else "Se"

    # These are the tags that trigger UJ penalty:
    tc_tag_for_UJ = tc_tag
    ir_tag_for_UJ = ("Ir_d" if use_group else "Ir")

    tc_atoms = sorted({e.u_atom for e in all_dir if Utag(e) == tc_tag} |
                      {e.v_atom for e in all_dir if Vtag(e) == tc_tag})
    se_atoms = sorted({e.u_atom for e in all_dir if Utag(e) == se_tag} |
                      {e.v_atom for e in all_dir if Vtag(e) == se_tag})

    if not tc_atoms:
        raise RuntimeError(f"No Tc nodes found under tag '{tc_tag}'. Check --prefer_group/--tc_group.")
    if not se_atoms:
        raise RuntimeError(f"No Se nodes found under tag '{se_tag}'. Check --prefer_group/--se_group.")

    print(f"[INFO] use_group={use_group}, enable_UJ={args.enable_UJ}, hund_coef={args.hund_coef}")
    print(f"[INFO] Tc tag: {tc_tag}, Tc atoms: {tc_atoms}")
    print(f"[INFO] Se tag: {se_tag}, Se atoms: {se_atoms}")
    print(f"[INFO] mediators: {mediators}")
    if args.enable_UJ:
        print(f"[INFO] U_Tc={args.U_Tc} eV, U_Ir={args.U_Ir} eV, JH_Tc={args.JH_Tc} eV, JH_Ir={args.JH_Ir} eV")

    # Build filtered edge pools
    tc_to_se = defaultdict(list)  # Tc atom -> edges Tc->Se
    se_to_tc = defaultdict(list)  # Se atom -> edges Se->Tc
    se_to_x = {M: defaultdict(list) for M in mediators}  # Se atom -> edges Se->M
    x_to_se = {M: defaultdict(list) for M in mediators}  # M atom  -> edges M->Se

    for e in all_dir:
        if e.absH < args.min_absH:
            continue

        u = Utag(e)
        v = Vtag(e)

        if u == tc_tag and v == se_tag and e.dist <= args.d_tcse:
            tc_to_se[e.u_atom].append(e)
            continue
        if u == se_tag and v == tc_tag and e.dist <= args.d_tcse:
            se_to_tc[e.u_atom].append(e)
            continue

        if e.dist <= args.d_sex:
            for M in mediators:
                if u == se_tag and v == M:
                    se_to_x[M][e.u_atom].append(e)
                elif u == M and v == se_tag:
                    x_to_se[M][e.u_atom].append(e)

    keep_top_per_key(tc_to_se, topn=max(args.top_per_se_tc, 30))
    keep_top_per_key(se_to_tc, topn=max(args.top_per_se_tc, 30))
    for M in mediators:
        keep_top_per_key(se_to_x[M], topn=max(args.top_per_se_x, 50))
        keep_top_per_key(x_to_se[M], topn=max(args.top_per_x, 50))

    def compute_denoms(tc_wf, se1_wf, x_wf, se2_wf) -> Tuple[float, float, float, float]:
        floor = args.delta_floor
        if args.delta_mode == "sequential":
            d1 = delta_pair_with_UJ(
                eps, wf_tag, se1_wf, tc_wf, floor,
                enable_UJ=args.enable_UJ,
                U_Tc=args.U_Tc, U_Ir=args.U_Ir, JH_Tc=args.JH_Tc, JH_Ir=args.JH_Ir,
                hund_coef=args.hund_coef,
                tc_tag_for_UJ=tc_tag_for_UJ, ir_tag_for_UJ=ir_tag_for_UJ
            )
            d2 = delta_pair_with_UJ(
                eps, wf_tag, x_wf, se1_wf, floor,
                enable_UJ=args.enable_UJ,
                U_Tc=args.U_Tc, U_Ir=args.U_Ir, JH_Tc=args.JH_Tc, JH_Ir=args.JH_Ir,
                hund_coef=args.hund_coef,
                tc_tag_for_UJ=tc_tag_for_UJ, ir_tag_for_UJ=ir_tag_for_UJ
            )
            d3 = delta_pair_with_UJ(
                eps, wf_tag, se2_wf, x_wf, floor,
                enable_UJ=args.enable_UJ,
                U_Tc=args.U_Tc, U_Ir=args.U_Ir, JH_Tc=args.JH_Tc, JH_Ir=args.JH_Ir,
                hund_coef=args.hund_coef,
                tc_tag_for_UJ=tc_tag_for_UJ, ir_tag_for_UJ=ir_tag_for_UJ
            )
        else:
            d1 = delta_pair_with_UJ(
                eps, wf_tag, se1_wf, tc_wf, floor,
                enable_UJ=args.enable_UJ,
                U_Tc=args.U_Tc, U_Ir=args.U_Ir, JH_Tc=args.JH_Tc, JH_Ir=args.JH_Ir,
                hund_coef=args.hund_coef,
                tc_tag_for_UJ=tc_tag_for_UJ, ir_tag_for_UJ=ir_tag_for_UJ
            )
            d2 = delta_pair_with_UJ(
                eps, wf_tag, x_wf, tc_wf, floor,
                enable_UJ=args.enable_UJ,
                U_Tc=args.U_Tc, U_Ir=args.U_Ir, JH_Tc=args.JH_Tc, JH_Ir=args.JH_Ir,
                hund_coef=args.hund_coef,
                tc_tag_for_UJ=tc_tag_for_UJ, ir_tag_for_UJ=ir_tag_for_UJ
            )
            d3 = delta_pair_with_UJ(
                eps, wf_tag, se2_wf, tc_wf, floor,
                enable_UJ=args.enable_UJ,
                U_Tc=args.U_Tc, U_Ir=args.U_Ir, JH_Tc=args.JH_Tc, JH_Ir=args.JH_Ir,
                hund_coef=args.hund_coef,
                tc_tag_for_UJ=tc_tag_for_UJ, ir_tag_for_UJ=ir_tag_for_UJ
            )

        if any([d != d for d in (d1, d2, d3)]):  # NaN
            return float("nan"), d1, d2, d3
        return d1 * d2 * d3, d1, d2, d3

    paths = []

    for tc0 in tc_atoms:
        shift0 = (0, 0, 0)

        for e1 in tc_to_se.get(tc0, []):  # Tc->Se1
            se1 = e1.v_atom
            shift_se1 = add_shift(shift0, (e1.Rx, e1.Ry, e1.Rz))

            for M in mediators:
                for e2 in se_to_x[M].get(se1, []):  # Se1->X
                    x = e2.v_atom
                    shift_x = add_shift(shift_se1, (e2.Rx, e2.Ry, e2.Rz))

                    for e3 in x_to_se[M].get(x, []):  # X->Se2
                        se2 = e3.v_atom
                        shift_se2 = add_shift(shift_x, (e3.Rx, e3.Ry, e3.Rz))

                        for e4 in se_to_tc.get(se2, []):  # Se2->Tc1
                            tc1 = e4.v_atom
                            shift_tc1 = add_shift(shift_se2, (e4.Rx, e4.Ry, e4.Rz))

                            if args.require_same_tc_atom and (tc1 != tc0):
                                continue

                            netR = shift_tc1
                            if args.exclude_netR0 and netR == (0, 0, 0):
                                continue
                            if args.max_netR_L1 < 999 and abs_R_L1(netR) > args.max_netR_L1:
                                continue

                            N = e1.absH * e2.absH * e3.absH * e4.absH
                            D, d1, d2, d3 = compute_denoms(e1.u_wf, e1.v_wf, e2.v_wf, e3.v_wf)
                            if D != D:
                                continue

                            score = N / D
                            paths.append((score, N, D, d1, d2, d3,
                                          M, tc0, se1, x, se2, tc1, netR,
                                          e1, e2, e3, e4))

    if not paths:
        raise RuntimeError("No paths found. Try loosening distances, lowering --min_absH, "
                           "or check tags (prefer_group/tc_group/se_group/mediators).")

    # de-dup by signature
    best = {}
    for item in paths:
        score = item[0]
        key = (item[6], item[7], item[8], item[9], item[10], item[11], item[12],
               (item[13].u_wf, item[13].v_wf, item[13].Rx, item[13].Ry, item[13].Rz),
               (item[14].u_wf, item[14].v_wf, item[14].Rx, item[14].Ry, item[14].Rz),
               (item[15].u_wf, item[15].v_wf, item[15].Rx, item[15].Ry, item[15].Rz),
               (item[16].u_wf, item[16].v_wf, item[16].Rx, item[16].Ry, item[16].Rz))
        if key not in best or score > best[key][0]:
            best[key] = item

    uniq = list(best.values())
    uniq.sort(key=lambda x: x[0], reverse=True)
    uniq = uniq[:args.top_paths]

    with open(args.out, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow([
            # run meta
            "use_group", "enable_UJ", "U_Tc", "U_Ir", "JH_Tc", "JH_Ir", "hund_coef", "delta_mode", "delta_floor",
            # path score
            "mediator",
            "score_num_over_denom", "N_prod_absH", "D_prod_delta",
            "delta1", "delta2", "delta3",
            "Tc0_atom", "Se1_atom", "X_atom", "Se2_atom", "Tc1_atom",
            "net_dRx", "net_dRy", "net_dRz",
            "t1_absH", "d1_A", "wf1_u", "wf1_v", "R1x", "R1y", "R1z",
            "t2_absH", "d2_A", "wf2_u", "wf2_v", "R2x", "R2y", "R2z",
            "t3_absH", "d3_A", "wf3_u", "wf3_v", "R3x", "R3y", "R3z",
            "t4_absH", "d4_A", "wf4_u", "wf4_v", "R4x", "R4y", "R4z",
        ])

        for (score, N, D, d1, d2, d3,
             M, tc0, se1, x, se2, tc1, netR,
             e1, e2, e3, e4) in uniq:
            w.writerow([
                str(use_group), str(args.enable_UJ),
                f"{args.U_Tc:.6g}", f"{args.U_Ir:.6g}", f"{args.JH_Tc:.6g}", f"{args.JH_Ir:.6g}",
                f"{args.hund_coef:.6g}", args.delta_mode, f"{args.delta_floor:.6g}",
                M,
                f"{score:.10g}", f"{N:.10g}", f"{D:.10g}",
                f"{d1:.10g}", f"{d2:.10g}", f"{d3:.10g}",
                tc0, se1, x, se2, tc1,
                netR[0], netR[1], netR[2],
                f"{e1.absH:.10g}", f"{e1.dist:.6f}", e1.u_wf, e1.v_wf, e1.Rx, e1.Ry, e1.Rz,
                f"{e2.absH:.10g}", f"{e2.dist:.6f}", e2.u_wf, e2.v_wf, e2.Rx, e2.Ry, e2.Rz,
                f"{e3.absH:.10g}", f"{e3.dist:.6f}", e3.u_wf, e3.v_wf, e3.Rx, e3.Ry, e3.Rz,
                f"{e4.absH:.10g}", f"{e4.dist:.6f}", e4.u_wf, e4.v_wf, e4.Rx, e4.Ry, e4.Rz,
            ])

    print(f"[DONE] wrote ranked unique paths to: {args.out}")
    print(f"[INFO] delta_mode={args.delta_mode}, delta_floor={args.delta_floor} eV")
    print(f"[INFO] enable_UJ={args.enable_UJ}, U_Tc={args.U_Tc}, U_Ir={args.U_Ir}, "
          f"JH_Tc={args.JH_Tc}, JH_Ir={args.JH_Ir}, hund_coef={args.hund_coef}")


if __name__ == "__main__":
    main()


```


使用方法：

```python

 python 2step.py --edges edges.csv --hr wannier90_hr.dat --out Ir_d_UJ.csv --mediators Ir --enable_UJ --U_Tc 3 --U_Ir 1 --hund_coef 2 --delta_mode sequential --delta_floor 0.1 --d_tcse 3.0 --d_sex 3.0 --min_absH 1e-3 --top_paths 2000 --require_same_tc_atom --max_netR_L1 6 --exclude_netR0

```

（4）第四步，筛选各个原子链

选择各个原子链中，评分排名前3的原子链

```python
#!/usr/bin/env python3
import argparse
import pandas as pd


def find_col(df, candidates, required=True, name=""):
    for c in candidates:
        if c in df.columns:
            return c
    if required:
        raise RuntimeError(f"Cannot find column for {name}. Tried {candidates}. "
                           f"Available columns: {list(df.columns)}")
    return None


def pretty_mediator(tag: str) -> str:
    # Ir_d -> Ir, Ge_p -> Ge
    if isinstance(tag, str) and "_" in tag:
        return tag.split("_")[0]
    return str(tag)


def load_2step(csv_path: str, source_label: str, topk: int):
    df = pd.read_csv(csv_path)

    score_col = find_col(df, ["score_num_over_denom", "score", "S"], True, "score")
    med_col   = find_col(df, ["mediator", "mediator_tag", "M"], True, "mediator")

    Tc0 = find_col(df, ["Tc0", "Tc0_atom"], True, "Tc0")
    Se1 = find_col(df, ["Se1", "Se1_atom"], True, "Se1")
    X   = find_col(df, ["X", "X_atom"], True, "X")
    Se2 = find_col(df, ["Se2", "Se2_atom"], True, "Se2")
    Tc1 = find_col(df, ["Tc1", "Tc1_atom"], True, "Tc1")

    netRx = find_col(df, ["net_dRx", "netRx", "net_Rx"], required=False, name="netRx")
    netRy = find_col(df, ["net_dRy", "netRy", "net_Ry"], required=False, name="netRy")
    netRz = find_col(df, ["net_dRz", "netRz", "net_Rz"], required=False, name="netRz")
    has_netR = (netRx is not None) and (netRy is not None) and (netRz is not None)

    df = df.copy()
    df["score"] = pd.to_numeric(df[score_col], errors="coerce")
    df = df.dropna(subset=["score"])
    df = df.sort_values("score", ascending=False)

    if topk and topk > 0:
        df = df.head(topk)

    df["source"] = source_label
    df["mediator"] = df[med_col].astype(str)

    df["Tc0"] = df[Tc0].astype(int)
    df["Se1"] = df[Se1].astype(int)
    df["X"]   = df[X].astype(int)
    df["Se2"] = df[Se2].astype(int)
    df["Tc1"] = df[Tc1].astype(int)

    if has_netR:
        df["netRx"] = df[netRx].astype(int)
        df["netRy"] = df[netRy].astype(int)
        df["netRz"] = df[netRz].astype(int)
    else:
        df["netRx"] = 0
        df["netRy"] = 0
        df["netRz"] = 0

    # path string: Tc2-Se9-Ir1-Se8-Tc2
    def mk(row):
        M = pretty_mediator(row["mediator"])
        return f"Tc{row['Tc0']}-Se{row['Se1']}-{M}{row['X']}-Se{row['Se2']}-Tc{row['Tc1']}"
    df["path_str"] = df.apply(mk, axis=1)

    return df


def main():
    ap = argparse.ArgumentParser(
        description="Merge Ir/Ge 2step outputs, group by atom-chain, and keep top-3 rows per chain by score."
    )
    ap.add_argument("--ir_csv", default="Ir_d_UJ.csv")
    ap.add_argument("--ge_csv", default="Ge_1e-3_0.1.csv")
    ap.add_argument("--top_each", type=int, default=2000,
                    help="Read top K rows from each file before grouping (0 = read all). Default 2000.")
    ap.add_argument("--top_per_chain", type=int, default=3,
                    help="Keep top N rows per atom-chain (default 3).")
    ap.add_argument("--key_with_netR", action="store_true",
                    help="Include netR (net_dR*) in chain key (recommended if netR exists).")
    ap.add_argument("--out", default="chains_top3.csv")
    args = ap.parse_args()

    topk = None if args.top_each == 0 else args.top_each

    df_ir = load_2step(args.ir_csv, "Ir", 0 if topk is None else topk)
    df_ge = load_2step(args.ge_csv, "Ge", 0 if topk is None else topk)

    df = pd.concat([df_ir, df_ge], ignore_index=True)

    # group key: atom chain (+ optional netR)
    key_cols = ["Tc0", "Se1", "X", "Se2", "Tc1"]
    if args.key_with_netR:
        key_cols += ["netRx", "netRy", "netRz"]

    # rank chains by their best score (for stable chain_id)
    best = df.groupby(key_cols, dropna=False)["score"].max().reset_index()
    best = best.sort_values("score", ascending=False).reset_index(drop=True)
    best["chain_id"] = range(1, len(best) + 1)

    df = df.merge(best[key_cols + ["chain_id"]], on=key_cols, how="left")

    # within each chain keep top N
    df = df.sort_values("score", ascending=False).copy()
    df["rank_in_chain"] = df.groupby("chain_id")["score"].rank(method="first", ascending=False).astype(int)
    out = df[df["rank_in_chain"] <= args.top_per_chain].copy()
    out = out.sort_values(["chain_id", "rank_in_chain"], ascending=[True, True])

    # choose output columns (keep more if present)
    cols = ["chain_id", "rank_in_chain", "source", "path_str", "score",
            "Tc0", "Se1", "X", "Se2", "Tc1"]
    if args.key_with_netR:
        cols += ["netRx", "netRy", "netRz"]

    # optional physics columns if present
    for c in ["t1_absH", "t2_absH", "t3_absH", "t4_absH", "delta1", "delta2", "delta3"]:
        if c in out.columns:
            cols.append(c)

    # also include raw mediator tag (Ir_d / Ge_p) if you want
    if "mediator" in out.columns:
        cols.insert(cols.index("path_str"), "mediator")

    out[cols].to_csv(args.out, index=False)

    # brief preview
    print("\n=== Per-chain top results (top per chain) ===")
    print(f"[INFO] merged_rows={len(df)}  chains={best.shape[0]}  output_rows={len(out)}")
    print(out[cols].head(30).to_string(index=False))
    print(f"\n[DONE] wrote: {args.out}\n")


if __name__ == "__main__":
    main()


```

使用方法：

```python

python a.py --ir_csv Ir_d_UJ.csv --ge_csv Ge_1e-3_0.1.csv --top_each 2000 --top_per_chain 3 --out chains_top3.csv

```
使用两次，考虑netR

```python

python a.py --ir_csv Ir_d_UJ.csv --ge_csv Ge_1e-3_0.1.csv --top_each 2000 --top_per_chain 3 --key_with_netR --out chains_top3_netR.csv

```
(5) 第五步 提取超超交换各个原子之间的跃迁参数

运行下方代码：

```python

#!/usr/bin/env python3
import argparse
import pandas as pd


def main():
    ap = argparse.ArgumentParser(
        description="Extract Tc–Se, Se–X, X–Se, Se–Tc hoppings from chains_top3.csv and print numeric values."
    )
    ap.add_argument("--in_csv", default="chains_top3.csv", help="Input chains table from 2step grouping")
    ap.add_argument("--out_csv", default="hopping_edges_from_chains.csv", help="Output expanded edge list")
    ap.add_argument("--top_edge_each", type=int, default=10, help="Top-N edges per edge-type by |t| (default 10)")
    args = ap.parse_args()

    df = pd.read_csv(args.in_csv)

    # required columns
    req = ["path_str", "t1_absH", "t2_absH", "t3_absH", "t4_absH"]
    miss = [c for c in req if c not in df.columns]
    if miss:
        raise RuntimeError(f"Missing columns in {args.in_csv}: {miss}. Available: {list(df.columns)}")

    # atom indices if present
    atom_cols = ["Tc0", "Se1", "X", "Se2", "Tc1"]
    has_atoms = all(c in df.columns for c in atom_cols)

    # mediator tag if present
    med_col = "mediator" if "mediator" in df.columns else None

    rows = []
    for _, r in df.iterrows():
        path = r["path_str"]
        source = r["source"] if "source" in df.columns else ""
        chain_id = r["chain_id"] if "chain_id" in df.columns else None
        rank_in_chain = r["rank_in_chain"] if "rank_in_chain" in df.columns else None

        if has_atoms:
            Tc0, Se1, X, Se2, Tc1 = int(r["Tc0"]), int(r["Se1"]), int(r["X"]), int(r["Se2"]), int(r["Tc1"])
        else:
            Tc0 = Se1 = X = Se2 = Tc1 = None

        med = str(r[med_col]) if med_col else ""
        # Expand 4 steps
        rows.append({
            "chain_id": chain_id, "rank_in_chain": rank_in_chain, "source": source, "mediator": med,
            "path_str": path, "edge_type": "Tc-Se", "edge_str": f"Tc{Tc0}-Se{Se1}" if has_atoms else "",
            "absH_eV": float(r["t1_absH"])
        })
        rows.append({
            "chain_id": chain_id, "rank_in_chain": rank_in_chain, "source": source, "mediator": med,
            "path_str": path, "edge_type": "Se-X", "edge_str": f"Se{Se1}-X{X}" if has_atoms else "",
            "absH_eV": float(r["t2_absH"])
        })
        rows.append({
            "chain_id": chain_id, "rank_in_chain": rank_in_chain, "source": source, "mediator": med,
            "path_str": path, "edge_type": "X-Se", "edge_str": f"X{X}-Se{Se2}" if has_atoms else "",
            "absH_eV": float(r["t3_absH"])
        })
        rows.append({
            "chain_id": chain_id, "rank_in_chain": rank_in_chain, "source": source, "mediator": med,
            "path_str": path, "edge_type": "Se-Tc", "edge_str": f"Se{Se2}-Tc{Tc1}" if has_atoms else "",
            "absH_eV": float(r["t4_absH"])
        })

    out = pd.DataFrame(rows)
    out.to_csv(args.out_csv, index=False)

    # Print per-path detailed values
    print("\n=== Per-path hoppings (|t| in eV) ===")
    show_cols = ["chain_id", "rank_in_chain", "source", "path_str", "edge_type", "edge_str", "absH_eV"]
    if "chain_id" not in out.columns:
        show_cols = ["path_str", "edge_type", "edge_str", "absH_eV"]
    print(out[show_cols].to_string(index=False))

    # Top edges per type
    print(f"\n=== Top {args.top_edge_each} edges per edge_type (by |t|) ===")
    for et in ["Tc-Se", "Se-X", "X-Se", "Se-Tc"]:
        sub = out[out["edge_type"] == et].sort_values("absH_eV", ascending=False).head(args.top_edge_each)
        print(f"\n[{et}]")
        print(sub[["path_str", "edge_str", "absH_eV"]].to_string(index=False))

    print(f"\n[DONE] wrote: {args.out_csv}\n")


if __name__ == "__main__":
    main()


```

使用方法：

```python

python b.py --in_csv chains_top3.csv --out_csv hopping_edges_from_chains.csv

```

(6) 第六步 提取Tc之间直接交换，与超超交换强度的数量级作比较

```python

#!/usr/bin/env python3
import argparse
import pandas as pd


def main():
    ap = argparse.ArgumentParser(
        description="Extract Tc–Tc direct-exchange hoppings from edges.csv (Tc_d <-> Tc_d), ranked by |H|."
    )
    ap.add_argument("--edges", default="edges.csv", help="Input edges.csv")
    ap.add_argument("--out", default="TcTc_direct_hoppings_top.csv", help="Output csv")
    ap.add_argument("--topn", type=int, default=50, help="Top-N hoppings to output (default 50)")

    ap.add_argument("--prefer_group", action="store_true",
                    help="Use group_m/group_n tags if present (recommended).")
    ap.add_argument("--tc_tag", default="Tc_d",
                    help="Tc tag when using group columns (default Tc_d). "
                         "If not using group, this becomes element symbol (default Tc).")
    ap.add_argument("--dist_max", type=float, default=0.0,
                    help="Optional distance cutoff in Å (0 = no cutoff).")
    ap.add_argument("--min_absH", type=float, default=0.0,
                    help="Optional minimum |H| cutoff in eV (default 0).")

    ap.add_argument("--undirected_dedup", action="store_true",
                    help="Deduplicate A->B and B->A (keep the larger |H|). Recommended for clean tables.")
    args = ap.parse_args()

    df = pd.read_csv(args.edges)

    # Basic required columns
    required = ["absH_eV", "dist_A", "atom_m", "atom_n", "Rx", "Ry", "Rz", "m", "n", "elem_m", "elem_n"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise RuntimeError(f"Missing columns in {args.edges}: {missing}\nAvailable: {list(df.columns)}")

    has_group = ("group_m" in df.columns) and ("group_n" in df.columns)
    use_group = args.prefer_group and has_group

    # Normalize tag in element-mode
    if use_group:
        tc_tag = args.tc_tag
        tag_m = df["group_m"].astype(str).str.strip()
        tag_n = df["group_n"].astype(str).str.strip()
    else:
        tc_tag = args.tc_tag.split("_")[0]  # Tc_d -> Tc
        tag_m = df["elem_m"].astype(str).str.strip()
        tag_n = df["elem_n"].astype(str).str.strip()

    # Filter Tc-Tc (direct exchange channel)
    sub = df[(tag_m == tc_tag) & (tag_n == tc_tag)].copy()

    # Cutoffs
    sub["absH_eV"] = pd.to_numeric(sub["absH_eV"], errors="coerce")
    sub["dist_A"] = pd.to_numeric(sub["dist_A"], errors="coerce")
    sub = sub.dropna(subset=["absH_eV", "dist_A"])

    if args.min_absH > 0:
        sub = sub[sub["absH_eV"] >= args.min_absH]
    if args.dist_max and args.dist_max > 0:
        sub = sub[sub["dist_A"] <= args.dist_max]

    if sub.empty:
        raise RuntimeError(
            f"No Tc–Tc edges found. Check --prefer_group and --tc_tag.\n"
            f"use_group={use_group}, tc_tag={tc_tag}, "
            f"rows={len(df)}, columns={list(df.columns)}"
        )

    # Optional: keep only one of A->B and B->A
    if args.undirected_dedup:
        # Create undirected key: (min(atom), max(atom), Rx,Ry,Rz, min(wf), max(wf)) is too strict.
        # For a clean "direct exchange" table, usually dedup by (min(atom), max(atom), Rx,Ry,Rz) is enough.
        a = sub["atom_m"].astype(int)
        b = sub["atom_n"].astype(int)
        sub["_a"] = a.where(a <= b, b)
        sub["_b"] = b.where(a <= b, a)
        sub["_key"] = (
            sub["_a"].astype(str) + "-" + sub["_b"].astype(str) + "|R=" +
            sub["Rx"].astype(int).astype(str) + "," +
            sub["Ry"].astype(int).astype(str) + "," +
            sub["Rz"].astype(int).astype(str)
        )
        # Keep max |H| per key
        sub = sub.sort_values("absH_eV", ascending=False).groupby("_key", as_index=False).head(1)

    # Sort by |H| desc and take topn
    sub = sub.sort_values("absH_eV", ascending=False).head(args.topn)

    # Build a nice edge label
    sub["edge_str"] = "Tc" + sub["atom_m"].astype(int).astype(str) + "-Tc" + sub["atom_n"].astype(int).astype(str)

    # Include re/im if present
    cols = [
        "edge_str",
        "absH_eV",
        "dist_A",
        "atom_m", "atom_n",
        "m", "n",
        "Rx", "Ry", "Rz",
    ]
    if "reH_eV" in sub.columns:
        cols.insert(2, "reH_eV")
    if "imH_eV" in sub.columns:
        cols.insert(3 if "reH_eV" in sub.columns else 2, "imH_eV")
    # Also keep shell/group/pair if present
    for c in ["shell", "pair", "group_m", "group_n", "elem_m", "elem_n"]:
        if c in sub.columns and c not in cols:
            cols.append(c)

    out = sub[cols].copy()
    out.to_csv(args.out, index=False)

    # Print preview
    print(f"\n[INFO] use_group={use_group}, tc_tag={tc_tag}")
    print(f"[INFO] Tc–Tc edges found: {len(df[(tag_m==tc_tag) & (tag_n==tc_tag)])}")
    print(f"[INFO] output topn={len(out)} -> {args.out}\n")
    print(out.head(min(20, len(out))).to_string(index=False))
    print()

if __name__ == "__main__":
    main()


```


使用方法：

```python

python c.py --edges edges.csv --prefer_group --tc_tag Tc_d --undirected_dedup --dist_max 6.58 --topn 3 --out TcTc_direct_nn.csv

```




至此，得到了超超交换路径与直接交换路径的定量表述。


